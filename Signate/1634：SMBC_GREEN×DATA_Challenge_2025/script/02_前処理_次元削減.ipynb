{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea1283ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90382c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/iwasakitakahiro/github')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import common\n",
    "\n",
    "# モジュールの再読み込み\n",
    "importlib.reload(common)\n",
    "\n",
    "_common = common.Common()\n",
    "_common.BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6088f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/学習用データ/train.csv')\n",
    "test_df = pd.read_csv('../input/評価用データ/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30dea78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[_common.UNIQUE_KEY_COLS] = pd.to_datetime(train_df[_common.UNIQUE_KEY_COLS], utc=True)\n",
    "test_df[_common.UNIQUE_KEY_COLS] = pd.to_datetime(test_df[_common.UNIQUE_KEY_COLS], utc=True)\n",
    "\n",
    "train_df = train_df.set_index(_common.UNIQUE_KEY_COLS)\n",
    "test_df = test_df.set_index(_common.UNIQUE_KEY_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4f7d9",
   "metadata": {},
   "source": [
    "## 欠損値補完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a94b377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前後の値による線形補完\n",
    "missing_cols_train = train_df.columns[train_df.isnull().any()].tolist()\n",
    "missing_cols_test = test_df.columns[test_df.isnull().any()].tolist()\n",
    "\n",
    "train_df[missing_cols_train] = train_df[missing_cols_train].interpolate(method='linear')\n",
    "test_df[missing_cols_test] = test_df[missing_cols_test].interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc5970",
   "metadata": {},
   "source": [
    "## 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2217bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index()\n",
    "test_df = test_df.reset_index()\n",
    "\n",
    "train_df = train_df[_common.TRAIN_FEATURE_COLS]\n",
    "test_df = test_df[_common.TEST_FEATURE_COLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed0736",
   "metadata": {},
   "source": [
    "### 発電・供給データ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec9af1",
   "metadata": {},
   "source": [
    "#### 発電量合計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f128182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"generation\" を含む列名だけを抽出\n",
    "gene_cols = [col for col in train_df.columns if 'generation' in col]\n",
    "\n",
    "# 合計を新しい列として追加\n",
    "train_df['gene_sum'] = train_df[gene_cols].sum(axis=1)\n",
    "test_df['gene_sum'] = test_df[gene_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49cc687",
   "metadata": {},
   "source": [
    "#### 再生可能エネルギー比率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e428eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['sus_amount'] = train_df[_common.SUS_GENE_COLS].sum(axis=1)\n",
    "# test_df['sus_amount'] = test_df[_common.SUS_GENE_COLS].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47227c26",
   "metadata": {},
   "source": [
    "#### 火力発電比率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96bb8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fossil_cols = [col for col in train_df.columns if 'generation_fossil' in col]\n",
    "# train_df['fossil_amount'] = train_df[fossil_cols].sum(axis=1)\n",
    "# test_df['fossil_amount'] = test_df[fossil_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55645353",
   "metadata": {},
   "source": [
    "#### 発電量と供給量の比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd2da7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['gene_load_ratio'] = train_df['gene_sum'] / train_df['total_load_actual']\n",
    "test_df['gene_load_ratio'] = test_df['gene_sum'] / test_df['total_load_actual']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2cd7a",
   "metadata": {},
   "source": [
    "#### 発電コストの算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1900d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 各発電方式のコスト辞書（EUR/MWh）\n",
    "# cost_dict = {\n",
    "#     'biomass': 100,\n",
    "#     'brown_coal/lignite': 60,\n",
    "#     'gas': 70,\n",
    "#     'coal': 80,\n",
    "#     'oil': 150,\n",
    "#     'pump_hydro': 0,\n",
    "#     'runofriver_hydro': 45,\n",
    "#     'dam_hydro': 60,\n",
    "#     'nuclear': 50,\n",
    "#     'other': 80,\n",
    "#     'other_renewables': 85,\n",
    "#     'solar': 35,\n",
    "#     'waste': 100,\n",
    "#     'wind_onshore': 45\n",
    "# }\n",
    "\n",
    "# # 各発電量列 × コスト → 発電コスト列を作成\n",
    "# train_df['gene_cost'] = (\n",
    "#     train_df['generation_biomass'] * cost_dict['biomass'] +\n",
    "#     train_df['generation_fossil_brown_coal/lignite'] * cost_dict['brown_coal/lignite'] +\n",
    "#     train_df['generation_fossil_gas'] * cost_dict['gas'] +\n",
    "#     train_df['generation_fossil_hard_coal'] * cost_dict['coal'] +\n",
    "#     train_df['generation_fossil_oil'] * cost_dict['oil'] +\n",
    "#     train_df['generation_hydro_pumped_storage_consumption'] * cost_dict['pump_hydro'] +\n",
    "#     train_df['generation_hydro_run_of_river_and_poundage'] * cost_dict['runofriver_hydro'] +\n",
    "#     train_df['generation_hydro_water_reservoir'] * cost_dict['dam_hydro'] +\n",
    "#     train_df['generation_nuclear'] * cost_dict['nuclear'] +\n",
    "#     train_df['generation_other'] * cost_dict['other'] +\n",
    "#     train_df['generation_other_renewable'] * cost_dict['other_renewables'] +\n",
    "#     train_df['generation_solar'] * cost_dict['solar'] +\n",
    "#     train_df['generation_waste'] * cost_dict['waste'] +\n",
    "#     train_df['generation_wind_onshore'] * cost_dict['wind_onshore']\n",
    "# )\n",
    "\n",
    "# test_df['gene_cost'] = (\n",
    "#     test_df['generation_biomass'] * cost_dict['biomass'] +\n",
    "#     test_df['generation_fossil_brown_coal/lignite'] * cost_dict['brown_coal/lignite'] +\n",
    "#     test_df['generation_fossil_gas'] * cost_dict['gas'] +\n",
    "#     test_df['generation_fossil_hard_coal'] * cost_dict['coal'] +\n",
    "#     test_df['generation_fossil_oil'] * cost_dict['oil'] +\n",
    "#     test_df['generation_hydro_pumped_storage_consumption'] * cost_dict['pump_hydro'] +\n",
    "#     test_df['generation_hydro_run_of_river_and_poundage'] * cost_dict['runofriver_hydro'] +\n",
    "#     test_df['generation_hydro_water_reservoir'] * cost_dict['dam_hydro'] +\n",
    "#     test_df['generation_nuclear'] * cost_dict['nuclear'] +\n",
    "#     test_df['generation_other'] * cost_dict['other'] +\n",
    "#     test_df['generation_other_renewable'] * cost_dict['other_renewables'] +\n",
    "#     test_df['generation_solar'] * cost_dict['solar'] +\n",
    "#     test_df['generation_waste'] * cost_dict['waste'] +\n",
    "#     test_df['generation_wind_onshore'] * cost_dict['wind_onshore']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429cfb8",
   "metadata": {},
   "source": [
    "#### 残余需要、残余比率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8df147dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残余需要 = 総需要量 - 再エネ発電量\n",
    "train_df['residual_demand'] = train_df['total_load_actual'] - train_df[_common.SUS_GENE_COLS].sum(axis=1)\n",
    "test_df['residual_demand'] = test_df['total_load_actual'] - test_df[_common.SUS_GENE_COLS].sum(axis=1)\n",
    "\n",
    "# # 残余比率 = 残余需要 ÷ 総需要量\n",
    "train_df['residual_demand_ratio'] = train_df['residual_demand'] / train_df['total_load_actual']\n",
    "test_df['residual_demand_ratio'] = test_df['residual_demand'] / test_df['total_load_actual']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e59b2",
   "metadata": {},
   "source": [
    "#### メリットオーダー（電力市場で電力を供給する順番を、発電コストが安い順に並べたもの）を用いた特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "69ca0c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 加重平均発電コスト（€/MWh）\n",
    "# train_df['w_gene_cost'] = train_df['gene_cost'] / train_df['gene_sum']\n",
    "# test_df['w_gene_cost'] = test_df['gene_cost'] / test_df['gene_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2542943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高コスト発電比率\n",
    "train_df['high_cost_ratio'] = (train_df['generation_fossil_gas'] + train_df['generation_fossil_gas'] + train_df['generation_fossil_oil']) / train_df['gene_sum']\n",
    "test_df['high_cost_ratio'] = (test_df['generation_fossil_gas'] + test_df['generation_fossil_gas'] + test_df['generation_fossil_oil']) / test_df['gene_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "911a25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.drop(columns=gene_cols)\n",
    "# test_df = test_df.drop(columns=gene_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f40e59",
   "metadata": {},
   "source": [
    "#### 需給逼迫フラグ\n",
    "残余需要が過去平均と比べて高水準であればフラグを立てる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e5937c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_residual = train_df['residual_demand'].quantile(0.9)\n",
    "\n",
    "train_df['tight_supply_flag'] = (train_df['residual_demand'] > q3_residual).astype(int)\n",
    "test_df['tight_supply_flag']  = (test_df['residual_demand']  > q3_residual).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48be22d2",
   "metadata": {},
   "source": [
    "### 時系列特徴量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bbcdc8",
   "metadata": {},
   "source": [
    "#### 時間の三角関数の変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "783259eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['hour_sin'] = np.sin(2 * np.pi * train_df[_common.UNIQUE_KEY_COLS].dt.hour / 24)\n",
    "train_df['hour_cos'] = np.cos(2 * np.pi * train_df[_common.UNIQUE_KEY_COLS].dt.hour / 24)\n",
    "\n",
    "test_df['hour_sin'] = np.sin(2 * np.pi * test_df[_common.UNIQUE_KEY_COLS].dt.hour / 24)\n",
    "test_df['hour_cos'] = np.cos(2 * np.pi * test_df[_common.UNIQUE_KEY_COLS].dt.hour / 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f329b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['weekday_sin'] = np.sin(2 * np.pi * train_df[_common.UNIQUE_KEY_COLS].dt.weekday / 7)\n",
    "train_df['weekday_cos'] = np.cos(2 * np.pi * train_df[_common.UNIQUE_KEY_COLS].dt.weekday / 7)\n",
    "\n",
    "test_df['weekday_sin'] = np.sin(2 * np.pi * test_df[_common.UNIQUE_KEY_COLS].dt.weekday / 7)\n",
    "test_df['weekday_cos'] = np.cos(2 * np.pi * test_df[_common.UNIQUE_KEY_COLS].dt.weekday / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a36e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['month_sin'] = np.sin(2 * np.pi * train_df[_common.UNIQUE_KEY_COLS].dt.month / 12)\n",
    "train_df['month_cos'] = np.cos(2 * np.pi * train_df[_common.UNIQUE_KEY_COLS].dt.month / 12)\n",
    "\n",
    "test_df['month_sin'] = np.sin(2 * np.pi * test_df[_common.UNIQUE_KEY_COLS].dt.month / 12)\n",
    "test_df['month_cos'] = np.cos(2 * np.pi * test_df[_common.UNIQUE_KEY_COLS].dt.month / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "727d7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['quarter_sin'] = np.sin(2 * np.pi * train_df[_common.UNIQUE_KEY_COLS].dt.quarter / 4)\n",
    "train_df['quarter_cos'] = np.cos(2 * np.pi * train_df[_common.UNIQUE_KEY_COLS].dt.quarter / 4)\n",
    "\n",
    "test_df['quarter_sin'] = np.sin(2 * np.pi * test_df[_common.UNIQUE_KEY_COLS].dt.quarter / 4)\n",
    "test_df['quarter_cos'] = np.cos(2 * np.pi * test_df[_common.UNIQUE_KEY_COLS].dt.quarter / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e764567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: クオーターも作る、dayofyearインデックスの効果を調べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15282847",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['hour'] = np.arctan2(train_df['hour_sin'], train_df['hour_cos'])\n",
    "train_df['weekday'] = np.arctan2(train_df['weekday_cos'], train_df['weekday_sin'])\n",
    "train_df['month'] = np.arctan2(train_df['month_cos'], train_df['month_sin'])\n",
    "train_df['quarter'] = np.arctan2(train_df['quarter_cos'], train_df['quarter_sin'])\n",
    "test_df['hour'] = np.arctan2(test_df['hour_cos'], test_df['hour_sin'])\n",
    "test_df['weekday'] = np.arctan2(test_df['weekday_cos'], test_df['weekday_sin'])\n",
    "test_df['month'] = np.arctan2(test_df['month_cos'], test_df['month_sin'])\n",
    "test_df['quarter'] = np.arctan2(test_df['quarter_cos'], test_df['quarter_sin'])\n",
    "\n",
    "train_df = train_df.drop(columns=['hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos', 'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos'])\n",
    "test_df = test_df.drop(columns=['hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos', 'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f110d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['dayofyear'] = train_df[_common.UNIQUE_KEY_COLS].dt.dayofyear\n",
    "test_df['dayofyear']  = test_df[_common.UNIQUE_KEY_COLS].dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b60013",
   "metadata": {},
   "source": [
    "#### 発電量・供給量の時系列特徴量（ラグ、ローリング、エクスパンディング）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ad5edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# まず識別用フラグ追加（どちらがtrainかtestか後でわかるように）\n",
    "train_df['is_train'] = True\n",
    "test_df['is_train'] = False\n",
    "\n",
    "# train, testを結合（indexを一意にしたい場合はreset_index(drop=True)も可）\n",
    "all_df = pd.concat([train_df, test_df], axis=0, sort=False)\n",
    "\n",
    "# ---- ここでラグ・ローリング特徴量を一括計算 ----\n",
    "cols = ['gene_sum', 'total_load_actual']\n",
    "lag_hours       = [1, 24, 48, 72]\n",
    "rolling_windows = [168, 336, 672]     # 7, 14, 28 日\n",
    "scale_cols      = cols                # z-score を作る対象\n",
    "\n",
    "for col in cols:\n",
    "    # # --- ラグ ---\n",
    "    # for lag in lag_hours:\n",
    "    #     all_df[f'{col}_lag{lag}'] = all_df[col].shift(lag)\n",
    "\n",
    "    # # --- ローリング平均（同じ窓幅で他の統計も可） ---\n",
    "    # for win in rolling_windows:\n",
    "    #     all_df[f'{col}_rolling_mean{win}'] = (\n",
    "    #         all_df[col]\n",
    "    #         .shift(1)                          # 現在行を除外したい場合\n",
    "    #         .rolling(window=win, min_periods=1)\n",
    "    #         .mean()\n",
    "    #     )\n",
    "\n",
    "    # --- expanding 平均 / 標準偏差 / z-score ---\n",
    "    #   ・shift(1) を噛ませれば「t 時点の値を含まない」純粋な過去統計\n",
    "    past_series = all_df[col].shift(1)\n",
    "\n",
    "    all_df[f'{col}_exp_mean_to_t'] = past_series.expanding().mean()\n",
    "    all_df[f'{col}_exp_std_to_t']  = past_series.expanding().std(ddof=0)\n",
    "\n",
    "    # 0 除算を避けるため std==0 → NaN\n",
    "    all_df.loc[all_df[f'{col}_exp_std_to_t'] == 0, f'{col}_exp_std_to_t'] = np.nan\n",
    "\n",
    "    all_df[f'{col}_scaled'] = (\n",
    "        (all_df[col] - all_df[f'{col}_exp_mean_to_t']) /\n",
    "        all_df[f'{col}_exp_std_to_t']\n",
    "    )\n",
    "\n",
    "# ---- 計算後に再びtrain/testに分割 ----\n",
    "train_df = (\n",
    "    all_df[all_df['is_train']]\n",
    "    .drop(columns=['is_train'])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "test_df = (\n",
    "    all_df[~all_df['is_train']]\n",
    "    .drop(columns=['is_train', _common.TARGET_COL])  # 目的変数は残さない\n",
    "    .copy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454bc8cd",
   "metadata": {},
   "source": [
    "### 気象データ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ac0b4",
   "metadata": {},
   "source": [
    "#### 湿度を露点温度へ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a7ab2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dew_point(temp_k, rh):\n",
    "    \"\"\"\n",
    "    ケルビン温度と相対湿度から露点温度(ケルビン変換)を計算\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    temp_c = temp_k - 273.15  # K → ℃\n",
    "    a = 17.62\n",
    "    b = 243.12\n",
    "    gamma = np.log(rh / 100) + (a * temp_c) / (b + temp_c)\n",
    "    return (b * gamma) / (a - gamma) + 273.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "19689d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cities = ['madrid', 'barcelona', 'bilbao', 'seville', 'valencia']\n",
    "\n",
    "# 各都市について露点温度を計算して新しい列に追加\n",
    "for city in cities:\n",
    "    temp_col = f'{city}_temp'\n",
    "    hum_col = f'{city}_humidity'\n",
    "    dew_col = f'{city}_dew_point'\n",
    "    if temp_col in train_df.columns and hum_col in train_df.columns:\n",
    "        train_df[dew_col] = calc_dew_point(train_df[temp_col], train_df[hum_col])\n",
    "        train_df = train_df.drop(columns=hum_col)\n",
    "    if temp_col in test_df.columns and hum_col in test_df.columns:\n",
    "        test_df[dew_col] = calc_dew_point(test_df[temp_col], test_df[hum_col])\n",
    "        test_df = test_df.drop(columns=hum_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06ddbb",
   "metadata": {},
   "source": [
    "#### 人口比率による加重平均"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8c505",
   "metadata": {},
   "source": [
    "|年|マドリード|バルセロナ|ビルバオ|セビリア|バレンシア|\n",
    "|-|-|-|-|-|-|\n",
    "|2015|約13.4%|約11.4%|約0.76%|約1.5%|約1.8%|\n",
    "|2016|約13.6%|約11.5%|約0.76%|約1.5%|約1.8%|\n",
    "|2017|約13.7%|約11.6%|約0.76%|約1.5%|約1.8%|\n",
    "|2018|約13.9%|約11.7%|約0.75%|約1.5%|約1.8%|\n",
    "|平均|13.7%|11.4%|0.76%|1.5%|1.8%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ece5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_ratio = {\n",
    "    'madrid': 0.137,\n",
    "    'barcelona': 0.114,\n",
    "    'bilbao': 0.0076,\n",
    "    'seville': 0.015,\n",
    "    'valencia': 0.018\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ac28b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_cols = ['wind_speed', 'temp', 'pressure', 'dew_point', 'clouds_all']\n",
    "\n",
    "def add_weighted_weather(df, pop_ratio, weather_cols):\n",
    "    cities = list(pop_ratio.keys())\n",
    "    for w_col in weather_cols:\n",
    "        weighted_vals = np.zeros(len(df))\n",
    "        weight_sum = sum(pop_ratio[city] for city in cities)\n",
    "        for city in cities:\n",
    "            colname = f\"{city}_{w_col}\"\n",
    "            weighted_vals += df[colname].fillna(0) * pop_ratio[city]\n",
    "        df[f'weighted_{w_col}'] = weighted_vals / weight_sum\n",
    "    # 元カラム名リストを返す（削除用）\n",
    "    drop_cols = [f\"{city}_{w_col}\" for city in cities for w_col in weather_cols]\n",
    "    return df.drop(columns=drop_cols)\n",
    "\n",
    "# 実行例\n",
    "train_df = add_weighted_weather(train_df, pop_ratio, weather_cols)\n",
    "test_df = add_weighted_weather(test_df, pop_ratio, weather_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dfda9",
   "metadata": {},
   "source": [
    "#### 異常気象フラグ\n",
    "高温・低温で一定水準を超えたらフラグを立てる\n",
    "冷暖房度日（HDD/CDD）を調べたい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114ed9d",
   "metadata": {},
   "source": [
    "## 価格を特徴量に入れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3c5a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ターゲットエンコーディング（曜日×時間）\n",
    "for df in [train_df, test_df]:\n",
    "    df['tmp_month'] = pd.to_datetime(df[_common.UNIQUE_KEY_COLS]).dt.month\n",
    "    df['tmp_weekday'] = pd.to_datetime(df[_common.UNIQUE_KEY_COLS]).dt.weekday\n",
    "    df['tmp_hour'] = pd.to_datetime(df[_common.UNIQUE_KEY_COLS]).dt.hour\n",
    "\n",
    "# === ターゲットエンコーディング（曜日 × 時間） ===\n",
    "group_cols = ['tmp_month', 'tmp_weekday', 'tmp_hour']\n",
    "encoded_col = \"encoded_price_weekday_hour\"\n",
    "\n",
    "# train から平均価格テーブルを作成\n",
    "mean_price_table = train_df.groupby(group_cols)[_common.TARGET_COL].mean().reset_index()\n",
    "mean_price_table = mean_price_table.rename(columns={_common.TARGET_COL: encoded_col})\n",
    "\n",
    "# train/test にエンコード値を結合\n",
    "train_df = train_df.merge(mean_price_table, on=group_cols, how='left')\n",
    "test_df = test_df.merge(mean_price_table, on=group_cols, how='left')\n",
    "\n",
    "# === エンコードに使った列は削除（sin/cosで代替済みのため） ===\n",
    "train_df = train_df.drop(columns=group_cols)\n",
    "test_df = test_df.drop(columns=group_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d321a4b3",
   "metadata": {},
   "source": [
    "## エンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6a83f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_group(col):\n",
    "    col = str(col)\n",
    "    if col.find('clear') != -1:\n",
    "        return 1\n",
    "    elif col.find('clouds') != -1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "38c84c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in _common.CATEGORY_COLS:\n",
    "#     new_col = col + '_LabelEn'\n",
    "#     train_df[new_col] = train_df[col].apply(weather_group)\n",
    "#     train_df = train_df.drop(columns=col)\n",
    "#     test_df[new_col] = test_df[col].apply(weather_group)\n",
    "#     test_df = test_df.drop(columns=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8b468",
   "metadata": {},
   "source": [
    "#### 祝日フラグ\n",
    "ChatGPTに2015年〜2018年の休日・祝日にフラグを立てる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "33b182f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import holidays\n",
    "\n",
    "es_holidays = holidays.ES()  # this is a dict-like object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3b992206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['is_holiday'] = (train_df[_common.UNIQUE_KEY_COLS].dt.date.isin(es_holidays).astype(int))\n",
    "# test_df['is_holiday'] = (test_df[_common.UNIQUE_KEY_COLS].dt.date.isin(es_holidays).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83862612",
   "metadata": {},
   "source": [
    "## 主成分分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bead69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df):\n",
    "  \"\"\"\n",
    "  one hot encoder.\n",
    "\n",
    "  Args:\n",
    "      df (pd.DataFrame): source dataframe.\n",
    "\n",
    "  Returns:\n",
    "      pd.DataFrame: The one hot encoded dataframe.\n",
    "  \"\"\"\n",
    "  for col in df.columns:\n",
    "    if df[col].dtype == 'object' or df[col].dtype == 'category':\n",
    "      one_hot = pd.get_dummies(df[col], prefix=f'{col}_', dtype=int)\n",
    "      df = df.drop(col, axis=1)\n",
    "      df = df.join(one_hot)\n",
    "  return df\n",
    "\n",
    "def generate_hour_one_hot(df):\n",
    "  one_hot = pd.get_dummies(df['hour'].astype(str), prefix='hour_', dtype=int)\n",
    "  df = df.drop('hour', axis=1)\n",
    "  df = df.join(one_hot)\n",
    "  return df\n",
    "\n",
    "def generate_dayofweek_one_hot(df):\n",
    "  one_hot = pd.get_dummies(df['dayofweek'].astype(str), prefix='dayofweek_', dtype=int)\n",
    "  df = df.drop('dayofweek', axis=1)\n",
    "  df = df.join(one_hot)\n",
    "  return df\n",
    "\n",
    "def generate_dayofyear_one_hot(df):\n",
    "  one_hot = pd.get_dummies(df['dayofyear'].clip(upper=365).astype(str), prefix='dayofyear_', dtype=int)\n",
    "  df = df.drop('dayofyear', axis=1)\n",
    "  df = df.join(one_hot)\n",
    "  return df\n",
    "\n",
    "def setup_time(df):\n",
    "  \"\"\"\n",
    "  setup time(utc=True) values to dataframe index.\n",
    "\n",
    "  Args:\n",
    "      df (pd.DataFrame): source dataframe.\n",
    "\n",
    "  Returns:\n",
    "      pd.DataFrame: time indexed dataframe.\n",
    "  \"\"\"\n",
    "  df['time'] = pd.to_datetime(df['time'], utc=True)\n",
    "  df.set_index('time', inplace=True)\n",
    "  df['hour'] = df.index.hour\n",
    "  df['dayofweek'] = df.index.dayofweek\n",
    "  df['dayofyear'] = df.index.dayofyear\n",
    "  return df\n",
    "\n",
    "def setup_dataframe(file):\n",
    "  \"\"\"\n",
    "  setup dataframe.\n",
    "\n",
    "  Args:\n",
    "      file: csv file.\n",
    "\n",
    "  Returns:\n",
    "      pd.DataFrame: dataframe.\n",
    "  \"\"\"\n",
    "  df = pd.read_csv(os.path.join(DIR, file))\n",
    "  df = setup_time(df)\n",
    "  #df = generate_hour_one_hot(df)\n",
    "  #df = generate_dayofweek_one_hot(df)\n",
    "  #df = generate_dayofyear_one_hot(df)\n",
    "  df = one_hot_encoder(df)\n",
    "  return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d7faf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['year'] = train_df[_common.UNIQUE_KEY_COLS].dt.year\n",
    "# train_df['month'] = train_df[_common.UNIQUE_KEY_COLS].dt.month\n",
    "\n",
    "# # Select only numeric columns\n",
    "# numeric_cols = train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "# # Exclude 'price_actual', 'year', and 'month' from the numeric columns for correlation calculation against 'price_actual'\n",
    "# numeric_cols_for_corr = [col for col in numeric_cols if col not in ['price_actual', 'year', 'month', 'hour', 'dayofyear', ]]\n",
    "\n",
    "# # Create an empty dictionary to store the correlation dataframes\n",
    "# monthly_yearly_correlations = {}\n",
    "\n",
    "# # Iterate through each numeric column (excluding 'price_actual', 'year', 'month')\n",
    "# for col in numeric_cols_for_corr:\n",
    "#     # Calculate the Spearman correlation for each year and month\n",
    "#     corr_df = train_df.groupby(['year', 'month'])[[col, 'price_actual']].corr(method='spearman').unstack().iloc[:, 1].unstack()\n",
    "#     monthly_yearly_correlations[col] = corr_df\n",
    "\n",
    "# # Combine the correlation dataframes into a single dataframe\n",
    "# # This will be a MultiIndex dataframe where the outer index is the column name\n",
    "# # and the inner index is the year and month\n",
    "# correlation_summary_df = pd.concat(monthly_yearly_correlations, axis=0)\n",
    "\n",
    "# # Display the resulting dataframe\n",
    "# correlation_summary_df.fillna(0, inplace=True)\n",
    "# correlation_summary_df_abs = correlation_summary_df.abs()\n",
    "\n",
    "# corr_mean_val = correlation_summary_df_abs.mean()\n",
    "# corr_std_val = correlation_summary_df_abs.std(ddof=0)\n",
    "\n",
    "# # Calculate the threshold: mean + 2 * std\n",
    "# target_threshold_2std = corr_mean_val + 2.0 * corr_std_val\n",
    "\n",
    "# # Identify data points where the absolute correlation is greater than the threshold in any month\n",
    "# # We use .stack() to make it easier to filter based on values\n",
    "# anomalous_correlations = correlation_summary_df_abs[correlation_summary_df_abs > target_threshold_2std].stack().reset_index()\n",
    "\n",
    "# print(\"Data points where absolute correlation is greater than mean + 2*std in any month:\")\n",
    "# FEATURES = anomalous_correlations['level_0'].unique()\n",
    "# FEATURES = [column for column in FEATURES if column not in ['price_actual', 'year', 'month', 'hour', 'dayofyear', 'dayofweek']]\n",
    "# FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bad941c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATION_FEATURES = [column for column in FEATURES if column.startswith('generation')]\n",
    "# WEATHER_FEATURES = [column for column in FEATURES if column not in GENERATION_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "00ece089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt: GENERATION_FEATURESに定義された項目でPCAを取得\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # PCAモデルの初期化 (コンポーネント数を指定しない場合は、特徴量の数になる)\n",
    "# pca_generation = PCA(n_components=3, random_state=42)\n",
    "# pca_weather = PCA(n_components=5, random_state=42)\n",
    "# pca_all = PCA(n_components=8, random_state=42)\n",
    "\n",
    "# # データを変換\n",
    "# principal_components_g = pca_generation.fit_transform(train_df[GENERATION_FEATURES])\n",
    "# principal_components_w = pca_weather.fit_transform(train_df[WEATHER_FEATURES])\n",
    "# principal_components_a = pca_all.fit_transform(train_df[GENERATION_FEATURES + WEATHER_FEATURES])\n",
    "\n",
    "# def calculate_vector_norm_ratio(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "#   \"\"\"\n",
    "#   Calculate the ratio of the norms of two vectors.\n",
    "\n",
    "#   Args:\n",
    "#       vec1 (np.ndarray): The first input vector.\n",
    "#       vec2 (np.ndarray): The second input vector.\n",
    "\n",
    "#   Returns:\n",
    "#       float: The ratio of the norm of vec1 to the norm of vec2.\n",
    "#              Returns 0 if the norm of vec2 is zero to avoid division by zero.\n",
    "#   \"\"\"\n",
    "#   norm1 = np.linalg.norm(vec1)\n",
    "#   norm2 = np.linalg.norm(vec2)\n",
    "\n",
    "#   if norm2 == 0:\n",
    "#     return 0.0\n",
    "#   else:\n",
    "#     return norm1 / norm2\n",
    "\n",
    "# def calculate_cossim(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "#   \"\"\"\n",
    "#   Calculate the cosine similarity between two vectors.\n",
    "\n",
    "#   Args:\n",
    "#       vec1 (np.ndarray): The first input vector.\n",
    "#       vec2 (np.ndarray): The second input vector.\n",
    "\n",
    "#   Returns:\n",
    "#       float: The cosine similarity between the two vectors.\n",
    "#   \"\"\"\n",
    "#   # Reshape vectors for cosine_similarity function if they are 1D\n",
    "#   vec1 = vec1.flatten()\n",
    "#   vec2 = vec2.flatten()\n",
    "\n",
    "#   return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# def generation_feature_relations(x,pca=pca_generation):\n",
    "#   \"\"\"\n",
    "#   Calculate relations between generation features using PCA.\n",
    "\n",
    "#   Args:\n",
    "#       x (np.ndarray): vector of generation features.\n",
    "\n",
    "#   Returns:\n",
    "#       cosine_similarity (float): The cosine similarity between two vectors.\n",
    "#       vector_norm_ratio (float): The ratio of the norms of two vectors.\n",
    "#       principal_components_1 (first): The principal components of the input vector.\n",
    "#       principal_components_2 (second): The principal components of the input vector.\n",
    "#       principal_components_3 (third): The principal components of the input vector.\n",
    "#   \"\"\"\n",
    "#   x_trans = pca.transform([x])\n",
    "#   x_inv = pca.inverse_transform(x_trans)\n",
    "#   cosine_similarity_value = calculate_cossim(x.values, x_inv)\n",
    "#   vector_norm_ratio_value = calculate_vector_norm_ratio(x, x_inv)\n",
    "#   return cosine_similarity_value, vector_norm_ratio_value,x_trans[0][0],x_trans[0][1],x_trans[0][2]\n",
    "\n",
    "# def weather_feature_relations(x,pca=pca_weather):\n",
    "#   \"\"\"\n",
    "#   Calculate relations between weather features using PCA.\n",
    "\n",
    "#   Args:\n",
    "#       x (np.ndarray): vector of waether features.\n",
    "\n",
    "#   Returns:\n",
    "#       cosine_similarity (float): The cosine similarity between two vectors.\n",
    "#       vector_norm_ratio (float): The ratio of the norms of two vectors.\n",
    "#       principal_components_1 (first): The principal components of the input vector.\n",
    "#       principal_components_2 (second): The principal components of the input vector.\n",
    "#       principal_components_3 (third): The principal components of the input vector.\n",
    "#       principal_components_4 (4-th): The principal components of the input vector.\n",
    "#       principal_components_5 (5-th): The principal components of the input vector.\n",
    "#   \"\"\"\n",
    "#   x_trans = pca.transform([x])\n",
    "#   x_inv = pca.inverse_transform(x_trans)\n",
    "#   cosine_similarity_value = calculate_cossim(x.values, x_inv)\n",
    "#   vector_norm_ratio_value = calculate_vector_norm_ratio(x, x_inv)\n",
    "#   return cosine_similarity_value, vector_norm_ratio_value,x_trans[0][0],x_trans[0][1],x_trans[0][2],x_trans[0][3],x_trans[0][4]\n",
    "\n",
    "# def all_feature_relations(x,pca=pca_all):\n",
    "#   \"\"\"\n",
    "#   Calculate relations between all features using PCA.\n",
    "\n",
    "#   Args:\n",
    "#       x (np.ndarray): vector of all features.\n",
    "\n",
    "#   Returns:\n",
    "#       cosine_similarity (float): The cosine similarity between two vectors.\n",
    "#       vector_norm_ratio (float): The ratio of the norms of two vectors.\n",
    "#       principal_components_1 (first): The principal components of the input vector.\n",
    "#       principal_components_2 (second): The principal components of the input vector.\n",
    "#       principal_components_3 (third): The principal components of the input vector.\n",
    "#       principal_components_4 (4-th): The principal components of the input vector.\n",
    "#       principal_components_5 (5-th): The principal components of the input vector.\n",
    "#       principal_components_5 (6-th): The principal components of the input vector.\n",
    "#       principal_components_5 (7-th): The principal components of the input vector.\n",
    "#       principal_components_5 (8-th): The principal components of the input vector.\n",
    "#   \"\"\"\n",
    "#   x_trans = pca.transform([x])\n",
    "#   x_inv = pca.inverse_transform(x_trans)\n",
    "#   cosine_similarity_value = calculate_cossim(x.values, x_inv)\n",
    "#   vector_norm_ratio_value = calculate_vector_norm_ratio(x, x_inv)\n",
    "#   return cosine_similarity_value, vector_norm_ratio_value,x_trans[0][0],x_trans[0][1],x_trans[0][2],x_trans[0][3],x_trans[0][4],x_trans[0][5],x_trans[0][6],x_trans[0][7]\n",
    "\n",
    "\n",
    "# # Principal components を DataFrame に変換\n",
    "# principal_g_df = train_df[GENERATION_FEATURES].apply(lambda x: generation_feature_relations(x), axis=1)\n",
    "# principal_g_df = pd.DataFrame({'g_cosine_similarity': [x[0] for x in principal_g_df],\n",
    "#                              'g_vector_norm_ratio': [x[1] for x in principal_g_df],\n",
    "#                               'pc_g_1': [x[2] for x in principal_g_df],\n",
    "#                               'pc_g_2': [x[3] for x in principal_g_df],\n",
    "#                               'pc_g_3': [x[4] for x in principal_g_df],\n",
    "#                              }, index=train_df.index)\n",
    "# principal_w_df = train_df[WEATHER_FEATURES].apply(lambda x: weather_feature_relations(x), axis=1)\n",
    "# principal_w_df = pd.DataFrame({'w_cosine_similarity': [x[0] for x in principal_w_df],\n",
    "#                              'w_vector_norm_ratio': [x[1] for x in principal_w_df],\n",
    "#                               'pc_w_1': [x[2] for x in principal_w_df],\n",
    "#                               'pc_w_2': [x[3] for x in principal_w_df],\n",
    "#                               'pc_w_3': [x[4] for x in principal_w_df],\n",
    "#                               'pc_w_4': [x[5] for x in principal_w_df],\n",
    "#                               'pc_w_5': [x[6] for x in principal_w_df],\n",
    "#                              }, index=train_df.index)\n",
    "# principal_a_df = train_df[GENERATION_FEATURES + WEATHER_FEATURES].apply(lambda x: all_feature_relations(x), axis=1)\n",
    "# principal_a_df = pd.DataFrame({'a_cosine_similarity': [x[0] for x in principal_a_df],\n",
    "#                              'a_vector_norm_ratio': [x[1] for x in principal_a_df],\n",
    "#                               'pc_a_1': [x[2] for x in principal_a_df],\n",
    "#                               'pc_a_2': [x[3] for x in principal_a_df],\n",
    "#                               'pc_a_3': [x[4] for x in principal_a_df],\n",
    "#                               'pc_a_4': [x[5] for x in principal_a_df],\n",
    "#                               'pc_a_5': [x[6] for x in principal_a_df],\n",
    "#                               'pc_a_6': [x[7] for x in principal_a_df],\n",
    "#                               'pc_a_7': [x[8] for x in principal_a_df],\n",
    "#                               'pc_a_8': [x[9] for x in principal_a_df],\n",
    "#                              }, index=train_df.index)\n",
    "\n",
    "# # 元のDataFrameに結合することもできます\n",
    "# train_df_pca = pd.concat([train_df, principal_g_df, principal_w_df, principal_a_df], axis=1)\n",
    "\n",
    "# principal_g_df_test = test_df[GENERATION_FEATURES].apply(lambda x: generation_feature_relations(x), axis=1)\n",
    "# principal_g_df_test = pd.DataFrame({'g_cosine_similarity': [x[0] for x in principal_g_df_test],\n",
    "#                                   'g_vector_norm_ratio': [x[1] for x in principal_g_df_test],\n",
    "#                                   'pc_g_1': [x[2] for x in principal_g_df_test],\n",
    "#                                   'pc_g_2': [x[3] for x in principal_g_df_test],\n",
    "#                                   'pc_g_3': [x[4] for x in principal_g_df_test],\n",
    "#                                   }, index=test_df.index)\n",
    "# principal_w_df_test = test_df[WEATHER_FEATURES].apply(lambda x: weather_feature_relations(x), axis=1)\n",
    "# principal_w_df_test = pd.DataFrame({'w_cosine_similarity': [x[0] for x in principal_w_df_test],\n",
    "#                                   'w_vector_norm_ratio': [x[1] for x in principal_w_df_test],\n",
    "#                                   'pc_w_1': [x[2] for x in principal_w_df_test],\n",
    "#                                   'pc_w_2': [x[3] for x in principal_w_df_test],\n",
    "#                                   'pc_w_3': [x[4] for x in principal_w_df_test],\n",
    "#                                   'pc_w_4': [x[5] for x in principal_w_df_test],\n",
    "#                                   'pc_w_5': [x[6] for x in principal_w_df_test],\n",
    "#                                   }, index=test_df.index)\n",
    "# principal_a_df_test = test_df[GENERATION_FEATURES + WEATHER_FEATURES].apply(lambda x: all_feature_relations(x), axis=1)\n",
    "# principal_a_df_test = pd.DataFrame({'a_cosine_similarity': [x[0] for x in principal_a_df_test],\n",
    "#                              'a_vector_norm_ratio': [x[1] for x in principal_a_df_test],\n",
    "#                               'pc_a_1': [x[2] for x in principal_a_df_test],\n",
    "#                               'pc_a_2': [x[3] for x in principal_a_df_test],\n",
    "#                               'pc_a_3': [x[4] for x in principal_a_df_test],\n",
    "#                               'pc_a_4': [x[5] for x in principal_a_df_test],\n",
    "#                               'pc_a_5': [x[6] for x in principal_a_df_test],\n",
    "#                               'pc_a_6': [x[7] for x in principal_a_df_test],\n",
    "#                               'pc_a_7': [x[8] for x in principal_a_df_test],\n",
    "#                               'pc_a_8': [x[9] for x in principal_a_df_test],\n",
    "#                              }, index=test_df.index)\n",
    "\n",
    "# test_df_pca = pd.concat([test_df, principal_g_df_test, principal_w_df_test, principal_a_df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "25d5ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_generation.explained_variance_/pca_generation.explained_variance_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b33e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_weather.explained_variance_/pca_weather.explained_variance_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b371fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_all.explained_variance_/pca_all.explained_variance_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a96604",
   "metadata": {},
   "source": [
    "## 特徴量選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d5ae432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.dropna(axis=0, how='any')\n",
    "# data_x = train_df.drop(columns=[_common.TARGET_COL, _common.UNIQUE_KEY_COLS])\n",
    "# data_y = train_df[_common.TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e05a2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rf_reg = RandomForestRegressor(\n",
    "#     n_estimators=500,\n",
    "#     n_jobs=-1,\n",
    "#     max_depth=10,\n",
    "#     random_state=1234 \n",
    "# )\n",
    "# rf_reg = rf_reg.fit(data_x, data_y.values.ravel())\n",
    "\n",
    "# # 特徴量重要度の取得\n",
    "# fti = rf_reg.feature_importances_\n",
    "# feature_importance_df = pd.DataFrame({\n",
    "#     'feature': data_x.columns,\n",
    "#     'importance': fti\n",
    "# }).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "61aab0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_pca = train_df_pca[['pc_a_1', 'pc_a_2', 'pc_a_3']].reset_index()\n",
    "# test_df_pca = test_df_pca[['pc_a_1', 'pc_a_2', 'pc_a_3']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "57e2f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.merge(train_df, train_df_pca, how='left', on=_common.UNIQUE_KEY_COLS)\n",
    "# test_df = pd.merge(test_df, test_df_pca, how='left', on=_common.UNIQUE_KEY_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ecec3c",
   "metadata": {},
   "source": [
    "## 前処理済みのファイル出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7e72b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8f715c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../output/中間データ/学習用データ/train_preprocessed.csv', index=False)\n",
    "test_df.to_csv('../output/中間データ/評価用データ/test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1c808fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dfのみに存在: {'price_actual'}\n",
      "test_dfのみに存在: set()\n"
     ]
    }
   ],
   "source": [
    "# カラム名のセットを取得\n",
    "train_cols = set(train_df.columns)\n",
    "test_cols = set(test_df.columns)\n",
    "\n",
    "# どちらかにしかないカラム\n",
    "only_in_train = train_cols - test_cols\n",
    "only_in_test = test_cols - train_cols\n",
    "\n",
    "print(\"train_dfのみに存在:\", only_in_train)\n",
    "print(\"test_dfのみに存在:\", only_in_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
