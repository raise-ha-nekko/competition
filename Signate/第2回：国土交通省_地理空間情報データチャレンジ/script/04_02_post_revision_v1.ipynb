{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18fa7556",
   "metadata": {},
   "source": [
    "# 後補正・提出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7375f2",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b518e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの取り扱いに関するライブラリ\n",
    "import numpy as np # 高速計算\n",
    "import pandas as pd # 表データの扱い\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29fd42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自身がファイルを格納したディレクトリを指定\n",
    "ROOT_DIR = '../input/'\n",
    "intermediate_path = '../output/intermediate_file/'\n",
    "submit_file_path = ROOT_DIR + 'sample_submit.csv'\n",
    "model_path = '../output/model/'\n",
    "oof_path = '../output/oof/'\n",
    "pred_path = '../output/pred/'\n",
    "\n",
    "# スクリプトのバージョン指定\n",
    "training_ver = 8\n",
    "inference_ver = 8\n",
    "post_revision_ver = 1\n",
    "submit_ver = 2\n",
    "\n",
    "today = dt.datetime.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33bd4b4",
   "metadata": {},
   "source": [
    "## File Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae27961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_residential = pd.read_parquet(f'{intermediate_path}pred_df_residential_v{inference_ver}.parquet')\n",
    "test_df_house = pd.read_parquet(f'{intermediate_path}pred_df_house_v{inference_ver}.parquet')\n",
    "test_df_other = pd.read_parquet(f'{intermediate_path}pred_df_other_v{inference_ver}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655fab34",
   "metadata": {},
   "source": [
    "## 後補正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31fe0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df_residential = pd.read_csv(f'{oof_path}train_with_pred_residential_lgb_v{training_ver}.csv')\n",
    "# oof_df_house = pd.read_csv(f'{oof_path}train_with_pred_house_lgb_v{training_ver}.csv')\n",
    "# # oof_df_other = pd.read_csv(f'{oof_path}train_with_pred_other_lgb_v{training_ver}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985b182",
   "metadata": {},
   "source": [
    "#### Residential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7fbd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def mape(y, p):\n",
    "    y = np.asarray(y)\n",
    "    p = np.asarray(p)\n",
    "    mask = (y > 0) & np.isfinite(y) & np.isfinite(p) & (p > 0)\n",
    "    return float(np.mean(np.abs(y[mask] - p[mask]) / y[mask]))\n",
    "\n",
    "def fit_global_scale(df, y_col='money_room', pred_col='pred'):\n",
    "    d = df[[y_col, pred_col]].dropna()\n",
    "    d = d[(d[y_col] > 0) & (d[pred_col] > 0)]\n",
    "    return float(np.median(d[y_col] / d[pred_col]))\n",
    "\n",
    "def fit_group_scale_shrink(df, group_col, y_col='money_room', pred_col='pred', k=1000):\n",
    "    d = df[[group_col, y_col, pred_col]].dropna()\n",
    "    d = d[(d[y_col] > 0) & (d[pred_col] > 0)]\n",
    "    d = d.assign(ratio=d[y_col] / d[pred_col])\n",
    "\n",
    "    global_scale = float(np.median(d['ratio']))\n",
    "\n",
    "    g = (\n",
    "        d.groupby(group_col)['ratio']\n",
    "        .agg(n='size', raw_scale='median')\n",
    "        .reset_index()\n",
    "    )\n",
    "    g['w'] = g['n'] / (g['n'] + k)\n",
    "    g['final_scale'] = g['w'] * g['raw_scale'] + (1 - g['w']) * global_scale\n",
    "    scale_map = dict(zip(g[group_col], g['final_scale']))\n",
    "    return scale_map, global_scale, g\n",
    "\n",
    "def apply_scale(df, pred_col, scale_map, group_col=None, default_scale=1.0):\n",
    "    s = df[pred_col].to_numpy().copy()\n",
    "    if group_col is None:\n",
    "        return s * default_scale\n",
    "    scale = df[group_col].map(scale_map).fillna(default_scale).to_numpy()\n",
    "    return s * scale\n",
    "\n",
    "def prepare_long(df, y_col='money_room', oof_col='oof_pred', ho_col='ho_pred'):\n",
    "    # oof行\n",
    "    oof = df[df[oof_col].notna()].copy()\n",
    "    oof['pred'] = oof[oof_col]\n",
    "    oof['split'] = 'oof'\n",
    "    # ho行\n",
    "    ho = df[df[ho_col].notna()].copy()\n",
    "    ho['pred'] = ho[ho_col]\n",
    "    ho['split'] = 'ho'\n",
    "    return pd.concat([oof, ho], axis=0, ignore_index=True)\n",
    "\n",
    "def evaluate_all(df_long, group_col='structure_group', y_col='money_room'):\n",
    "    res = []\n",
    "\n",
    "    # baseline\n",
    "    for split in ['oof', 'ho']:\n",
    "        d = df_long[df_long['split'] == split]\n",
    "        res.append((f'baseline_{split}', mape(d[y_col], d['pred'])))\n",
    "\n",
    "    # A: global scale（oof+ho混ぜる版 / oofのみ版を比較しても良い）\n",
    "    gs_all = fit_global_scale(df_long, y_col=y_col, pred_col='pred')\n",
    "    for split in ['oof', 'ho']:\n",
    "        d = df_long[df_long['split'] == split]\n",
    "        p = d['pred'].to_numpy() * gs_all\n",
    "        res.append((f'global_allfit_{split}', mape(d[y_col], p)))\n",
    "\n",
    "    # B: structure shrinkage (k探索)\n",
    "    # scale推定は「oofだけでfit」→「hoで評価」が基本\n",
    "    df_fit = df_long[df_long['split'] == 'oof']\n",
    "    for k in [50, 100, 200, 500, 1000, 2000, 5000]:\n",
    "        scale_map, gs_fit, gtbl = fit_group_scale_shrink(df_fit, group_col=group_col, y_col=y_col, pred_col='pred', k=k)\n",
    "        for split in ['oof', 'ho']:\n",
    "            d = df_long[df_long['split'] == split]\n",
    "            p = apply_scale(d, 'pred', scale_map, group_col=group_col, default_scale=gs_fit)\n",
    "            res.append((f'structure_k{k}_fitOOF_{split}', mape(d[y_col], p)))\n",
    "\n",
    "    out = pd.DataFrame(res, columns=['method', 'mape']).sort_values('mape')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3be72788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        method      mape\n",
      "3             global_allfit_ho  0.127060\n",
      "5      structure_k50_fitOOF_ho  0.129303\n",
      "7     structure_k100_fitOOF_ho  0.129303\n",
      "9     structure_k200_fitOOF_ho  0.129305\n",
      "11    structure_k500_fitOOF_ho  0.129306\n",
      "13   structure_k1000_fitOOF_ho  0.129306\n",
      "15   structure_k2000_fitOOF_ho  0.129306\n",
      "17   structure_k5000_fitOOF_ho  0.129306\n",
      "1                  baseline_ho  0.129844\n",
      "0                 baseline_oof  0.138244\n",
      "4     structure_k50_fitOOF_oof  0.138571\n",
      "6    structure_k100_fitOOF_oof  0.138577\n",
      "8    structure_k200_fitOOF_oof  0.138581\n",
      "10   structure_k500_fitOOF_oof  0.138586\n",
      "12  structure_k1000_fitOOF_oof  0.138589\n",
      "14  structure_k2000_fitOOF_oof  0.138592\n",
      "16  structure_k5000_fitOOF_oof  0.138594\n",
      "2            global_allfit_oof  0.141198\n"
     ]
    }
   ],
   "source": [
    "df_long = prepare_long(oof_df_residential, y_col='money_room', oof_col='residential_lgb_oof_pred', ho_col='residential_lgb_ho_pred')\n",
    "result = evaluate_all(df_long, group_col='structure_group', y_col='money_room')\n",
    "print(result.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb2a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aab658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf7b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['money_room', 'residential_lgb_oof_pred']\n",
    "# oof_part = oof_df_residential[cols].rename(columns={'residential_lgb_oof_pred':'pred'})\n",
    "\n",
    "# ho_part = oof_df_residential[['money_room', 'residential_lgb_ho_pred']] \\\n",
    "#     .dropna(subset=['residential_lgb_ho_pred']) \\\n",
    "#     .rename(columns={'residential_lgb_ho_pred':'pred'})\n",
    "\n",
    "# df_all = pd.concat([oof_part, ho_part], axis=0)\n",
    "\n",
    "# mask = (df_all['pred'] > 0) & df_all['pred'].notna()\n",
    "# global_scale = np.median(df_all.loc[mask, 'money_room'] / df_all.loc[mask, 'pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9839b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_structure_scale(oof_df, k=1000):\n",
    "#     d = (\n",
    "#         oof_df\n",
    "#         .dropna(subset=['oof_pred'])\n",
    "#         .assign(ratio=lambda x: x['money_room']/x['oof_pred'])\n",
    "#         .groupby('structure_group')\n",
    "#         .agg(\n",
    "#             n=('ratio','size'),\n",
    "#             raw_scale=('ratio','median')\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     global_scale = np.median(oof_df['money_room'] / oof_df['oof_pred'])\n",
    "\n",
    "#     d['w'] = d['n'] / (d['n'] + k)\n",
    "#     d['final_scale'] = d['w'] * d['raw_scale'] + (1 - d['w']) * global_scale\n",
    "#     return d[['n','raw_scale','final_scale']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cfa3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_df_residential = compute_structure_scale(oof_df_residential)\n",
    "\n",
    "# def apply_structure_scale(pred_df, scale_df, default_scale):\n",
    "#     pred_df = pred_df.copy()\n",
    "#     scale_map = scale_df['final_scale'].to_dict()\n",
    "#     pred_df['scale'] = pred_df['structure_group'].map(scale_map).fillna(default_scale)\n",
    "#     pred_df['pred_adj'] = pred_df['pred'] * pred_df['scale']\n",
    "#     return pred_df\n",
    "\n",
    "# test_df_residential = apply_structure_scale(test_df_residential, scale_df_residential, global_scale)\n",
    "# test_df_residential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d6878",
   "metadata": {},
   "source": [
    "#### House"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808b014",
   "metadata": {},
   "source": [
    "#### Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d0a94",
   "metadata": {},
   "source": [
    "## 提出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f855f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pred と pred_log の整合確認（全行）\n",
    "# err = (test_df_residential['pred_log'] - np.log(test_df_residential['pred'])).abs()\n",
    "# print(err.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770f90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv(submit_file_path, header=None)\n",
    "submit_df.columns = ['id', 'pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16a1834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df['pred'] = np.nan\n",
    "\n",
    "for df_part in [\n",
    "    test_df_residential,\n",
    "    test_df_house,\n",
    "    test_df_other,\n",
    "]:\n",
    "    idx = submit_df.index.intersection(df_part.index)\n",
    "    submit_df.loc[idx, 'pred'] = df_part.loc[idx, 'pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a97b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN preds: 0 / 112437\n"
     ]
    }
   ],
   "source": [
    "n_nan = submit_df['pred'].isna().sum()\n",
    "print('NaN preds:', n_nan, '/', len(submit_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "477e4215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residential hit: 58834 part_rows: 58834\n",
      "house hit: 48594 part_rows: 48594\n",
      "other hit: 5009 part_rows: 5009\n"
     ]
    }
   ],
   "source": [
    "for name, df_part in [\n",
    "    ('residential', test_df_residential),\n",
    "    ('house', test_df_house),\n",
    "    ('other', test_df_other),\n",
    "]:\n",
    "    n_hit = len(submit_df.index.intersection(df_part.index))\n",
    "    print(name, 'hit:', n_hit, 'part_rows:', len(df_part))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d19c325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap r-h: 0\n",
      "overlap r-o: 0\n",
      "overlap h-o: 0\n"
     ]
    }
   ],
   "source": [
    "idx_r = test_df_residential.index\n",
    "idx_h = test_df_house.index\n",
    "idx_o = test_df_other.index\n",
    "\n",
    "print('overlap r-h:', len(idx_r.intersection(idx_h)))\n",
    "print('overlap r-o:', len(idx_r.intersection(idx_o)))\n",
    "print('overlap h-o:', len(idx_h.intersection(idx_o)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0560469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv(\n",
    "    f'{pred_path}submit_{today}_atohosei-v{post_revision_ver}_v{submit_ver}.csv',\n",
    "    index=False,\n",
    "    header=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
