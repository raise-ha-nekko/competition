{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4569e3",
   "metadata": {},
   "source": [
    "# モデル学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7375f2",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b518e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの取り扱いに関するライブラリ\n",
    "import numpy as np # 高速計算\n",
    "import pandas as pd # 表データの扱い\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "# 可視化に関するライブラリ\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fd42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自身がファイルを格納したディレクトリを指定\n",
    "ROOT_DIR = '../input/'\n",
    "data_definition_path = ROOT_DIR + 'data_definition.xlsx'\n",
    "intermediate_path = '../output/intermediate_file/'\n",
    "model_path = '../output/model/'\n",
    "oof_path = '../output/oof/'\n",
    "fi_path = '../output/fi/'\n",
    "\n",
    "# スクリプトのバージョン指定\n",
    "create_tbl_ver = 2\n",
    "training_ver = 7\n",
    "\n",
    "\n",
    "today = dt.datetime.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafd1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = 'residential'\n",
    "# target_model = 'house'\n",
    "# target_model = 'other'\n",
    "\n",
    "alg = 'lgb'\n",
    "# alg = 'cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33bd4b4",
   "metadata": {},
   "source": [
    "## File Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae27961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(f'{intermediate_path}train_df_{target_model}_v{create_tbl_ver}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63cbbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = 'target_ym'\n",
    "target_col = 'money_room'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5978b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_cols = train_df.columns.to_list()\n",
    "\n",
    "idx_key_cols = [\n",
    "    'Prefecture name',\n",
    "    'City/town/village name',\n",
    "    'zone_residential_rank'\n",
    "]\n",
    "drop_cols = set([target_col] + idx_key_cols)\n",
    "\n",
    "fe_cols = [c for c in fe_cols if c not in drop_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec8687",
   "metadata": {},
   "source": [
    "#### カテゴリ型へ変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7841b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_candidate = ['building_category', 'land_area_kind', 'walk_distance_bin', 'building_land_chimoku',\n",
    "            'land_chisei','land_road_cond', 'access_zone', 'fireproof_x_structure', 'structure_group'\n",
    "]\n",
    "\n",
    "cat_cols = [c for c in cat_cols_candidate if c in fe_cols]\n",
    "train_df[cat_cols] = train_df[cat_cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44012809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべての category 列のリスト\n",
    "obj_cols = train_df[fe_cols].select_dtypes(['object']).columns.tolist()\n",
    "train_df[obj_cols] = train_df[obj_cols].astype('category')\n",
    "cat_cols += obj_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a9b4c",
   "metadata": {},
   "source": [
    "## データ分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe379a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 東京23区 ---\n",
    "TOKYO_23 = [\n",
    "    '千代田区', '中央区', '港区', '新宿区', '文京区', '台東区',\n",
    "    '墨田区', '江東区', '品川区', '目黒区', '大田区', '世田谷区',\n",
    "    '渋谷区', '中野区', '杉並区', '豊島区', '北区', '荒川区',\n",
    "    '板橋区', '練馬区', '足立区', '葛飾区', '江戸川区'\n",
    "]\n",
    "\n",
    "# --- 政令指定都市 ---\n",
    "SEIREI_CITIES = [\n",
    "    '札幌市', '仙台市', 'さいたま市', '千葉市', '横浜市', '川崎市', '相模原市',\n",
    "    '新潟市', '静岡市', '浜松市', '名古屋市',\n",
    "    '京都市', '大阪市', '堺市', '神戸市',\n",
    "    '岡山市', '広島市', '北九州市', '福岡市', '熊本市'\n",
    "]\n",
    "\n",
    "# --- 首都圏（都道府県） ---\n",
    "CAPITAL_PREFS = ['東京都', '神奈川県', '埼玉県', '千葉県']\n",
    "\n",
    "# --- 県庁所在地（市名のみ） ---\n",
    "PREF_CAPITALS = [\n",
    "    '札幌市','青森市','盛岡市','仙台市','秋田市','山形市','福島市',\n",
    "    '水戸市','宇都宮市','前橋市','さいたま市','千葉市','新宿区',\n",
    "    '横浜市','新潟市','富山市','金沢市','福井市','甲府市','長野市',\n",
    "    '岐阜市','静岡市','名古屋市','津市','大津市','京都市','大阪市',\n",
    "    '神戸市','奈良市','和歌山市','鳥取市','松江市','岡山市','広島市',\n",
    "    '山口市','徳島市','高松市','松山市','高知市','福岡市','佐賀市',\n",
    "    '長崎市','熊本市','大分市','宮崎市','鹿児島市','那覇市'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3ffaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_city = train_df.index[\n",
    "    (\n",
    "        (train_df['Prefecture_name'] == '東京都') &\n",
    "        (train_df['City/town/village_name'].isin(TOKYO_23))\n",
    "    )\n",
    "    |\n",
    "    (train_df['City/town/village_name'].isin(['大阪市', '名古屋市']))\n",
    "]\n",
    "\n",
    "mid_city = train_df.index[\n",
    "    (\n",
    "        # 首都圏（23区除外）\n",
    "        (\n",
    "            train_df['Prefecture_name'].isin(CAPITAL_PREFS)\n",
    "            &\n",
    "            ~(\n",
    "                (train_df['Prefecture_name'] == '東京都') &\n",
    "                (train_df['City/town/village_name'].isin(TOKYO_23))\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # 政令指定都市\n",
    "        (train_df['City/town/village_name'].isin(SEIREI_CITIES))\n",
    "        |\n",
    "        # 県庁所在地\n",
    "        (train_df['City/town/village_name'].isin(PREF_CAPITALS))\n",
    "    )\n",
    "    &\n",
    "    ~train_df.index.isin(main_city)\n",
    "]\n",
    "\n",
    "other = train_df.index[\n",
    "    ~train_df.index.isin(main_city)\n",
    "    &\n",
    "    ~train_df.index.isin(mid_city)\n",
    "]\n",
    "\n",
    "urban_idx_dict = {\n",
    "    'main_city': main_city,\n",
    "    'mid_city': mid_city,\n",
    "    'other': other,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea534a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_low_density = train_df.index[\n",
    "    train_df['zone_residential_rank'] == 1\n",
    "]\n",
    "\n",
    "idx_mid_density = train_df.index[\n",
    "    train_df['zone_residential_rank'] == 2\n",
    "]\n",
    "\n",
    "idx_high_density = train_df.index[\n",
    "    train_df['zone_residential_rank'].isin([3, 4, 0]) |\n",
    "    train_df['zone_residential_rank'].isna()\n",
    "]\n",
    "\n",
    "density_idx_dict = {\n",
    "    'low': idx_low_density,\n",
    "    'mid': idx_mid_density,\n",
    "    'high': idx_high_density,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60293231",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_model == 'house':\n",
    "    idx_dict = density_idx_dict\n",
    "elif target_model == 'residential':\n",
    "    idx_dict = urban_idx_dict\n",
    "elif target_model == 'other':\n",
    "    idx_dict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31789808",
   "metadata": {},
   "source": [
    "## モデル学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8b48b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_for_oof = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6e71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_col = 'target_year'\n",
    "\n",
    "# LightGBM のベースパラメータ（あなたの設定）\n",
    "base_params = {\n",
    "    'lgb': {\n",
    "        # 'objective': 'regression_l1',  # ← log-MAEを直接最適化\n",
    "        # 'metric': 'l1',\n",
    "        'min_child_samples': 20,       # houseはノイズが出やすい\n",
    "        'reg_alpha': 0.5,\n",
    "        'reg_lambda': 1.0,\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 100,\n",
    "        'max_depth': -1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    },\n",
    "    'cat': {\n",
    "        'loss_function': 'MAE',          # ← 重要（歪み抑制）\n",
    "        'iterations': 2000,\n",
    "        'learning_rate': 0.03,\n",
    "        'depth': 6,                      # 深くしすぎない\n",
    "        'l2_leaf_reg': 5,\n",
    "        'random_strength': 0.8,\n",
    "        'bagging_temperature': 0.5,\n",
    "        'eval_metric': 'MAE',\n",
    "        'early_stopping_rounds': 100,\n",
    "        'verbose': False\n",
    "    },\n",
    "    'enet': {\n",
    "        'alpha': 0.001,        # 小さめ\n",
    "        'l1_ratio': 0.2,       # Ridge寄り\n",
    "        'fit_intercept': True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1401e",
   "metadata": {},
   "source": [
    "#### 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21325420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    \"\"\"MAPE計算（%ではなくratio）。\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return np.mean(np.abs(y_true - y_pred) / np.clip(y_true, 1e-6, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42e4326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _infer_te_source_col(te_col: str) -> str:\n",
    "    # \"xxx_te\" -> \"xxx\"\n",
    "    return te_col[:-3] if te_col.endswith(\"_te\") else te_col\n",
    "\n",
    "def fit_target_encoding_map(\n",
    "    s_cat: pd.Series,\n",
    "    y: pd.Series,\n",
    "    smoothing: float = 50.0,\n",
    "    min_samples_leaf: int = 1,\n",
    ") -> tuple[pd.Series, float]:\n",
    "    \"\"\"\n",
    "    1列のカテゴリ s_cat をターゲット y で target encoding するための mapping を作る。\n",
    "    smoothing: 大きいほど全体平均に寄る（過学習防止）\n",
    "    \"\"\"\n",
    "    s_cat = s_cat.astype(\"object\")\n",
    "    y = y.astype(float)\n",
    "\n",
    "    prior = float(y.mean())\n",
    "\n",
    "    stats = (\n",
    "        pd.DataFrame({\"cat\": s_cat, \"y\": y})\n",
    "          .groupby(\"cat\")[\"y\"]\n",
    "          .agg([\"count\", \"mean\"])\n",
    "    )\n",
    "\n",
    "    # count が小さいカテゴリは prior に寄せる（smoothing + min_samples_leaf）\n",
    "    count = stats[\"count\"].astype(float)\n",
    "    mean  = stats[\"mean\"].astype(float)\n",
    "\n",
    "    # smoothing 係数（一般的な ridge-like smoothing）\n",
    "    # enc = (count*mean + smoothing*prior) / (count + smoothing)\n",
    "    enc = (count * mean + smoothing * prior) / (count + smoothing)\n",
    "\n",
    "    # さらに min_samples_leaf 未満は prior へ\n",
    "    if min_samples_leaf > 1:\n",
    "        enc[count < min_samples_leaf] = prior\n",
    "\n",
    "    return enc, prior\n",
    "\n",
    "def apply_target_encoding(\n",
    "    s_cat: pd.Series,\n",
    "    mapping: pd.Series,\n",
    "    prior: float,\n",
    ") -> pd.Series:\n",
    "    s_cat = s_cat.astype(\"object\")\n",
    "    return s_cat.map(mapping).fillna(prior).astype(float)\n",
    "\n",
    "def recompute_te_for_fold(\n",
    "    train_df: pd.DataFrame,\n",
    "    tr_idx: pd.Index,\n",
    "    apply_idx_list: list[pd.Index],\n",
    "    te_cols: list[str],\n",
    "    y_tr: pd.Series,\n",
    "    smoothing: float = 50.0,\n",
    "    min_samples_leaf: int = 1,\n",
    "    verbose: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    te_cols（例: eki_name1_te）の各列について、\n",
    "    学習fold（tr_idx）だけでTEをfitし、apply_idx_list の各Indexへ適用する。\n",
    "\n",
    "    返り値: {te_col: {\"src_col\":..., \"mapping\":..., \"prior\":...}} （必要なら保存用）\n",
    "    \"\"\"\n",
    "    te_meta = {}\n",
    "\n",
    "    for te_col in te_cols:\n",
    "        src_col = _infer_te_source_col(te_col)\n",
    "        if src_col not in train_df.columns:\n",
    "            if verbose:\n",
    "                print(f\"[WARN] TE元列が見つからないためスキップ: {te_col} (src={src_col})\")\n",
    "            continue\n",
    "\n",
    "        mapping, prior = fit_target_encoding_map(\n",
    "            train_df.loc[tr_idx, src_col],\n",
    "            y_tr.loc[tr_idx],\n",
    "            smoothing=smoothing,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "        )\n",
    "\n",
    "        # 各適用先（train/val/ho など）へ適用\n",
    "        for idx in apply_idx_list:\n",
    "            train_df.loc[idx, te_col] = apply_target_encoding(\n",
    "                train_df.loc[idx, src_col],\n",
    "                mapping,\n",
    "                prior\n",
    "            ).values\n",
    "\n",
    "        te_meta[te_col] = {\"src_col\": src_col, \"mapping\": mapping, \"prior\": prior}\n",
    "\n",
    "    return te_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7f3b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "def run_cv_by_separate(\n",
    "    train_df: pd.DataFrame,\n",
    "    base_cols: list[str],\n",
    "    target_col: str,\n",
    "    year_col: str,\n",
    "    base_params: dict,\n",
    "    alg: str,\n",
    "    idx_dict: Optional[dict[str, pd.Index]] = None,\n",
    "    n_splits: int = 5,\n",
    "    te_smoothing: float = 50.0,\n",
    "    te_min_samples_leaf: int = 1,\n",
    "    cat_cols: list[str] | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    （idx_dictがあれば）分割ごとに CV / 最終モデル学習 / HO 予測を行う\n",
    "    idx_dictがNoneなら全データを1分割(all)として実行する\n",
    "\n",
    "    追加対応:\n",
    "      - alg == 'enet' のとき cat_cols を除外し、Scaling(Pipeline) を入れる\n",
    "    \"\"\"\n",
    "\n",
    "    y = train_df[target_col].astype(float)\n",
    "    y_log = np.log(y)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    te_cols_base = [c for c in base_cols if c.endswith('_te')]\n",
    "\n",
    "    if idx_dict is None:\n",
    "        idx_dict = {'all': train_df.index}\n",
    "\n",
    "    bias_rows: list[dict] = []\n",
    "\n",
    "    # ===== enet 用の特徴量リスト（cat_cols を除外） =====\n",
    "    if alg == 'enet':\n",
    "        cat_cols_set = set(cat_cols or [])\n",
    "        base_cols_enet = [c for c in base_cols if c not in cat_cols_set]\n",
    "        if len(base_cols_enet) == 0:\n",
    "            raise ValueError('alg==\"enet\" ですが、cat_cols除外後に特徴量が0件になりました。')\n",
    "    else:\n",
    "        base_cols_enet = base_cols\n",
    "    # ===================================================\n",
    "\n",
    "    for split_key, urban_idx in idx_dict.items():\n",
    "        print(f'\\n==============================')\n",
    "        print(f' Split: {split_key}')\n",
    "        print(f'==============================')\n",
    "\n",
    "        idx_cv = urban_idx.intersection(train_df.index[train_df[year_col] <= 2021])\n",
    "        idx_ho = urban_idx.intersection(train_df.index[train_df[year_col] == 2022])\n",
    "\n",
    "        if len(idx_cv) == 0 or len(idx_ho) == 0:\n",
    "            print(f'Skip {split_key} (no data)')\n",
    "            continue\n",
    "\n",
    "        y_ho_log = np.log(y.loc[idx_ho])\n",
    "\n",
    "        print(f'CV rows: {len(idx_cv)}')\n",
    "        print(f'HO rows: {len(idx_ho)}')\n",
    "\n",
    "        oof_pred_log = pd.Series(np.nan, index=idx_cv, dtype=float)\n",
    "        ho_pred_log_accum = pd.Series(0.0, index=idx_ho, dtype=float)\n",
    "\n",
    "        ho_log_by_fold = []\n",
    "        fi_list = []\n",
    "\n",
    "        # enet のときだけ base_cols を差し替える\n",
    "        use_cols = base_cols_enet if alg == 'enet' else base_cols\n",
    "\n",
    "        X_cv = train_df.loc[idx_cv, use_cols]\n",
    "        y_cv_log = y_log.loc[idx_cv]\n",
    "        groups_cv = train_df.loc[idx_cv, 'building_id']\n",
    "\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "        for fold, (tr_pos, va_pos) in enumerate(gkf.split(X_cv, y_cv_log, groups_cv), 1):\n",
    "            print(f'[{split_key}] Fold {fold}')\n",
    "\n",
    "            tr_idx = idx_cv[tr_pos]\n",
    "            va_idx = idx_cv[va_pos]\n",
    "\n",
    "            # --- TE 再計算（TE列がある場合のみ）---\n",
    "            if te_cols_base:\n",
    "                recompute_te_for_fold(\n",
    "                    train_df=train_df,\n",
    "                    tr_idx=tr_idx,\n",
    "                    apply_idx_list=[tr_idx, va_idx, idx_ho],\n",
    "                    te_cols=te_cols_base,\n",
    "                    y_tr=y_log,\n",
    "                    smoothing=te_smoothing,\n",
    "                    min_samples_leaf=te_min_samples_leaf,\n",
    "                )\n",
    "\n",
    "            # --- 学習 ---\n",
    "            if alg == 'lgb':\n",
    "                model = lgb.LGBMRegressor(**base_params[alg])\n",
    "                model.fit(train_df.loc[tr_idx, base_cols], y_log.loc[tr_idx])\n",
    "\n",
    "            elif alg == 'cat':\n",
    "                # CatBoost は文字列化が必要になりがちなので、対象列だけ局所的に作る\n",
    "                if cat_cols:\n",
    "                    for c in cat_cols:\n",
    "                        train_df.loc[:, c] = train_df.loc[:, c].astype('string').fillna('NA')\n",
    "                model = CatBoostRegressor(**base_params[alg], cat_features=(cat_cols or []))\n",
    "                model.fit(train_df.loc[tr_idx, base_cols], y_log.loc[tr_idx])\n",
    "\n",
    "            elif alg == 'enet':\n",
    "                # enet: cat_cols は除外済み(use_cols)、スケーリング＋欠損補完を必ず入れる\n",
    "                model = Pipeline(\n",
    "                    steps=[\n",
    "                        ('imputer', SimpleImputer(strategy='median')),\n",
    "                        ('scaler', StandardScaler(with_mean=True)),\n",
    "                        ('enet', ElasticNet(**base_params[alg], random_state=42)),\n",
    "                    ]\n",
    "                )\n",
    "                model.fit(train_df.loc[tr_idx, use_cols], y_log.loc[tr_idx])\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Unknown alg: {alg}')\n",
    "\n",
    "            # --- OOF ---\n",
    "            va_pred_log = model.predict(train_df.loc[va_idx, use_cols if alg == \"enet\" else base_cols])\n",
    "            oof_pred_log.loc[va_idx] = va_pred_log\n",
    "\n",
    "            # --- HO ---\n",
    "            ho_pred_log = model.predict(train_df.loc[idx_ho, use_cols if alg == \"enet\" else base_cols])\n",
    "            ho_pred_log_accum += ho_pred_log / n_splits\n",
    "            ho_log_by_fold.append(pd.Series(ho_pred_log, index=idx_ho))\n",
    "\n",
    "            # --- FI ---\n",
    "            if alg == 'lgb':\n",
    "                fi_list.append(pd.DataFrame({\n",
    "                    'feature': base_cols,\n",
    "                    'importance': model.booster_.feature_importance(importance_type='gain'),\n",
    "                    'fold': fold,\n",
    "                    'urban': split_key,\n",
    "                }))\n",
    "            elif alg == 'cat':\n",
    "                fi_list.append(pd.DataFrame({\n",
    "                    'feature': base_cols,\n",
    "                    'importance': model.get_feature_importance(type='PredictionValuesChange'),\n",
    "                    'fold': fold,\n",
    "                    'urban': split_key,\n",
    "                }))\n",
    "            elif alg == 'enet':\n",
    "                # enet は係数を FI として保存（安全化）\n",
    "                coef = model.named_steps['enet'].coef_\n",
    "\n",
    "                # coef が 2次元の可能性に備える（念のため）\n",
    "                coef = np.asarray(coef).ravel()\n",
    "\n",
    "                n_feat = len(use_cols)\n",
    "                n_coef = len(coef)\n",
    "\n",
    "                if n_feat != n_coef:\n",
    "                    print(f'[WARN] enet FI skipped: n_feat={n_feat}, n_coef={n_coef} (split={split_key}, fold={fold})')\n",
    "                else:\n",
    "                    fi_list.append(pd.DataFrame({\n",
    "                        'feature': use_cols,\n",
    "                        'importance': np.abs(coef),\n",
    "                        'fold': fold,\n",
    "                        'urban': split_key,\n",
    "                    }))\n",
    "\n",
    "        ho_log_stack = pd.concat(ho_log_by_fold, axis=1)\n",
    "        ho_mu = ho_log_stack.mean(axis=1)\n",
    "        ho_sigma = ho_log_stack.std(axis=1)\n",
    "\n",
    "        print(f'[{split_key}] Final model training')\n",
    "\n",
    "        if te_cols_base:\n",
    "            recompute_te_for_fold(\n",
    "                train_df=train_df,\n",
    "                tr_idx=idx_cv,\n",
    "                apply_idx_list=[idx_cv, idx_ho],\n",
    "                te_cols=te_cols_base,\n",
    "                y_tr=y_log,\n",
    "                smoothing=te_smoothing,\n",
    "                min_samples_leaf=te_min_samples_leaf,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        if alg == 'lgb':\n",
    "            final_model = lgb.LGBMRegressor(**base_params[alg])\n",
    "            final_model.fit(train_df.loc[idx_cv, base_cols], y_log.loc[idx_cv])\n",
    "\n",
    "        elif alg == 'cat':\n",
    "            if cat_cols:\n",
    "                for c in cat_cols:\n",
    "                    train_df.loc[:, c] = train_df.loc[:, c].astype('string').fillna('NA')\n",
    "            final_model = CatBoostRegressor(**base_params[alg], cat_features=(cat_cols or []))\n",
    "            final_model.fit(train_df.loc[idx_cv, base_cols], y_log.loc[idx_cv])\n",
    "\n",
    "        elif alg == 'enet':\n",
    "            final_model = Pipeline(\n",
    "                steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='median')),\n",
    "                    ('scaler', StandardScaler(with_mean=True)),\n",
    "                    ('enet', ElasticNet(**base_params[alg], random_state=42)),\n",
    "                ]\n",
    "            )\n",
    "            final_model.fit(train_df.loc[idx_cv, use_cols], y_log.loc[idx_cv])\n",
    "\n",
    "        ho_pred_log_final = pd.Series(\n",
    "            final_model.predict(train_df.loc[idx_ho, use_cols if alg == \"enet\" else base_cols]),\n",
    "            index=idx_ho\n",
    "        )\n",
    "\n",
    "        ho_residual_log_final = (y_ho_log - ho_pred_log_final)\n",
    "        bias_ho_log_final = float(ho_residual_log_final.mean())\n",
    "\n",
    "        bias_rows.append({\n",
    "            'split': split_key,\n",
    "            'bias_ho_log_final': bias_ho_log_final,\n",
    "            'n_ho': int(len(idx_ho)),\n",
    "        })\n",
    "\n",
    "        results[split_key] = {\n",
    "            'idx_cv': idx_cv,\n",
    "            'idx_ho': idx_ho,\n",
    "            'y_ho_log': y_ho_log,\n",
    "            'oof_pred_log': oof_pred_log,\n",
    "            'ho_pred_log_cv_mean': ho_pred_log_accum,\n",
    "            'ho_pred_log_final': ho_pred_log_final,\n",
    "            'ho_residual_log_final': ho_residual_log_final,\n",
    "            'bias_ho_log_final': bias_ho_log_final,\n",
    "            'ho_mu': ho_mu,\n",
    "            'ho_sigma': ho_sigma,\n",
    "            'final_model': final_model,\n",
    "            'fi': pd.concat(fi_list, ignore_index=True) if fi_list else pd.DataFrame(),\n",
    "            'used_cols': use_cols,\n",
    "        }\n",
    "\n",
    "    bias_table = pd.DataFrame(bias_rows).sort_values('split').reset_index(drop=True)\n",
    "\n",
    "    return {\n",
    "        'results_by_split': results,\n",
    "        'bias_table_ho_log_final': bias_table,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453fb1d",
   "metadata": {},
   "source": [
    "#### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fc40576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " Split: main_city\n",
      "==============================\n",
      "CV rows: 31627\n",
      "HO rows: 11070\n",
      "[main_city] Fold 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34183\n",
      "[LightGBM] [Info] Number of data points in the train set: 25301, number of used features: 290\n",
      "[LightGBM] [Info] Start training from score 17.378418\n",
      "[main_city] Fold 2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34146\n",
      "[LightGBM] [Info] Number of data points in the train set: 25301, number of used features: 290\n",
      "[LightGBM] [Info] Start training from score 17.385160\n",
      "[main_city] Fold 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34149\n",
      "[LightGBM] [Info] Number of data points in the train set: 25302, number of used features: 292\n",
      "[LightGBM] [Info] Start training from score 17.384745\n",
      "[main_city] Fold 4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34156\n",
      "[LightGBM] [Info] Number of data points in the train set: 25302, number of used features: 292\n",
      "[LightGBM] [Info] Start training from score 17.379419\n",
      "[main_city] Fold 5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34147\n",
      "[LightGBM] [Info] Number of data points in the train set: 25302, number of used features: 291\n",
      "[LightGBM] [Info] Start training from score 17.380206\n",
      "[main_city] Final model training\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name1_te (src=eki_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name2_te (src=eki_name2)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name1_te (src=rosen_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name2_te (src=rosen_name2)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34383\n",
      "[LightGBM] [Info] Number of data points in the train set: 31627, number of used features: 293\n",
      "[LightGBM] [Info] Start training from score 17.381590\n",
      "\n",
      "==============================\n",
      " Split: mid_city\n",
      "==============================\n",
      "CV rows: 54265\n",
      "HO rows: 15471\n",
      "[mid_city] Fold 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35661\n",
      "[LightGBM] [Info] Number of data points in the train set: 43412, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score 16.846262\n",
      "[mid_city] Fold 2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35665\n",
      "[LightGBM] [Info] Number of data points in the train set: 43412, number of used features: 302\n",
      "[LightGBM] [Info] Start training from score 16.844383\n",
      "[mid_city] Fold 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35689\n",
      "[LightGBM] [Info] Number of data points in the train set: 43412, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score 16.841980\n",
      "[mid_city] Fold 4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35657\n",
      "[LightGBM] [Info] Number of data points in the train set: 43412, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score 16.846852\n",
      "[mid_city] Fold 5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35687\n",
      "[LightGBM] [Info] Number of data points in the train set: 43412, number of used features: 301\n",
      "[LightGBM] [Info] Start training from score 16.846183\n",
      "[mid_city] Final model training\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name1_te (src=eki_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name2_te (src=eki_name2)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name1_te (src=rosen_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name2_te (src=rosen_name2)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35871\n",
      "[LightGBM] [Info] Number of data points in the train set: 54265, number of used features: 302\n",
      "[LightGBM] [Info] Start training from score 16.845132\n",
      "\n",
      "==============================\n",
      " Split: other\n",
      "==============================\n",
      "CV rows: 62062\n",
      "HO rows: 20659\n",
      "[other] Fold 1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36476\n",
      "[LightGBM] [Info] Number of data points in the train set: 49649, number of used features: 306\n",
      "[LightGBM] [Info] Start training from score 16.707063\n",
      "[other] Fold 2\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36457\n",
      "[LightGBM] [Info] Number of data points in the train set: 49649, number of used features: 306\n",
      "[LightGBM] [Info] Start training from score 16.706766\n",
      "[other] Fold 3\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36468\n",
      "[LightGBM] [Info] Number of data points in the train set: 49650, number of used features: 306\n",
      "[LightGBM] [Info] Start training from score 16.709578\n",
      "[other] Fold 4\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36440\n",
      "[LightGBM] [Info] Number of data points in the train set: 49650, number of used features: 306\n",
      "[LightGBM] [Info] Start training from score 16.699632\n",
      "[other] Fold 5\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36447\n",
      "[LightGBM] [Info] Number of data points in the train set: 49650, number of used features: 306\n",
      "[LightGBM] [Info] Start training from score 16.699848\n",
      "[other] Final model training\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name1_te (src=eki_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name2_te (src=eki_name2)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name1_te (src=rosen_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name2_te (src=rosen_name2)\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36626\n",
      "[LightGBM] [Info] Number of data points in the train set: 62062, number of used features: 306\n",
      "[LightGBM] [Info] Start training from score 16.704577\n"
     ]
    }
   ],
   "source": [
    "out_lgb = run_cv_by_separate(\n",
    "    train_df=train_df_for_oof,\n",
    "    base_cols=fe_cols,\n",
    "    target_col=target_col,\n",
    "    year_col=year_col,\n",
    "    alg=alg,\n",
    "    base_params=base_params,\n",
    "    idx_dict=idx_dict,\n",
    "    cat_cols=cat_cols,\n",
    ")\n",
    "\n",
    "results_lgb = out_lgb['results_by_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e013583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "\n",
    "# X_oof = pd.concat([\n",
    "#     pd.concat([r['oof_pred_log'] for r in results_lgb.values()]),\n",
    "#     pd.concat([r['oof_pred_log'] for r in results_cat.values()]),\n",
    "#     pd.concat([r['oof_pred_log'] for r in results_enet.values()]),\n",
    "# ], axis=1).sort_index()\n",
    "\n",
    "# X_oof.columns = ['lgb', 'cat', 'enet']\n",
    "\n",
    "# y_oof_log = np.log(train_df_for_oof.loc[X_oof.index, target_col])\n",
    "\n",
    "# ridge = Ridge(alpha=1.0, fit_intercept=False)\n",
    "# ridge.fit(X_oof, y_oof_log)\n",
    "\n",
    "# print(dict(zip(X_oof.columns, ridge.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3cbcdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_lgb, w_cat, w_enet = ridge.coef_  # fit_intercept=False の場合\n",
    "# # intercept があるなら別途\n",
    "\n",
    "# ho_log = (\n",
    "#     w_lgb * pd.concat([r['ho_pred_log_final'] for r in results_lgb.values()]) +\n",
    "#     w_cat * pd.concat([r['ho_pred_log_final'] for r in results_cat.values()]) +\n",
    "#     w_enet * pd.concat([r['ho_pred_log_final'] for r in results_enet.values()])\n",
    "# ).sort_index()\n",
    "\n",
    "# y_ho = train_df_for_oof.loc[ho_log.index, target_col]\n",
    "# print('HO MAPE (stacked):', mape(y_ho, np.exp(ho_log)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bce9a2",
   "metadata": {},
   "source": [
    "#### 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ece09c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def fit_low_calibrator_by_effective_land_price(\n",
    "    oof_df: pd.DataFrame,\n",
    "    y_col: str,\n",
    "    pred_log_col: str,\n",
    "    elp_col: str = 'effective_land_price',\n",
    "    n_bins: int = 20,\n",
    "    min_bin: int = 200,\n",
    "    low_q: float = 0.2,\n",
    "):\n",
    "    d = oof_df[[y_col, pred_log_col, elp_col]].copy()\n",
    "    mask = d[y_col].notna() & d[pred_log_col].notna() & d[elp_col].notna() & (d[y_col] > 0)\n",
    "    d = d.loc[mask].copy()\n",
    "\n",
    "    low_th = float(d[y_col].quantile(low_q))\n",
    "    d_low = d[d[y_col] <= low_th].copy()\n",
    "\n",
    "    y = d_low[y_col].astype('float64').to_numpy()\n",
    "    pred = np.exp(d_low[pred_log_col].astype('float64').to_numpy())\n",
    "    elp = d_low[elp_col].astype('float64').to_numpy()\n",
    "\n",
    "    ratio = y / np.clip(pred, 1e-12, None)\n",
    "\n",
    "    qs = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    edges = np.unique(np.nanquantile(elp, qs))\n",
    "    bins = pd.IntervalIndex.from_breaks(edges, closed='right')\n",
    "    b = pd.cut(elp, bins=bins)\n",
    "\n",
    "    s = pd.Series(ratio)\n",
    "    med = s.groupby(b).median()\n",
    "    cnt = s.groupby(b).size()\n",
    "\n",
    "    global_med = float(np.nanmedian(ratio))\n",
    "    global_log_delta = float(np.log(np.clip(global_med, 1e-12, None)))\n",
    "\n",
    "    bin_log_delta = np.full(len(bins), global_log_delta, dtype='float64')\n",
    "    for i, iv in enumerate(bins):\n",
    "        c = int(cnt.get(iv, 0))\n",
    "        m = float(med.get(iv, np.nan))\n",
    "        if (c >= min_bin) and np.isfinite(m):\n",
    "            bin_log_delta[i] = float(np.log(np.clip(m, 1e-12, None)))\n",
    "\n",
    "    return {\n",
    "        'elp_col': elp_col,\n",
    "        'edges': edges,\n",
    "        'bin_log_delta': bin_log_delta,\n",
    "        'global_log_delta': global_log_delta,\n",
    "        'low_q': low_q,\n",
    "        'low_th': low_th,\n",
    "    }\n",
    "\n",
    "def _lookup_bin_log_delta(elp: np.ndarray, edges: np.ndarray, bin_log_delta: np.ndarray, global_log_delta: float):\n",
    "    idx = np.digitize(elp, edges, right=True) - 1\n",
    "    idx = np.clip(idx, 0, len(edges) - 2)\n",
    "    out = bin_log_delta[idx]\n",
    "    out = np.where(np.isfinite(elp), out, global_log_delta)\n",
    "    return out\n",
    "\n",
    "def apply_low_gate_and_calibration_log(\n",
    "    df: pd.DataFrame,\n",
    "    base_pred_log: np.ndarray,\n",
    "    calibrator: dict,\n",
    "    gate_q_lo: float = 0.5,\n",
    "    gate_q_hi: float = 0.8,\n",
    "    k: float = 10.0,\n",
    "):\n",
    "    elp = df[calibrator['elp_col']].astype('float64').to_numpy()\n",
    "    q_lo = float(np.nanquantile(elp, gate_q_lo))\n",
    "    q_hi = float(np.nanquantile(elp, gate_q_hi))\n",
    "    denom = max(q_hi - q_lo, 1e-6)\n",
    "\n",
    "    z = (elp - q_lo) / denom\n",
    "    w = _sigmoid(k * (z - 0.5))  # elpが高いほど補正を強く\n",
    "\n",
    "    # is_low_like = (\n",
    "    #     (df['listing_months_log'] >= df['listing_months_log'].quantile(0.7)) &\n",
    "    #     (df['livability_score'] <= df['livability_score'].quantile(0.6))\n",
    "    # )\n",
    "\n",
    "    is_low_like = (\n",
    "        (df['effective_land_price_div_median_price_1000m'] >= df['effective_land_price_div_median_price_1000m'].quantile(0.7)) &\n",
    "        (df['livability_score'] <= df['livability_score'].quantile(0.6))\n",
    "    )\n",
    "\n",
    "\n",
    "    w = w * is_low_like.astype('float64')\n",
    "\n",
    "    add_log = _lookup_bin_log_delta(\n",
    "        elp,\n",
    "        calibrator['edges'],\n",
    "        calibrator['bin_log_delta'],\n",
    "        calibrator['global_log_delta'],\n",
    "    )\n",
    "\n",
    "    base_pred_log = np.asarray(base_pred_log, dtype='float64')\n",
    "    calibrated_log = base_pred_log + add_log\n",
    "    final_pred_log = base_pred_log * (1.0 - w) + calibrated_log * w\n",
    "    return final_pred_log, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4537e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_results(results, train_df, target_col, label):\n",
    "    print(f'\\n===== {label} =====')\n",
    "\n",
    "    print('--- OOF MAPE ---')\n",
    "    for urban_key, res in results.items():\n",
    "        idx_cv = res['idx_cv']\n",
    "        y_true = train_df.loc[idx_cv, target_col]\n",
    "        y_pred = np.exp(res['oof_pred_log'])\n",
    "        print(f'{urban_key:10s} | {mape(y_true, y_pred):.6f}')\n",
    "\n",
    "    print('--- HO MAPE ---')\n",
    "    for urban_key, res in results.items():\n",
    "        idx_ho = res['idx_ho']\n",
    "        y_true = train_df.loc[idx_ho, target_col]\n",
    "        y_pred = np.exp(res['ho_pred_log_final'])\n",
    "        print(f'{urban_key:10s} | {mape(y_true, y_pred):.6f}')\n",
    "\n",
    "    # all\n",
    "    oof_log_all = pd.concat([r['oof_pred_log'] for r in results.values()]).sort_index()\n",
    "    y_oof_all = train_df.loc[oof_log_all.index, target_col]\n",
    "    print(f'OOF MAPE (all): {mape(y_oof_all, np.exp(oof_log_all)):.6f}')\n",
    "\n",
    "    ho_log_all = pd.concat([r['ho_pred_log_final'] for r in results.values()]).sort_index()\n",
    "    y_ho_all = train_df.loc[ho_log_all.index, target_col]\n",
    "    print(f'HO  MAPE (all): {mape(y_ho_all, np.exp(ho_log_all)):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ff16c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LightGBM =====\n",
      "--- OOF MAPE ---\n",
      "main_city  | 0.121656\n",
      "mid_city   | 0.140458\n",
      "other      | 0.150801\n",
      "--- HO MAPE ---\n",
      "main_city  | 0.120234\n",
      "mid_city   | 0.139134\n",
      "other      | 0.129812\n",
      "OOF MAPE (all): 0.140778\n",
      "HO  MAPE (all): 0.130621\n"
     ]
    }
   ],
   "source": [
    "eval_results(results_lgb, train_df_for_oof, target_col, 'LightGBM')\n",
    "# eval_results(results_cat,  train_df_for_oof, target_col, 'CatBoost')\n",
    "# eval_results(results_enet, train_df_for_oof, target_col, 'ElasticNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57c444c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {\n",
    "#     'lgb':  0.55,\n",
    "#     'cat':  0.30,\n",
    "#     'enet': 0.15,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63545fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_ensemble_by_split(\n",
    "#     results_lgb,\n",
    "#     results_cat,\n",
    "#     results_enet,\n",
    "#     train_df,\n",
    "#     target_col,\n",
    "#     weights,\n",
    "# ):\n",
    "#     print('\\n===== Ensemble =====')\n",
    "\n",
    "#     print('--- OOF MAPE ---')\n",
    "#     for urban_key in results_lgb.keys():\n",
    "#         idx_cv = results_lgb[urban_key]['idx_cv']\n",
    "\n",
    "#         oof_log = (\n",
    "#             weights['lgb']  * results_lgb[urban_key]['oof_pred_log'] +\n",
    "#             weights['cat']  * results_cat[urban_key]['oof_pred_log'] +\n",
    "#             weights['enet'] * results_enet[urban_key]['oof_pred_log']\n",
    "#         )\n",
    "\n",
    "#         y_true = train_df.loc[idx_cv, target_col]\n",
    "#         y_pred = np.exp(oof_log)\n",
    "#         print(f'{urban_key:10s} | {mape(y_true, y_pred):.6f}')\n",
    "\n",
    "#     print('--- HO MAPE ---')\n",
    "#     for urban_key in results_lgb.keys():\n",
    "#         idx_ho = results_lgb[urban_key]['idx_ho']\n",
    "\n",
    "#         ho_log = (\n",
    "#             weights['lgb']  * results_lgb[urban_key]['ho_pred_log_final'] +\n",
    "#             weights['cat']  * results_cat[urban_key]['ho_pred_log_final'] +\n",
    "#             weights['enet'] * results_enet[urban_key]['ho_pred_log_final']\n",
    "#         )\n",
    "\n",
    "#         y_true = train_df.loc[idx_ho, target_col]\n",
    "#         y_pred = np.exp(ho_log)\n",
    "#         print(f'{urban_key:10s} | {mape(y_true, y_pred):.6f}')\n",
    "\n",
    "\n",
    "# def eval_ensemble_all(\n",
    "#     results_lgb,\n",
    "#     results_cat,\n",
    "#     results_enet,\n",
    "#     train_df,\n",
    "#     target_col,\n",
    "#     weights,\n",
    "# ):\n",
    "#     # --- OOF ---\n",
    "#     oof_log = (\n",
    "#         weights['lgb']  * pd.concat([r['oof_pred_log'] for r in results_lgb.values()]) +\n",
    "#         weights['cat']  * pd.concat([r['oof_pred_log'] for r in results_cat.values()]) +\n",
    "#         weights['enet'] * pd.concat([r['oof_pred_log'] for r in results_enet.values()])\n",
    "#     ).sort_index()\n",
    "\n",
    "#     y_oof = train_df.loc[oof_log.index, target_col]\n",
    "#     print(f'OOF MAPE (ensemble, all): {mape(y_oof, np.exp(oof_log)):.6f}')\n",
    "\n",
    "#     # --- HO ---\n",
    "#     ho_log = (\n",
    "#         weights['lgb']  * pd.concat([r['ho_pred_log_final'] for r in results_lgb.values()]) +\n",
    "#         weights['cat']  * pd.concat([r['ho_pred_log_final'] for r in results_cat.values()]) +\n",
    "#         weights['enet'] * pd.concat([r['ho_pred_log_final'] for r in results_enet.values()])\n",
    "#     ).sort_index()\n",
    "\n",
    "#     y_ho = train_df.loc[ho_log.index, target_col]\n",
    "#     print(f'HO  MAPE (ensemble, all): {mape(y_ho, np.exp(ho_log)):.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5361790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_ensemble_by_split(\n",
    "#     results_lgb,\n",
    "#     results_cat,\n",
    "#     results_enet,\n",
    "#     train_df_for_oof,\n",
    "#     target_col,\n",
    "#     weights,\n",
    "# )\n",
    "\n",
    "# eval_ensemble_all(\n",
    "#     results_lgb,\n",
    "#     results_cat,\n",
    "#     results_enet,\n",
    "#     train_df_for_oof,\n",
    "#     target_col,\n",
    "#     weights,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392257ac",
   "metadata": {},
   "source": [
    "#### 特徴量重要度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d2276f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_city\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>has_ldk</td>\n",
       "      <td>18737.204115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>log_weighted_land_price_3_x_livability_score</td>\n",
       "      <td>11814.949881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>senyu_area</td>\n",
       "      <td>7748.294436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>money_kyoueki_std</td>\n",
       "      <td>6466.409720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>log_weighted_land_price_3_x_age_decay_30</td>\n",
       "      <td>2288.777053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>log_land_price_x_livability_score</td>\n",
       "      <td>1872.668973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>year_built</td>\n",
       "      <td>1489.074419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>building_senyu_area_median</td>\n",
       "      <td>1443.141045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>wet_area_upgrade_count</td>\n",
       "      <td>1322.530986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City/town/village_name</td>\n",
       "      <td>1270.086350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>room_count</td>\n",
       "      <td>1164.747804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>senyu_area_log</td>\n",
       "      <td>1052.576951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>log_land_price_x_age_decay_30</td>\n",
       "      <td>1003.216631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>low_price_proxy</td>\n",
       "      <td>936.862436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>senyu_area_ratio_to_median</td>\n",
       "      <td>788.384280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>area_per_room_x_built_diff</td>\n",
       "      <td>731.958234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>mean_price_2000m_mansion_log</td>\n",
       "      <td>622.713748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>eki_name1_te</td>\n",
       "      <td>615.056608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>livability_score</td>\n",
       "      <td>614.720650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>unit_count</td>\n",
       "      <td>520.514184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          feature    importance\n",
       "86                                        has_ldk  18737.204115\n",
       "215  log_weighted_land_price_3_x_livability_score  11814.949881\n",
       "302                                    senyu_area   7748.294436\n",
       "248                             money_kyoueki_std   6466.409720\n",
       "211      log_weighted_land_price_3_x_age_decay_30   2288.777053\n",
       "209             log_land_price_x_livability_score   1872.668973\n",
       "359                                    year_built   1489.074419\n",
       "26                     building_senyu_area_median   1443.141045\n",
       "356                        wet_area_upgrade_count   1322.530986\n",
       "0                          City/town/village_name   1270.086350\n",
       "292                                    room_count   1164.747804\n",
       "303                                senyu_area_log   1052.576951\n",
       "205                 log_land_price_x_age_decay_30   1003.216631\n",
       "218                               low_price_proxy    936.862436\n",
       "306                    senyu_area_ratio_to_median    788.384280\n",
       "17                     area_per_room_x_built_diff    731.958234\n",
       "234                  mean_price_2000m_mansion_log    622.713748\n",
       "63                                   eki_name1_te    615.056608\n",
       "202                              livability_score    614.720650\n",
       "342                                    unit_count    520.514184"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid_city\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>log_weighted_land_price_3_x_age_decay_30</td>\n",
       "      <td>28020.154994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>log_land_price_x_age_decay_30</td>\n",
       "      <td>14655.026858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>log_weighted_land_price_3_x_livability_score</td>\n",
       "      <td>13839.162457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City/town/village_name</td>\n",
       "      <td>7327.796269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>has_ldk</td>\n",
       "      <td>6415.301390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>senyu_area</td>\n",
       "      <td>5423.895033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>premium_equipment_count</td>\n",
       "      <td>5080.761378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>log_land_price_x_livability_score</td>\n",
       "      <td>4019.958909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>mean_price_500m_mansion_log</td>\n",
       "      <td>2995.897241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>mean_price_1000m_mansion_log</td>\n",
       "      <td>2650.141231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>year_built</td>\n",
       "      <td>2109.104474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>mean_price_1000m_log</td>\n",
       "      <td>1811.939579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>senyu_area_ratio_to_median</td>\n",
       "      <td>1651.398690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>median_price_1000m_log</td>\n",
       "      <td>1452.589256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>money_kyoueki_std</td>\n",
       "      <td>1427.226701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>eki_name1_te</td>\n",
       "      <td>1338.719116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>median_price_2000m_log</td>\n",
       "      <td>1326.761265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>building_senyu_area_median</td>\n",
       "      <td>1292.673493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>mean_price_300m_mansion_log</td>\n",
       "      <td>1128.008555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>senyu_area_log</td>\n",
       "      <td>1043.184166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          feature    importance\n",
       "211      log_weighted_land_price_3_x_age_decay_30  28020.154994\n",
       "205                 log_land_price_x_age_decay_30  14655.026858\n",
       "215  log_weighted_land_price_3_x_livability_score  13839.162457\n",
       "0                          City/town/village_name   7327.796269\n",
       "86                                        has_ldk   6415.301390\n",
       "302                                    senyu_area   5423.895033\n",
       "268                       premium_equipment_count   5080.761378\n",
       "209             log_land_price_x_livability_score   4019.958909\n",
       "238                   mean_price_500m_mansion_log   2995.897241\n",
       "232                  mean_price_1000m_mansion_log   2650.141231\n",
       "359                                    year_built   2109.104474\n",
       "231                          mean_price_1000m_log   1811.939579\n",
       "306                    senyu_area_ratio_to_median   1651.398690\n",
       "239                        median_price_1000m_log   1452.589256\n",
       "248                             money_kyoueki_std   1427.226701\n",
       "63                                   eki_name1_te   1338.719116\n",
       "240                        median_price_2000m_log   1326.761265\n",
       "26                     building_senyu_area_median   1292.673493\n",
       "236                   mean_price_300m_mansion_log   1128.008555\n",
       "303                                senyu_area_log   1043.184166"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>log_weighted_land_price_3_x_age_decay_30</td>\n",
       "      <td>45504.278106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>log_land_price_x_age_decay_30</td>\n",
       "      <td>15199.736111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>log_weighted_land_price_3_x_livability_score</td>\n",
       "      <td>13394.109102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City/town/village_name</td>\n",
       "      <td>11380.676344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>has_ldk</td>\n",
       "      <td>10872.404782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>low_price_proxy</td>\n",
       "      <td>5546.672580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>premium_equipment_count</td>\n",
       "      <td>5184.110216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>senyu_area</td>\n",
       "      <td>5175.457046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>log_land_price_x_livability_score</td>\n",
       "      <td>4975.039903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>money_kyoueki_std</td>\n",
       "      <td>3202.413422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>senyu_area_ratio_to_median</td>\n",
       "      <td>2539.041009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>mean_price_2000m_mansion_log</td>\n",
       "      <td>2150.170576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>eki_name1_te</td>\n",
       "      <td>2049.728506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>senyu_area_log</td>\n",
       "      <td>1787.455500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>livability_score</td>\n",
       "      <td>1635.496438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>building_senyu_area_median</td>\n",
       "      <td>1411.675136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>mean_price_1000m_mansion_log</td>\n",
       "      <td>1214.701381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>room_floor</td>\n",
       "      <td>1067.228420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>room_count</td>\n",
       "      <td>1021.172085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>balcony_area</td>\n",
       "      <td>862.440889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          feature    importance\n",
       "211      log_weighted_land_price_3_x_age_decay_30  45504.278106\n",
       "205                 log_land_price_x_age_decay_30  15199.736111\n",
       "215  log_weighted_land_price_3_x_livability_score  13394.109102\n",
       "0                          City/town/village_name  11380.676344\n",
       "86                                        has_ldk  10872.404782\n",
       "218                               low_price_proxy   5546.672580\n",
       "268                       premium_equipment_count   5184.110216\n",
       "302                                    senyu_area   5175.457046\n",
       "209             log_land_price_x_livability_score   4975.039903\n",
       "248                             money_kyoueki_std   3202.413422\n",
       "306                    senyu_area_ratio_to_median   2539.041009\n",
       "234                  mean_price_2000m_mansion_log   2150.170576\n",
       "63                                   eki_name1_te   2049.728506\n",
       "303                                senyu_area_log   1787.455500\n",
       "202                              livability_score   1635.496438\n",
       "26                     building_senyu_area_median   1411.675136\n",
       "232                  mean_price_1000m_mansion_log   1214.701381\n",
       "293                                    room_floor   1067.228420\n",
       "292                                    room_count   1021.172085\n",
       "19                                   balcony_area    862.440889"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fi_dict = {}\n",
    "\n",
    "for key, res in results_lgb.items():\n",
    "    fi_df_raw = res['fi']  # ← これは DataFrame\n",
    "\n",
    "    fi_df = (\n",
    "        fi_df_raw\n",
    "        .groupby('feature', as_index=False)['importance']\n",
    "        .mean()\n",
    "        .sort_values('importance', ascending=False)\n",
    "    )\n",
    "\n",
    "    fi_dict[key] = fi_df\n",
    "    print(key)\n",
    "    display(fi_dict[key].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bd1568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_all = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            res['fi'].assign(key=key)\n",
    "            for key, res in results_lgb.items()\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    .groupby('feature', as_index=False)['importance']\n",
    "    .mean()\n",
    "    .sort_values('importance', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec33d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_all.to_csv(f'{fi_path}feature_importance_{target_model}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb4e23",
   "metadata": {},
   "source": [
    "#### 特徴量重要度==0を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0625e263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 121 features\n"
     ]
    }
   ],
   "source": [
    "# excluded_fe_cols = fi_all.query('importance == 0')['feature'].tolist()\n",
    "excluded_fe_cols = fi_all.query('importance <= 1')['feature'].tolist()\n",
    "\n",
    "print(f'Removed {len(excluded_fe_cols)} features')\n",
    "\n",
    "fe_cols_filtered = [c for c in fe_cols if c not in excluded_fe_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f15e1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_filtered = [c for c in cat_cols if c not in excluded_fe_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0bda81",
   "metadata": {},
   "source": [
    "#### 特徴量選択して再学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdb914c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " Split: main_city\n",
      "==============================\n",
      "CV rows: 31627\n",
      "HO rows: 11070\n",
      "[main_city] Fold 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33881\n",
      "[LightGBM] [Info] Number of data points in the train set: 25301, number of used features: 237\n",
      "[LightGBM] [Info] Start training from score 17.378418\n",
      "[main_city] Fold 2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33854\n",
      "[LightGBM] [Info] Number of data points in the train set: 25301, number of used features: 239\n",
      "[LightGBM] [Info] Start training from score 17.385160\n",
      "[main_city] Fold 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33850\n",
      "[LightGBM] [Info] Number of data points in the train set: 25302, number of used features: 239\n",
      "[LightGBM] [Info] Start training from score 17.384745\n",
      "[main_city] Fold 4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33851\n",
      "[LightGBM] [Info] Number of data points in the train set: 25302, number of used features: 239\n",
      "[LightGBM] [Info] Start training from score 17.379419\n",
      "[main_city] Fold 5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 33845\n",
      "[LightGBM] [Info] Number of data points in the train set: 25302, number of used features: 239\n",
      "[LightGBM] [Info] Start training from score 17.380206\n",
      "[main_city] Final model training\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name1_te (src=eki_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name2_te (src=eki_name2)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name1_te (src=rosen_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name2_te (src=rosen_name2)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34028\n",
      "[LightGBM] [Info] Number of data points in the train set: 31627, number of used features: 239\n",
      "[LightGBM] [Info] Start training from score 17.381590\n",
      "\n",
      "==============================\n",
      " Split: mid_city\n",
      "==============================\n",
      "CV rows: 54265\n",
      "HO rows: 15471\n",
      "[mid_city] Fold 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35226\n",
      "[LightGBM] [Info] Number of data points in the train set: 43412, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.846262\n",
      "[mid_city] Fold 2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35225\n",
      "[LightGBM] [Info] Number of data points in the train set: 43412, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.844383\n",
      "[mid_city] Fold 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35249\n",
      "[LightGBM] [Info] Number of data points in the train set: 43412, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.841980\n",
      "[mid_city] Fold 4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35211\n",
      "[LightGBM] [Info] Number of data points in the train set: 43412, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.846852\n",
      "[mid_city] Fold 5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35242\n",
      "[LightGBM] [Info] Number of data points in the train set: 43412, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.846183\n",
      "[mid_city] Final model training\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name1_te (src=eki_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name2_te (src=eki_name2)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name1_te (src=rosen_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name2_te (src=rosen_name2)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 35358\n",
      "[LightGBM] [Info] Number of data points in the train set: 54265, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.845132\n",
      "\n",
      "==============================\n",
      " Split: other\n",
      "==============================\n",
      "CV rows: 62062\n",
      "HO rows: 20659\n",
      "[other] Fold 1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36174\n",
      "[LightGBM] [Info] Number of data points in the train set: 49649, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.707063\n",
      "[other] Fold 2\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36145\n",
      "[LightGBM] [Info] Number of data points in the train set: 49649, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.706766\n",
      "[other] Fold 3\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36171\n",
      "[LightGBM] [Info] Number of data points in the train set: 49650, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.709578\n",
      "[other] Fold 4\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36154\n",
      "[LightGBM] [Info] Number of data points in the train set: 49650, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.699632\n",
      "[other] Fold 5\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36148\n",
      "[LightGBM] [Info] Number of data points in the train set: 49650, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.699848\n",
      "[other] Final model training\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name1_te (src=eki_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: eki_name2_te (src=eki_name2)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name1_te (src=rosen_name1)\n",
      "[WARN] TE元列が見つからないためスキップ: rosen_name2_te (src=rosen_name2)\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36294\n",
      "[LightGBM] [Info] Number of data points in the train set: 62062, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.704577\n"
     ]
    }
   ],
   "source": [
    "out_lgb_re = run_cv_by_separate(\n",
    "    train_df=train_df_for_oof,\n",
    "    base_cols=fe_cols_filtered,\n",
    "    target_col=target_col,\n",
    "    year_col=year_col,\n",
    "    alg=alg,\n",
    "    base_params=base_params,\n",
    "    idx_dict=idx_dict,\n",
    "    cat_cols=cat_cols_filtered,\n",
    ")\n",
    "\n",
    "results_lgb_re = out_lgb_re['results_by_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c3ae4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LightGBM（residential） =====\n",
      "--- OOF MAPE ---\n",
      "main_city  | 0.121730\n",
      "mid_city   | 0.140349\n",
      "other      | 0.150698\n",
      "--- HO MAPE ---\n",
      "main_city  | 0.118958\n",
      "mid_city   | 0.138846\n",
      "other      | 0.129870\n",
      "OOF MAPE (all): 0.140710\n",
      "HO  MAPE (all): 0.130253\n"
     ]
    }
   ],
   "source": [
    "eval_results(results_lgb_re,  train_df_for_oof, target_col, f'LightGBM（{target_model}）')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ffdfef",
   "metadata": {},
   "source": [
    "#### 学習データの予測結果出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53e88256",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_log_all = pd.concat([r['oof_pred_log'] for r in results_lgb_re.values()]).sort_index()\n",
    "ho_log_all = pd.concat([r['ho_pred_log_final'] for r in results_lgb_re.values()]).sort_index()\n",
    "\n",
    "oof_pred_all = np.exp(oof_log_all)\n",
    "ho_pred_all = np.exp(ho_log_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ca06c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) 予測結果を格納する DataFrame\n",
    "# ============================================================\n",
    "train_result_df = train_df_for_oof[fe_cols + [target_col]].copy()\n",
    "\n",
    "train_result_df['oof_pred'] = np.nan\n",
    "train_result_df['ho_pred'] = np.nan\n",
    "\n",
    "# --- HO のマスク（2022年） ---\n",
    "mask_ho_all = train_result_df['target_ym'].astype(str).str.startswith('2022')\n",
    "\n",
    "# index が完全一致するのでそのまま代入可\n",
    "train_result_df.loc[~mask_ho_all,     'oof_pred'] = oof_pred_all\n",
    "train_result_df.loc[mask_ho_all,      'ho_pred'] = ho_pred_all\n",
    "\n",
    "# 保存\n",
    "train_result_df.to_csv(f'{oof_path}oof_{target_model}_{today}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b31d2a",
   "metadata": {},
   "source": [
    "## 最終モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9941fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all     = train_df[target_col].astype(float)\n",
    "y_all_log = np.log(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68208777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "\n",
    "def train_final_models_by_split(\n",
    "    train_df: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    y_log: pd.Series,\n",
    "    alg: str,\n",
    "    params: dict,\n",
    "    idx_dict: Optional[dict[str, pd.Index]] = None,\n",
    "    cat_cols: Optional[list[str]] = None,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    splitごとに最終モデルを学習して返す。\n",
    "    idx_dict=None の場合は全データを 'all' として1モデル学習。\n",
    "    \"\"\"\n",
    "\n",
    "    models: dict[str, Any] = {}\n",
    "\n",
    "    # ===== Noneフォールバック =====\n",
    "    if idx_dict is None:\n",
    "        idx_dict = {'all': train_df.index}\n",
    "\n",
    "    # ===== cat_cols: 存在する列だけ使う =====\n",
    "    cat_cols_exist: list[str] = []\n",
    "    if cat_cols:\n",
    "        cat_cols_exist = [c for c in cat_cols if c in train_df.columns]\n",
    "\n",
    "    for split_key, split_idx in idx_dict.items():\n",
    "        if split_idx is None or len(split_idx) == 0:\n",
    "            print(f'Skip {split_key} (empty split_idx)')\n",
    "            continue\n",
    "\n",
    "        idx_use = pd.Index(split_idx).intersection(train_df.index)\n",
    "\n",
    "        if len(idx_use) == 0:\n",
    "            print(f'Skip {split_key} (no rows after intersection)')\n",
    "            continue\n",
    "\n",
    "        print(f'=== Final model training: {split_key} | rows={len(idx_use)} ===')\n",
    "\n",
    "        X_tr = train_df.loc[idx_use, feature_cols]\n",
    "        y_tr = y_log.loc[idx_use]\n",
    "\n",
    "        if alg == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params[alg])\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "        elif alg == 'cat':\n",
    "            model = CatBoostRegressor(**params[alg], cat_features=cat_cols_exist)\n",
    "\n",
    "            # CatBoostはカテゴリ列を文字列化しておくのが安全（既存方針に合わせる）\n",
    "            if cat_cols_exist:\n",
    "                X_tr = X_tr.copy()\n",
    "                for c in cat_cols_exist:\n",
    "                    X_tr[c] = X_tr[c].astype('string').fillna('NA')\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'Unknown alg: {alg}')\n",
    "\n",
    "        models[split_key] = model\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c7da655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final model training: main_city | rows=42697 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34187\n",
      "[LightGBM] [Info] Number of data points in the train set: 42697, number of used features: 241\n",
      "[LightGBM] [Info] Start training from score 17.408726\n",
      "=== Final model training: mid_city | rows=69736 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35655\n",
      "[LightGBM] [Info] Number of data points in the train set: 69736, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.867233\n",
      "=== Final model training: other | rows=82721 ===\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36549\n",
      "[LightGBM] [Info] Number of data points in the train set: 82721, number of used features: 246\n",
      "[LightGBM] [Info] Start training from score 16.722887\n"
     ]
    }
   ],
   "source": [
    "final_models = train_final_models_by_split(\n",
    "    train_df=train_df,\n",
    "    feature_cols=fe_cols_filtered,\n",
    "    y_log=np.log(train_df[target_col].astype(float)),\n",
    "    alg=alg,\n",
    "    params=base_params,\n",
    "    idx_dict=idx_dict,\n",
    "    cat_cols=cat_cols_filtered,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b41c34",
   "metadata": {},
   "source": [
    "## モデルの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2b413a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model_bundle(\n",
    "    model_dict: dict,\n",
    "    base_cols: list,\n",
    "    cat_cols: list | None,\n",
    "    save_path: str,\n",
    "):\n",
    "    bundle = {\n",
    "        'models': model_dict,\n",
    "        'base_cols': base_cols,\n",
    "        'cat_cols': cat_cols,\n",
    "    }\n",
    "\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(bundle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da44a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_bundle(\n",
    "    model_dict=final_models,\n",
    "    base_cols=fe_cols_filtered,\n",
    "    cat_cols=cat_cols_filtered,\n",
    "    save_path=f'{model_path}/{target_model}_model_v{training_ver}.pkl',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
