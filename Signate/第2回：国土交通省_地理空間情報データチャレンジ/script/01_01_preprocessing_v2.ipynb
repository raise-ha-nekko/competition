{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c54be4",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7375f2",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b518e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの取り扱いに関するライブラリ\n",
    "import numpy as np # 高速計算\n",
    "import pandas as pd # 表データの扱い\n",
    "\n",
    "# 可視化に関するライブラリ\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fd42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自身がファイルを格納したディレクトリを指定\n",
    "ROOT_DIR = '../../input/'\n",
    "train_file_path = ROOT_DIR + 'train.csv'\n",
    "test_file_path = ROOT_DIR + 'test.csv'\n",
    "data_definition_path = ROOT_DIR + 'data_definition.xlsx'\n",
    "intermediate_path = '../../output/intermediate_file/'\n",
    "gis_path = ROOT_DIR + 'GISデータ/'\n",
    "\n",
    "target_col = 'money_room'\n",
    "\n",
    "# スクリプトのバージョン指定\n",
    "preprocessing_ver = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33bd4b4",
   "metadata": {},
   "source": [
    "## File Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21beceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_definition = pd.ExcelFile(data_definition_path)\n",
    "data_definition_df = pd.read_excel(data_definition_path, sheet_name=data_definition.sheet_names[0])\n",
    "\n",
    "fe_cols = list(data_definition_df[data_definition_df['fe_cols'] == 1]['本番データ特徴量名'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae27961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file_path)[fe_cols + [target_col]]\n",
    "test_df = pd.read_csv(test_file_path)[fe_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d776842",
   "metadata": {},
   "source": [
    "## 建物種別の場合分け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8652e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def assign_building_category(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1) building_type → 細かいカテゴリ (building_category)\n",
    "    # ------------------------------------------------\n",
    "    # 1: マンション\n",
    "    # 2: タウンハウス\n",
    "    # 3: アパート\n",
    "    # 4: 一戸建\n",
    "    # 5: テラスハウス\n",
    "    # 6: 土地\n",
    "    # 7: 駐車場\n",
    "    # 8: ビル\n",
    "    # 9: 店舗\n",
    "    # 10: 倉庫\n",
    "    # 11: 工場\n",
    "    # 12: 寮\n",
    "    # 13: ホテル\n",
    "    # 14: 旅館\n",
    "    # 15: その他\n",
    "    # 901, 902, 999: 不明\n",
    "    mapping_row = {\n",
    "        1:   'mansion',\n",
    "        2:   'townhouse',\n",
    "        3:   'apartment',\n",
    "        4:   'house',\n",
    "        5:   'terrace_house',\n",
    "        6:   'land',\n",
    "        7:   'parking',\n",
    "        8:   'office_building',\n",
    "        9:   'shop',\n",
    "        10:  'warehouse',\n",
    "        11:  'factory',\n",
    "        12:  'dormitory',\n",
    "        13:  'hotel',\n",
    "        14:  'ryokan',\n",
    "        15:  'other',\n",
    "        901: 'unknown',\n",
    "        902: 'other_unknown',\n",
    "        999: 'other_unknown',\n",
    "    }\n",
    "\n",
    "    df['building_category'] = df['building_type'].map(mapping_row).fillna('unknown')\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2) モデル用の大分類 (model_category)\n",
    "    #    → 集合住宅 / 戸建 / 土地系 / 非住宅\n",
    "    # ------------------------------------------------\n",
    "    residential_multi_codes   = [1, 2, 3, 5, 12]  # マンション・アパート・タウン/テラス・寮\n",
    "    residential_detached_codes = [4]              # 一戸建\n",
    "    land_codes                = [6, 7]            # 土地・駐車場\n",
    "    non_residential_codes     = [8, 9, 10, 11, 13, 14, 15, 901, 902, 999]\n",
    "\n",
    "    model_mapping = {}\n",
    "\n",
    "    for c in residential_multi_codes:\n",
    "        model_mapping[c] = 'residential_multi'\n",
    "    for c in residential_detached_codes:\n",
    "        model_mapping[c] = 'house'\n",
    "    for c in land_codes:\n",
    "        model_mapping[c] = 'land'\n",
    "    for c in non_residential_codes:\n",
    "        model_mapping[c] = 'non_residential'\n",
    "\n",
    "    df['model_category'] = df['building_type'].map(model_mapping).fillna('unknown')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0022ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_preprocessed = assign_building_category(train_df)\n",
    "test_df_preprocessed  = assign_building_category(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187582d",
   "metadata": {},
   "source": [
    "## データ更新日以降のリフォーム・リノベ情報の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b26c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yyyymm_to_datetime(s):\n",
    "    \"\"\"\n",
    "    YYYYMM(float or int) → datetime(YYYY-MM-01) に変換する関数。\n",
    "    NaN は NaT のまま。\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(s.astype('Int64').astype(str), format='%Y%m', errors='coerce')\n",
    "\n",
    "\n",
    "def nullify_future_reform_info(df):\n",
    "    # 日付カラム一覧\n",
    "    yyyymm_cols = [\n",
    "        'reform_date',\n",
    "        'reform_wet_area_date',\n",
    "        'reform_interior_date',\n",
    "        'reform_exterior_date'\n",
    "    ]\n",
    "    free_date_cols = [\n",
    "        'renovation_date'\n",
    "    ]\n",
    "\n",
    "    # snapshot_modify_date を datetime に変換\n",
    "    df['snapshot_modify_date'] = pd.to_datetime(df['snapshot_modify_date'], errors='coerce')\n",
    "\n",
    "    # reform_date 系（float YYYYMM → datetime）\n",
    "    for col in yyyymm_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = convert_yyyymm_to_datetime(df[col])\n",
    "\n",
    "            # 未来情報なら NaT\n",
    "            df.loc[df[col] > df['snapshot_modify_date'], col] = pd.NaT\n",
    "\n",
    "    # renovation_date（文字列 → datetime）\n",
    "    for col in free_date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "            # 未来情報なら NaT\n",
    "            df.loc[df[col] > df['snapshot_modify_date'], col] = pd.NaT\n",
    "    \n",
    "    # リフォーム内容も欠損に変換\n",
    "    paired_cols = {\n",
    "        'reform_interior': 'reform_interior_date',\n",
    "        'reform_exterior': 'reform_exterior_date',\n",
    "        'reform_wet_area': 'reform_wet_area_date',\n",
    "    }\n",
    "\n",
    "    for flag_col, date_col in paired_cols.items():\n",
    "        if flag_col in df.columns and date_col in df.columns:\n",
    "            # date_col が未来なら flag_col を欠損へ\n",
    "            df.loc[df[date_col] > df['snapshot_modify_date'], flag_col] = pd.NA\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31cf0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_preprocessed = nullify_future_reform_info(train_df_preprocessed)\n",
    "test_df_preprocessed  = nullify_future_reform_info(test_df_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be04b7",
   "metadata": {},
   "source": [
    "## 都道府県・市区町村情報の置換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a7a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.read_excel(f'{ROOT_DIR}/data_definition.xlsx', sheet_name=data_definition.sheet_names[3])\n",
    "codes.columns = ['No.', 'addr1_1', 'addr1_2', 'Prefecture name',\n",
    "       'City/town/village name']\n",
    "codes = codes[['addr1_1', 'addr1_2', 'Prefecture name',\n",
    "       'City/town/village name']]\n",
    "\n",
    "train_df_preprocessed = pd.merge(train_df_preprocessed, codes, on=['addr1_1', 'addr1_2'], how='inner')\n",
    "test_df_preprocessed = pd.merge(test_df_preprocessed, codes, on=['addr1_1', 'addr1_2'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f673983",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_preprocessed.drop(['addr1_1', 'addr1_2'], axis=1, inplace=True)\n",
    "test_df_preprocessed.drop(['addr1_1', 'addr1_2'], axis=1, inplace=True)\n",
    "del codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a2206",
   "metadata": {},
   "source": [
    "## 面積関連の欠損値・異常値の処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4714e6",
   "metadata": {},
   "source": [
    "カラム名 | 意味 | 補足 | データの紐づけ | 欠損率\n",
    ":- |:- |:- |:- |:-\n",
    "total_floor_area | 延べ床面積 | 建物全体の床面積 | 棟情報 | 0.69\n",
    "building_area | 建築面積 | 建築面積 | 棟情報 | 0.98\n",
    "building_land_area | 土地面積 | | 棟情報 | 0.52\n",
    "land_area_all | 敷地全体面積 | 敷地全体の面積 | 棟情報 | 0.82\n",
    "unit_area_min | 専有面積 下限 | | 棟情報 | 0.65\n",
    "unit_area_max | 専有面積 上限 | | 棟情報 | 0.65\n",
    "land_kenpei | 建ぺい率(建築面積 ÷ 敷地面積) | 賃貸：土地、売買：土地で「都市計画」が 1:市街化区域 の場合に必須 単位：% | 棟情報 | 0.28\n",
    "land_youseki | 容積率(延べ床面積 ÷ 敷地面積) | 賃貸：土地、売買：土地で「都市計画」が 1:市街化区域 の場合に必須 単位：% | 棟情報 | 0.28\n",
    "unit_area | 専有面積 | | 棟情報 | 0.13\n",
    "snapshot_land_area | 区画面積(代表) | 単位：平米 | 物件情報 | 0.52\n",
    "house_area | 建物面積/専有面積(代表) | 単位：平米 | 物件情報 | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be462849",
   "metadata": {},
   "source": [
    "対応\n",
    "- 専有面積（senyu）\n",
    "    - 集合住宅：unit_area_corrected\n",
    "    - 戸建：house_area\n",
    "    - 土地・非住宅：NaN\n",
    "- 延床（nobeyuka）\n",
    "    - 戸建・非住宅：total_floor_area（なければ戸建は house_area も許容）\n",
    "    - 集合住宅・土地：NaN\n",
    "- 区画（kukaku）/ 土地（tochi）/ 敷地（shikichi）\n",
    "    - 集合住宅：基本 NaN（価格決定にはほぼ効かない）\n",
    "    - 戸建・土地・非住宅：snapshot_land_area → なければ building_land_area → なければ land_area_all を順に使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28761095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def assign_area_category(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # ========= 0. 便利マスク =========\n",
    "    is_multi  = df['model_category'] == 'residential_multi'\n",
    "    is_house  = df['model_category'] == 'house'\n",
    "    is_land   = df['model_category'] == 'land'\n",
    "    is_nonres = df['model_category'] == 'non_residential'\n",
    "\n",
    "    # ========= 1. unit_area_corrected =========\n",
    "    df['unit_area_corrected'] = df['unit_area']\n",
    "\n",
    "    # (A) min/max からの補正\n",
    "    mask_min = df['unit_area_corrected'].isna() & df['unit_area_min'].notna()\n",
    "    df.loc[mask_min, 'unit_area_corrected'] = df.loc[mask_min, 'unit_area_min']\n",
    "\n",
    "    mask_max = df['unit_area_corrected'].isna() & df['unit_area_max'].notna()\n",
    "    df.loc[mask_max, 'unit_area_corrected'] = df.loc[mask_max, 'unit_area_max']\n",
    "\n",
    "    # (B) min/max をはみ出しているものは中点で補正\n",
    "    mask_outside = (\n",
    "        df['unit_area_corrected'].notna()\n",
    "        & df['unit_area_min'].notna()\n",
    "        & df['unit_area_max'].notna()\n",
    "        & (\n",
    "            (df['unit_area_corrected'] < df['unit_area_min'])\n",
    "            | (df['unit_area_corrected'] > df['unit_area_max'])\n",
    "        )\n",
    "    )\n",
    "    df.loc[mask_outside, 'unit_area_corrected'] = (\n",
    "        (df['unit_area_min'] + df['unit_area_max']) / 2\n",
    "    )\n",
    "\n",
    "    # ========= 2. 専有面積 senyu_area =========\n",
    "    df['senyu_area'] = np.nan\n",
    "\n",
    "    # 集合住宅：unit_area_corrected\n",
    "    df.loc[is_multi, 'senyu_area'] = df.loc[is_multi, 'unit_area_corrected']\n",
    "\n",
    "    # 戸建：house_area\n",
    "    df.loc[is_house, 'senyu_area'] = df.loc[is_house, 'house_area']\n",
    "\n",
    "    # 土地・非住宅は NaN のまま（専有面積の概念なし）\n",
    "\n",
    "    # ========= 3. 延床面積 nobeyuka_area =========\n",
    "    df['nobeyuka_area'] = np.nan\n",
    "\n",
    "    # 戸建：total_floor_area があればそれ、なければ house_area を代用\n",
    "    mask_house_any = is_house & df['total_floor_area'].notna()\n",
    "    df.loc[mask_house_any, 'nobeyuka_area'] = df.loc[mask_house_any, 'total_floor_area']\n",
    "\n",
    "    mask_house_fallback = is_house & df['nobeyuka_area'].isna() & df['house_area'].notna()\n",
    "    df.loc[mask_house_fallback, 'nobeyuka_area'] = df.loc[mask_house_fallback, 'house_area']\n",
    "\n",
    "    # 非住宅：total_floor_area をそのまま使う\n",
    "    df.loc[is_nonres, 'nobeyuka_area'] = df.loc[is_nonres, 'total_floor_area']\n",
    "\n",
    "    # 集合住宅・土地は NaN のまま（延床は基本使わない）\n",
    "\n",
    "    # ========= 4. 区画面積 kukaku_area =========\n",
    "    # 土地・戸建・非住宅で使う（集合住宅は NaN）\n",
    "    df['kukaku_area'] = np.nan\n",
    "\n",
    "    # 優先順位: snapshot_land_area → building_land_area\n",
    "    base_land = df['snapshot_land_area'].copy()\n",
    "    base_land = base_land.fillna(df['building_land_area'])\n",
    "\n",
    "    mask_need_kukaku = is_house | is_land | is_nonres\n",
    "    df.loc[mask_need_kukaku, 'kukaku_area'] = base_land.loc[mask_need_kukaku]\n",
    "\n",
    "    # ========= 5. 土地面積 tochi_area =========\n",
    "    # 基本は building_land_area、あれば snapshot_land_area で上書き\n",
    "    df['tochi_area'] = np.nan\n",
    "\n",
    "    land_base = df['building_land_area'].copy()\n",
    "    land_base = land_base.where(land_base.notna(), df['snapshot_land_area'])\n",
    "    # 戸建・土地・非住宅にのみセット\n",
    "    mask_need_tochi = is_house | is_land | is_nonres\n",
    "    df.loc[mask_need_tochi, 'tochi_area'] = land_base.loc[mask_need_tochi]\n",
    "\n",
    "    # ========= 6. 敷地面積 shikichi_area =========\n",
    "    # site 全体の面積イメージ: land_area_all → tochi_area → kukaku_area\n",
    "    df['shikichi_area'] = np.nan\n",
    "\n",
    "    df.loc[:, 'shikichi_area'] = df['land_area_all']\n",
    "    df['shikichi_area'] = df['shikichi_area'].fillna(df['tochi_area'])\n",
    "    df['shikichi_area'] = df['shikichi_area'].fillna(df['kukaku_area'])\n",
    "\n",
    "    # 集合住宅は敷地面積の意味が薄いのであえて NaN にしてもよい\n",
    "    df.loc[is_multi, ['kukaku_area', 'tochi_area', 'shikichi_area']] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63808d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_preprocessed = assign_area_category(train_df_preprocessed)\n",
    "test_df_preprocessed  = assign_area_category(test_df_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71e5ef",
   "metadata": {},
   "source": [
    "## 欠損値・異常値の対応\n",
    "このブロックも見直した方がいいかも"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba69834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(train_df_preprocessed)\n",
    "\n",
    "# ===== 1. 結合 =====\n",
    "df = pd.concat([train_df_preprocessed, test_df_preprocessed],\n",
    "               axis=0, ignore_index=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. クリーニング（clean列を作る）\n",
    "#   面積系は building_category ごとの分位点で外れ値を NaN にする\n",
    "# -------------------------------------------------\n",
    "\n",
    "def make_area_clean(df: pd.DataFrame,\n",
    "                    col: str,\n",
    "                    lower_q: float = 0.01,\n",
    "                    upper_q: float = 0.99) -> pd.Series:\n",
    "    \"\"\"\n",
    "    model_category ごとに分位点クリーニング。\n",
    "    そのカテゴリに col の有効値が 1つもない場合は、clean 列はすべて NaN になる。\n",
    "    \"\"\"\n",
    "    q = (\n",
    "        df.loc[df[col].notna()]\n",
    "          .groupby('model_category')[col]   # ★ここだけ変更\n",
    "          .quantile([lower_q, upper_q])\n",
    "          .unstack()\n",
    "    )\n",
    "    q.columns = ['q_low', 'q_high']\n",
    "\n",
    "    def _clean(group: pd.Series) -> pd.Series:\n",
    "        cat = group.name\n",
    "        if cat not in q.index:\n",
    "            return pd.Series([np.nan] * len(group), index=group.index)\n",
    "        lo = q.loc[cat, 'q_low']\n",
    "        hi = q.loc[cat, 'q_high']\n",
    "        return group.where(group.between(lo, hi))\n",
    "\n",
    "    return df.groupby('model_category')[col].transform(_clean)  # ★ここも\n",
    "\n",
    "\n",
    "\n",
    "# 間取り数（これは従来どおりでよい）\n",
    "df['madori_number_clean'] = df['madori_number_all'].where(\n",
    "    df['madori_number_all'].between(1, 7)\n",
    ")\n",
    "\n",
    "# 専有面積・区画面積・延べ床・土地面積を building_category 別にクレンジング\n",
    "df['senyu_area_clean']     = make_area_clean(df, 'senyu_area')\n",
    "df['kukaku_area_clean']    = make_area_clean(df, 'kukaku_area')\n",
    "df['nobeyuka_area_clean']  = make_area_clean(df, 'nobeyuka_area')\n",
    "df['tochi_area_clean']     = make_area_clean(df, 'tochi_area')\n",
    "df['shikichi_area_clean']  = make_area_clean(df, 'shikichi_area')\n",
    "\n",
    "# 間取りコード：有効コード以外を NaN\n",
    "valid_codes = [10, 20, 25, 30, 35, 40, 45, 50, 55]\n",
    "df['madori_kind_clean'] = df['madori_kind_all'].where(\n",
    "    df['madori_kind_all'].isin(valid_codes)\n",
    ")\n",
    "\n",
    "# 階数：明らかな外れ値だけ除外（例：全体の 99% タイルで上限を切る）\n",
    "floor_upper = (\n",
    "    df.loc[df['floor_count'].notna(), 'floor_count']\n",
    "      .quantile(0.99)\n",
    ")\n",
    "df['floor_count_clean'] = df['floor_count'].where(df['floor_count'] <= floor_upper)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. building_id 単位で補完\n",
    "# -------------------------------------------------\n",
    "\n",
    "def mode_or_nan(s: pd.Series):\n",
    "    m = s.mode()\n",
    "    return m.iloc[0] if not m.empty else np.nan\n",
    "\n",
    "def median_or_nan(s: pd.Series):\n",
    "    return s.median() if s.notna().any() else np.nan\n",
    "\n",
    "g = df.groupby('building_id')\n",
    "\n",
    "# --- mode 補完するカラム ---\n",
    "mode_fill_specs = {\n",
    "    'madori_number_all': 'madori_number_clean',\n",
    "    'madori_kind_all':   'madori_kind_clean',\n",
    "    'floor_count':       'floor_count_clean',\n",
    "}\n",
    "\n",
    "for out_col, clean_col in mode_fill_specs.items():\n",
    "    df[out_col] = g[clean_col].transform(mode_or_nan)\n",
    "\n",
    "# --- median 補完するカラム（面積） ---\n",
    "median_fill_specs = {\n",
    "    'senyu_area':     'senyu_area_clean',\n",
    "    'kukaku_area':    'kukaku_area_clean',\n",
    "    'nobeyuka_area':  'nobeyuka_area_clean',\n",
    "    'tochi_area':     'tochi_area_clean',\n",
    "    'shikichi_area':  'shikichi_area_clean',\n",
    "}\n",
    "\n",
    "for out_col, clean_col in median_fill_specs.items():\n",
    "    df[out_col] = g[clean_col].transform(median_or_nan)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. その他の補正・フラグ\n",
    "# -------------------------------------------------\n",
    "\n",
    "# room_floor > floor_count の場合は floor_count でクリップ\n",
    "mask = (df['room_floor'] > df['floor_count']) & df['floor_count'].notna()\n",
    "df.loc[mask, 'room_floor'] = df.loc[mask, 'floor_count']\n",
    "\n",
    "# 欠損フラグ\n",
    "df['floor_count_missing']   = df['floor_count'].isna().astype(int)\n",
    "df['building_type_missing'] = df['building_type'].isna().astype(int)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. 再び train / test に分割\n",
    "# -------------------------------------------------\n",
    "train_df_preprocessed = df.iloc[:n_train].copy()\n",
    "test_df_preprocessed  = df.iloc[n_train:].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2409646",
   "metadata": {},
   "source": [
    "## 分布が歪なカラムをlog変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de80102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cols = [\n",
    "    'senyu_area',\n",
    "    'nobeyuka_area',\n",
    "    'kukaku_area',\n",
    "    'tochi_area',\n",
    "    'shikichi_area',\n",
    "    'unit_count'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ffd2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fe_cols = []\n",
    "\n",
    "for col in log_cols:\n",
    "    for df in [train_df_preprocessed, test_df_preprocessed]:\n",
    "        new_col = f'{col}_log'\n",
    "        # 負値はないはずだが、念のため0でクリップ\n",
    "        df[new_col] = np.log1p(df[col].clip(lower=0))\n",
    "    log_fe_cols.append(new_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ab427",
   "metadata": {},
   "source": [
    "## リフォーム・リノベのタグ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd242e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slashed_tags(df, cols_list):\n",
    "    \"\"\"スラッシュ区切り列を 0/1 の int8 フラグ列に分解する\"\"\"\n",
    "    temp_dfs = []\n",
    "    for col in cols_list:\n",
    "\n",
    "        temp_df = df[col].str.get_dummies(sep='/')\n",
    "        # if is_tag_master:\n",
    "        #     temp_df.rename(columns=tag_master, inplace=True)\n",
    "        temp_df = temp_df.add_prefix(f'{col} ')\n",
    "        temp_df = temp_df.astype('int8')\n",
    "\n",
    "        temp_dfs.append(temp_df)\n",
    "\n",
    "    # すべて結合\n",
    "    temp_dfs = pd.concat(temp_dfs, axis=1).astype('int8')\n",
    "    return temp_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f119a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- train + test を結合 ---\n",
    "combined_df = pd.concat([train_df_preprocessed, test_df_preprocessed], ignore_index=True)\n",
    "\n",
    "# --- 新しいタグ列を生成 ---\n",
    "slashed_cols = ['reform_interior', 'reform_exterior', 'reform_wet_area']\n",
    "slashed_df = get_slashed_tags(combined_df, slashed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d26a999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reform_cols = slashed_df.columns.tolist()\n",
    "\n",
    "# --- 元 DF に結合 ---\n",
    "combined_df = pd.concat([combined_df, slashed_df], axis=1)\n",
    "\n",
    "# --- スラッシュ区切り列を削除 ---\n",
    "combined_df = combined_df.drop(columns=slashed_cols)\n",
    "\n",
    "# --- 再分割 ---\n",
    "train_df_preprocessed = combined_df.iloc[:len(train_df_preprocessed)].copy()\n",
    "test_df_preprocessed  = combined_df.iloc[len(train_df_preprocessed):].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "733c7713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del slashed_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588a095",
   "metadata": {},
   "source": [
    "## 築年数の算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50e76860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_year(date_input):\n",
    "    try:\n",
    "        s = str(date_input)\n",
    "        if len(s) < 4:\n",
    "            return np.nan\n",
    "        return int(s[:4])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def add_age_features(df):\n",
    "    # 元の year_built と target_ym の年だけ抽出\n",
    "    df['built_year']  = df['year_built'].apply(parse_year)\n",
    "    df['target_year'] = df['target_ym'].apply(parse_year)\n",
    "\n",
    "    # 築年数 = 対象年 − 建築年\n",
    "    df['built_diff'] = df['target_year'] - df['built_year']\n",
    "\n",
    "    # 築年数がマイナスになることはありえないので NaN に修正\n",
    "    df.loc[df['built_diff'] < 0, 'built_diff'] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = add_age_features(train_df_preprocessed)\n",
    "test_df_preprocessed  = add_age_features(test_df_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca6fd4",
   "metadata": {},
   "source": [
    "## 周期変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40e0ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 緯度・経度をラジアンに ---\n",
    "for df in [train_df_preprocessed, test_df_preprocessed]:\n",
    "    df['lat_rad'] = np.radians(df['lat'].astype(float))\n",
    "    df['lon_rad'] = np.radians(df['lon'].astype(float))\n",
    "\n",
    "    # sin / cos 変換\n",
    "    df['sin_lat'] = np.sin(df['lat_rad'])\n",
    "    df['cos_lat'] = np.cos(df['lat_rad'])\n",
    "    df['sin_lon'] = np.sin(df['lon_rad'])\n",
    "    df['cos_lon'] = np.cos(df['lon_rad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df61c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 風向きも周期変換したい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f72156",
   "metadata": {},
   "source": [
    "## タグ情報のOneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "351b1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_info = pd.read_excel(f'{ROOT_DIR}/data_definition.xlsx', sheet_name=data_definition.sheet_names[2])\n",
    "tag_info = tag_info[['タグID', 'タグ内容', 'タグ分類']]\n",
    "\n",
    "facilities_info = pd.read_excel(f'{ROOT_DIR}/data_definition.xlsx', sheet_name=data_definition.sheet_names[4])\n",
    "facilities_info = facilities_info[['タグID', 'タグ内容', 'タグ分類']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d61aedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_master = pd.concat([tag_info, facilities_info], axis=0, ignore_index=True).drop_duplicates()\n",
    "tag_master = tag_master[tag_master['タグ分類'] != '不要']\n",
    "tag_master['タグ情報'] = tag_master['タグ分類'] + '_' + tag_master['タグ内容']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "795c0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_master['タグID'] = tag_master['タグID'].astype('str')\n",
    "tag_master.set_index('タグID', inplace=True)\n",
    "tag_master = tag_master.to_dict()['タグ情報']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "183d8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train_df_preprocessed, test_df_preprocessed], ignore_index=True)\n",
    "org_tag_cols = ['building_tag_id', 'unit_tag_id', 'statuses']\n",
    "\n",
    "# tag_master は「{tag_id: tag_name}」の辞書であることを前提\n",
    "valid_tag_ids = set(tag_master.keys())  # リネーム対象の tag_id\n",
    "valid_tag_names = set(tag_master.values())  # リネーム後のタグ名\n",
    "\n",
    "tag_dfs = []\n",
    "\n",
    "for col in org_tag_cols:\n",
    "    temp_df = combined_df[col].str.get_dummies(sep='/')\n",
    "\n",
    "    # --- rename ---\n",
    "    temp_df.rename(columns=tag_master, inplace=True)\n",
    "\n",
    "    # --- ★ リネームされなかった元IDを削除 ★ ---\n",
    "    # 残すべきカラム：valid_tag_names に含まれるもののみ\n",
    "    temp_df = temp_df.loc[:, temp_df.columns.isin(valid_tag_names)]\n",
    "\n",
    "    temp_df = temp_df.astype('int8')\n",
    "    tag_dfs.append(temp_df)\n",
    "\n",
    "# 結合\n",
    "tag_df = pd.concat(tag_dfs, axis=1).astype('int8')\n",
    "\n",
    "# 同名列は 1 を優先（max を取る）\n",
    "tag_df = tag_df.groupby(level=0, axis=1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb4aa5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_cols = tag_df.columns.drop_duplicates().tolist()\n",
    "\n",
    "# --- 元 DF に結合 ---\n",
    "combined_df = pd.concat([combined_df, tag_df], axis=1)\n",
    "\n",
    "# --- スラッシュ区切り列を削除 ---\n",
    "combined_df = combined_df.drop(columns=org_tag_cols)\n",
    "\n",
    "# --- 再分割 ---\n",
    "train_df_preprocessed = combined_df.iloc[:len(train_df_preprocessed)].copy()\n",
    "test_df_preprocessed  = combined_df.iloc[len(train_df_preprocessed):].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1acf9",
   "metadata": {},
   "source": [
    "## 地価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "675b879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:4612\n"
     ]
    }
   ],
   "source": [
    "land_gdf = gpd.read_file(f'{gis_path}公示地価/L01-23_GML/L01-23.geojson') \n",
    "print(land_gdf.crs) # EPSG:4612(日本測地系2000) EPSG:4326が世界測地系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73b8c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 対前年変動率も特徴量に入れたい\n",
    "land_gdf = land_gdf.rename(columns={\n",
    "    'L01_005': 'land_year',            # 年度（例: 2023）\n",
    "    'L01_006': 'land_price_2023',    # 公示地価 [円/㎡]\n",
    "    'L01_007': 'land_price_yoy_pct',   # 対前年変動率 [%]\n",
    "})\n",
    "\n",
    "year_to_col = {y: f'L01_{y - 1921:03d}' for y in range(2018, 2023)}\n",
    "rename_price_cols = {col: f'land_price_{year}' \n",
    "                     for year, col in year_to_col.items()}\n",
    "\n",
    "land_gdf = land_gdf.rename(columns=rename_price_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff57d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_jgd_from_wgs(df, el_col='el', nl_col='nl',\n",
    "                      lon_wgs_col='lon', lat_wgs_col='lat'):\n",
    "    \"\"\"\n",
    "    日本測地系(el/nl)が NaN で、世界測地系(lon_wgs/lat_wgs) がある行だけ\n",
    "    JGD2011(EPSG:6668) → JGD2000(EPSG:4612) に変換して補完する\n",
    "    \"\"\"\n",
    "    df['lon_jgd'] = df[el_col] / 3_600_000\n",
    "    df['lat_jgd'] = df[nl_col] / 3_600_000\n",
    "\n",
    "\n",
    "    # 1) 日本測地系が欠損していて、世界測地系はある行だけマスク\n",
    "    mask = df['lon_jgd'].isna() & df['lat_jgd'].isna() \\\n",
    "           & df[lon_wgs_col].notna() & df[lat_wgs_col].notna()\n",
    "\n",
    "    if mask.sum() == 0:\n",
    "        return df  # 補完する行がなければそのまま返す\n",
    "\n",
    "    df_sub = df.loc[mask].copy()\n",
    "\n",
    "    # 2) 世界測地系の GeoDataFrame（EPSG:4326 = WGS84）\n",
    "    gdf_wgs = gpd.GeoDataFrame(\n",
    "        df_sub,\n",
    "        geometry=gpd.points_from_xy(df_sub[lon_wgs_col], df_sub[lat_wgs_col]),\n",
    "        crs='EPSG:6668'\n",
    "    )\n",
    "\n",
    "    # 3) 日本測地系（JGD2000, EPSG:4612）に変換\n",
    "    gdf_jgd = gdf_wgs.to_crs(epsg=4612)\n",
    "\n",
    "    # 4) 変換後の座標を el/nl に入れる\n",
    "    df.loc[mask, 'lon_jgd'] = gdf_jgd.geometry.x.values  # 経度（日本測地系）\n",
    "    df.loc[mask, 'lat_jgd'] = gdf_jgd.geometry.y.values  # 緯度（日本測地系）\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b81db26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# まず train/test それぞれ実行\n",
    "train_df_preprocessed = fill_jgd_from_wgs(train_df_preprocessed)\n",
    "test_df_preprocessed = fill_jgd_from_wgs(test_df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b44f612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_land_price_features_haversine(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    land_gdf: gpd.GeoDataFrame,\n",
    "    lat_col: str,\n",
    "    lon_col: str,\n",
    "    year_col: str,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    各物件について、同じ year の land_price_YYYY を持つ標準地から\n",
    "\n",
    "    - 最近傍1点の地価: nearest_land_price\n",
    "    - その距離[m]: distance_to_landpoint_m\n",
    "    - log(nearest_land_price + 1): log_land_price\n",
    "    - 最近傍3点距離加重平均地価: weighted_land_price_3\n",
    "    - その log: log_weighted_land_price_3\n",
    "\n",
    "    を haversine 距離（球面距離）を用いて付与する。\n",
    "    緯度経度は「度（deg）」前提（JGD2000/JGD2011/WGS84 いずれでも OK、揃っていればよい）。\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) train/test を結合\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # 2) 座標がまともな行だけ対象\n",
    "    lat = combined_df[lat_col]\n",
    "    lon = combined_df[lon_col]\n",
    "\n",
    "    valid_mask = (\n",
    "        lat.notna() & lon.notna()\n",
    "        & lat.between(-90, 90)\n",
    "        & lon.between(-180, 180)\n",
    "    )\n",
    "    df_valid = combined_df.loc[valid_mask].copy()\n",
    "\n",
    "    n_all = len(combined_df)\n",
    "    nearest_all  = np.full(n_all, np.nan, dtype=float)\n",
    "    dist_all     = np.full(n_all, np.nan, dtype=float)\n",
    "    weighted_all = np.full(n_all, np.nan, dtype=float)\n",
    "\n",
    "    # 3) 標準地側の座標（land_gdf）をラジアンに変換\n",
    "    land_lon_deg = land_gdf.geometry.x.to_numpy()\n",
    "    land_lat_deg = land_gdf.geometry.y.to_numpy()\n",
    "\n",
    "    land_lon_rad = np.deg2rad(land_lon_deg)\n",
    "    land_lat_rad = np.deg2rad(land_lat_deg)\n",
    "\n",
    "    land_X = np.vstack([land_lat_rad, land_lon_rad]).T  # (n_land, 2)\n",
    "\n",
    "    # 4) 最近傍 3点探索 (haversine)\n",
    "    nn = NearestNeighbors(n_neighbors=3, metric=\"haversine\")\n",
    "    nn.fit(land_X)\n",
    "\n",
    "    # 5) 物件側の座標をラジアンに変換\n",
    "    prop_lat_deg = df_valid[lat_col].to_numpy()\n",
    "    prop_lon_deg = df_valid[lon_col].to_numpy()\n",
    "\n",
    "    prop_lat_rad = np.deg2rad(prop_lat_deg)\n",
    "    prop_lon_rad = np.deg2rad(prop_lon_deg)\n",
    "\n",
    "    prop_X = np.vstack([prop_lat_rad, prop_lon_rad]).T  # (n_valid, 2)\n",
    "\n",
    "    finite_mask = np.isfinite(prop_lat_rad) & np.isfinite(prop_lon_rad)\n",
    "    valid_index = df_valid.index.to_numpy()[finite_mask]\n",
    "\n",
    "    if len(valid_index) > 0:\n",
    "        # 距離は「ラジアン」で返ってくる\n",
    "        distances_rad, indices = nn.kneighbors(prop_X[finite_mask])  # (n_valid, 3)\n",
    "\n",
    "        # 年情報\n",
    "        years_valid = df_valid.loc[valid_index, year_col].to_numpy().astype(int)\n",
    "\n",
    "        nearest_prices  = []\n",
    "        nearest_dists_m = []\n",
    "        weighted_prices = []\n",
    "\n",
    "        R = 6_371_000.0  # 地球半径[m]\n",
    "\n",
    "        for dist_row_rad, idx_row, year in zip(distances_rad, indices, years_valid):\n",
    "            # 距離[m] に変換\n",
    "            dist_row_m = dist_row_rad * R\n",
    "\n",
    "            col_name = f'land_price_{year}'\n",
    "            if col_name not in land_gdf.columns:\n",
    "                raise KeyError(\n",
    "                    f'{col_name} が land_gdf に存在しません。'\n",
    "                    f'列名や対象年の範囲を確認してください。'\n",
    "                )\n",
    "\n",
    "            prices = land_gdf.iloc[idx_row][col_name].to_numpy().astype(float)\n",
    "\n",
    "            # --- 1点目（最近傍）の情報 ---\n",
    "            nearest_prices.append(prices[0])\n",
    "            nearest_dists_m.append(dist_row_m[0])\n",
    "\n",
    "            # --- 3点距離加重平均 ---\n",
    "            zero_mask = dist_row_m == 0\n",
    "            if zero_mask.any():\n",
    "                # 距離0が含まれる場合は、その点/その複数点の平均\n",
    "                wp = prices[zero_mask].mean()\n",
    "            else:\n",
    "                # 1/d で重み付け\n",
    "                w = 1.0 / dist_row_m\n",
    "                w = w / w.sum()\n",
    "                wp = np.dot(w, prices)\n",
    "\n",
    "            weighted_prices.append(wp)\n",
    "\n",
    "        nearest_prices  = np.array(nearest_prices, dtype=float)\n",
    "        nearest_dists_m = np.array(nearest_dists_m, dtype=float)\n",
    "        weighted_prices = np.array(weighted_prices, dtype=float)\n",
    "\n",
    "        nearest_all[valid_index]  = nearest_prices\n",
    "        dist_all[valid_index]     = nearest_dists_m\n",
    "        weighted_all[valid_index] = weighted_prices\n",
    "\n",
    "    # 6) combined_df に列を追加\n",
    "    combined_df = combined_df.copy()\n",
    "    combined_df['nearest_land_price']       = nearest_all\n",
    "    combined_df['distance_to_landpoint_m']  = dist_all\n",
    "    combined_df['log_land_price']           = np.log1p(combined_df['nearest_land_price'])\n",
    "    combined_df['weighted_land_price_3']    = weighted_all\n",
    "    combined_df['log_weighted_land_price_3'] = np.log1p(combined_df['weighted_land_price_3'])\n",
    "\n",
    "    # 7) train/test に戻す\n",
    "    n_train = len(train_df)\n",
    "    train_out = combined_df.iloc[:n_train].reset_index(drop=True)\n",
    "    test_out  = combined_df.iloc[n_train:].reset_index(drop=True)\n",
    "\n",
    "    return train_out, test_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "269f079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_preprocessed, test_df_preprocessed = add_land_price_features_haversine(\n",
    "    train_df=train_df_preprocessed,\n",
    "    test_df=test_df_preprocessed,\n",
    "    land_gdf=land_gdf,\n",
    "    lat_col='lat_jgd',\n",
    "    lon_col='lon_jgd',\n",
    "    year_col='target_year'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0474a4f",
   "metadata": {},
   "source": [
    "## 税部分の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f610566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: money_kyoueki_tax, parking_money_tax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1341481",
   "metadata": {},
   "source": [
    "## 比率算出の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e12d0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 私道、持分比率など"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88ee11",
   "metadata": {},
   "source": [
    "## 特徴量の追加・削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "329e9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_cols += [\n",
    "    'building_category', 'model_category',\n",
    "    'Prefecture name', 'City/town/village name',\n",
    "    'floor_count_missing', 'building_type_missing',\n",
    "    'built_diff', 'target_year',\n",
    "    'senyu_area', 'nobeyuka_area', 'kukaku_area', 'tochi_area', 'shikichi_area',\n",
    "    'nearest_land_price', 'weighted_land_price_3', 'distance_to_landpoint_m', 'log_land_price', 'log_weighted_land_price_3'\n",
    "] + log_fe_cols + tag_cols + reform_cols\n",
    "\n",
    "# 削除する特徴量\n",
    "remove_cols = [\n",
    "    'bukken_type', 'building_type',\n",
    "    'addr1_1', 'addr1_2',\n",
    "    'snapshot_modify_date',\n",
    "    'unit_area', 'house_area', 'total_floor_area', 'snapshot_land_area', 'building_land_area', 'land_area_all', 'building_area',\n",
    "    'el', 'nl',\n",
    "    'building_tag_id', 'unit_tag_id', 'statuses',\n",
    "] + slashed_cols\n",
    "fe_cols = [c for c in fe_cols if c not in remove_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7a29b",
   "metadata": {},
   "source": [
    "## 出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c83b8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_preprocessed[fe_cols + [target_col]].to_csv(f'{intermediate_path}train_df_preprocessed_v{preprocessing_ver}.csv', index=False)\n",
    "test_df_preprocessed[fe_cols].to_csv(f'{intermediate_path}test_df_preprocessed_v{preprocessing_ver}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
