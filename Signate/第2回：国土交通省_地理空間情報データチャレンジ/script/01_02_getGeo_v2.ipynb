{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c54be4",
   "metadata": {},
   "source": [
    "# 国土数値情報の取得"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7375f2",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b518e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの取り扱いに関するライブラリ\n",
    "import numpy as np # 高速計算\n",
    "import pandas as pd # 表データの扱い\n",
    "\n",
    "# 可視化に関するライブラリ\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import re\n",
    "from pyogrio import read_dataframe\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fd42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自身がファイルを格納したディレクトリを指定\n",
    "ROOT_DIR = '../input/'\n",
    "train_file_path = ROOT_DIR + 'train.csv'\n",
    "test_file_path = ROOT_DIR + 'test.csv'\n",
    "intermediate_path = '../output/intermediate_file/'\n",
    "gis_path = ROOT_DIR + 'GISデータ/'\n",
    "\n",
    "pkey_cols = ['target_ym', 'building_id', 'unit_id']\n",
    "geo_cols = ['lat', 'lon']\n",
    "\n",
    "geo_ver = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33bd4b4",
   "metadata": {},
   "source": [
    "## File Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae27961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_geo = pd.read_csv(train_file_path)[pkey_cols + geo_cols]\n",
    "test_df_geo = pd.read_csv(test_file_path)[pkey_cols + geo_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4d782",
   "metadata": {},
   "source": [
    "## データの変換"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c3f3c",
   "metadata": {},
   "source": [
    "#### 対象年の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689612a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_year(date_input):\n",
    "    try:\n",
    "        s = str(date_input)\n",
    "        if len(s) < 4:\n",
    "            return np.nan\n",
    "        return int(s[:4])\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba94e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_geo['target_year'] = train_df_geo['target_ym'].apply(parse_year)\n",
    "test_df_geo['target_year'] = test_df_geo['target_ym'].apply(parse_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19971b5a",
   "metadata": {},
   "source": [
    "#### 測地系の変換(EPSG:6668 → EPSG:4612)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff57d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_jgd_from_wgs(df, lon_wgs_col='lon', lat_wgs_col='lat'):\n",
    "    \"\"\"\n",
    "    JGD2011(EPSG:6668) → JGD2000(EPSG:4612) に変換して補完する\n",
    "    \"\"\"\n",
    "\n",
    "    # 2) 世界測地系の GeoDataFrame\n",
    "    gdf_wgs = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[lon_wgs_col], df[lat_wgs_col]),\n",
    "        crs='EPSG:6668'\n",
    "    )\n",
    "\n",
    "    # 3) 日本測地系（JGD2000, EPSG:4612）に変換\n",
    "    gdf_jgd = gdf_wgs.to_crs(epsg=4612)\n",
    "\n",
    "    # 4) 変換後の座標を el/nl に入れる\n",
    "    df['lon_jgd'] = gdf_jgd.geometry.x.values  # 経度（4612）\n",
    "    df['lat_jgd'] = gdf_jgd.geometry.y.values  # 緯度（4612）\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81db26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_geo = fill_jgd_from_wgs(train_df_geo)\n",
    "test_df_geo = fill_jgd_from_wgs(test_df_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1acf9",
   "metadata": {},
   "source": [
    "## 地価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675b879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:4612\n"
     ]
    }
   ],
   "source": [
    "# land_gdf2022 = read_dataframe(f'{gis_path}公示地価/L01-22_GML/L01-22.geojson') \n",
    "land_gdf = read_dataframe(f'{gis_path}公示地価/L01-23_GML/L01-23.geojson') \n",
    "print(land_gdf.crs) # EPSG:4612(日本測地系2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b8c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_gdf = land_gdf.rename(columns={\n",
    "    'L01_005': 'land_year',            # 年度（例: 2023）\n",
    "    'L01_006': 'land_price_2023',    # 公示地価 [円/㎡]\n",
    "    'L01_007': 'land_price_yoy_pct',   # 対前年変動率 [%]\n",
    "})\n",
    "\n",
    "year_to_col = {y: f'L01_{y - 1922:03d}' for y in range(2018, 2023)}\n",
    "rename_price_cols = {col: f'land_price_{year}' \n",
    "                     for year, col in year_to_col.items()}\n",
    "\n",
    "land_gdf = land_gdf.rename(columns=rename_price_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44f612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_land_price_features_haversine(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    land_gdf,\n",
    "    lat_col: str,\n",
    "    lon_col: str,\n",
    "    year_col: str,\n",
    "    n_neighbors: int = 3,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    target_year=y の各行について、標準地を (land_price_y>0 かつ land_price_{y-1}>0) に絞ってから\n",
    "    最近傍/距離加重の地価特徴量と前年比(yoy/dlog)を作る。\n",
    "\n",
    "    目的:\n",
    "    - 「前年が存在しない新設標準地（前年=0）」を参照して train に 0 地価が入るのを避ける\n",
    "    - yoy/dlog を意味のある標準地だけで計算する\n",
    "\n",
    "    前提:\n",
    "    - year_col は 2019 のような年\n",
    "    - land_gdf に land_price_YYYY 列がある\n",
    "    - lat/lon は経緯度（度）\n",
    "    \"\"\"\n",
    "\n",
    "    if n_neighbors <= 0:\n",
    "        raise ValueError('n_neighbors must be positive')\n",
    "\n",
    "    # --- 1) train/test を MultiIndex で安全に結合（index重複回避） ---\n",
    "    combined = pd.concat({'train': train_df, 'test': test_df}, axis=0, names=['__set__'])\n",
    "\n",
    "    # --- 2) 有効座標のみ ---\n",
    "    lat = combined[lat_col]\n",
    "    lon = combined[lon_col]\n",
    "    valid_mask = (\n",
    "        lat.notna() & lon.notna()\n",
    "        & lat.between(-90, 90)\n",
    "        & lon.between(-180, 180)\n",
    "    )\n",
    "    df_valid = combined.loc[valid_mask]\n",
    "\n",
    "    # --- 3) 出力（MultiIndexで持つ） ---\n",
    "    nearest_all = pd.Series(np.nan, index=combined.index, dtype='float64')\n",
    "    dist_all = pd.Series(np.nan, index=combined.index, dtype='float64')\n",
    "    weighted_all = pd.Series(np.nan, index=combined.index, dtype='float64')\n",
    "    nearest_prev_all = pd.Series(np.nan, index=combined.index, dtype='float64')\n",
    "    weighted_prev_all = pd.Series(np.nan, index=combined.index, dtype='float64')\n",
    "\n",
    "    if len(df_valid) > 0:\n",
    "        # 物件座標（df_validの行順で固定）\n",
    "        prop_lat_rad = np.deg2rad(df_valid[lat_col].to_numpy())\n",
    "        prop_lon_rad = np.deg2rad(df_valid[lon_col].to_numpy())\n",
    "        prop_X = np.vstack([prop_lat_rad, prop_lon_rad]).T\n",
    "        finite_mask = np.isfinite(prop_lat_rad) & np.isfinite(prop_lon_rad)\n",
    "\n",
    "        if finite_mask.any():\n",
    "            prop_X_f = prop_X[finite_mask]\n",
    "            idx_valid_f = df_valid.index.to_numpy()[finite_mask]  # MultiIndexラベル\n",
    "            years_valid = df_valid[year_col].to_numpy(dtype=np.int64)[finite_mask]\n",
    "\n",
    "            R = 6_371_000.0\n",
    "\n",
    "            # --- 4) yearごとに処理（yearごとに land_gdf を 0除外してNN） ---\n",
    "            unique_years = np.unique(years_valid)\n",
    "\n",
    "            for y in unique_years:\n",
    "                # 対象行（このyearの物件）\n",
    "                rows_mask = (years_valid == y)\n",
    "                if not rows_mask.any():\n",
    "                    continue\n",
    "\n",
    "                col_y = f'land_price_{int(y)}'\n",
    "                col_y_1 = f'land_price_{int(y) - 1}'\n",
    "\n",
    "                if col_y not in land_gdf.columns or col_y_1 not in land_gdf.columns:\n",
    "                    # 列がなければこのyearは作れないのでスキップ\n",
    "                    continue\n",
    "\n",
    "                # --- 4-1) 標準地を「当年>0 かつ 前年>0」に限定 ---\n",
    "                land_price_y = land_gdf[col_y].to_numpy(dtype=float)\n",
    "                land_price_y1 = land_gdf[col_y_1].to_numpy(dtype=float)\n",
    "\n",
    "                land_ok = (land_price_y > 0) & (land_price_y1 > 0)\n",
    "                if land_ok.sum() < n_neighbors:\n",
    "                    # 近傍が作れない（点が少ない）→ このyearの行は NaN のまま\n",
    "                    continue\n",
    "\n",
    "                land_sub = land_gdf.loc[land_ok]\n",
    "\n",
    "                # 座標\n",
    "                land_lon_rad = np.deg2rad(land_sub.geometry.x.to_numpy())\n",
    "                land_lat_rad = np.deg2rad(land_sub.geometry.y.to_numpy())\n",
    "                land_X = np.vstack([land_lat_rad, land_lon_rad]).T\n",
    "\n",
    "                # 値（subsetに対応）\n",
    "                land_price_y_sub = land_sub[col_y].to_numpy(dtype=float)\n",
    "                land_price_y1_sub = land_sub[col_y_1].to_numpy(dtype=float)\n",
    "\n",
    "                # --- 4-2) NN構築（yearごと） ---\n",
    "                nn = NearestNeighbors(n_neighbors=n_neighbors, metric='haversine')\n",
    "                nn.fit(land_X)\n",
    "\n",
    "                # --- 4-3) 物件→近傍検索（このyearの行だけ） ---\n",
    "                Xq = prop_X_f[rows_mask]\n",
    "                idx_q_labels = idx_valid_f[rows_mask]\n",
    "\n",
    "                distances_rad, indices = nn.kneighbors(Xq)  # (m, k)\n",
    "                distances_m = distances_rad * R\n",
    "\n",
    "                # land_price を (m,k) へ\n",
    "                prices = land_price_y_sub[indices]\n",
    "                prices_prev = land_price_y1_sub[indices]\n",
    "\n",
    "                # 最近傍（1点目）\n",
    "                nearest_prices = prices[:, 0]\n",
    "                nearest_prev_prices = prices_prev[:, 0]\n",
    "                nearest_dists = distances_m[:, 0]\n",
    "\n",
    "                # 距離加重平均（ゼロ距離対応）\n",
    "                d_m = distances_m\n",
    "                zero_mask = (d_m == 0)\n",
    "                has_zero = zero_mask.any(axis=1)\n",
    "\n",
    "                w = np.zeros_like(d_m, dtype=float)\n",
    "                nz = ~zero_mask\n",
    "                w[nz] = 1.0 / d_m[nz]\n",
    "\n",
    "                w_sum = w.sum(axis=1, keepdims=True)\n",
    "                w_sum = np.where(w_sum == 0, 1.0, w_sum)\n",
    "                w = w / w_sum\n",
    "\n",
    "                wp = (w * prices).sum(axis=1)\n",
    "                wp_prev = (w * prices_prev).sum(axis=1)\n",
    "\n",
    "                if has_zero.any():\n",
    "                    rows = np.where(has_zero)[0]\n",
    "                    for r in rows:\n",
    "                        z = zero_mask[r]\n",
    "                        wp[r] = prices[r, z].mean()\n",
    "                        wp_prev[r] = prices_prev[r, z].mean()\n",
    "\n",
    "                # --- 4-4) 代入（MultiIndexラベルで安全に） ---\n",
    "                nearest_all.loc[idx_q_labels] = nearest_prices\n",
    "                nearest_prev_all.loc[idx_q_labels] = nearest_prev_prices\n",
    "                dist_all.loc[idx_q_labels] = nearest_dists\n",
    "                weighted_all.loc[idx_q_labels] = wp\n",
    "                weighted_prev_all.loc[idx_q_labels] = wp_prev\n",
    "\n",
    "    # --- 5) 列付与 ---\n",
    "    out = combined.copy()\n",
    "    out['nearest_land_price'] = nearest_all\n",
    "    out['distance_to_landpoint_m'] = dist_all\n",
    "    out['log_land_price'] = np.log1p(out['nearest_land_price'])\n",
    "\n",
    "    out['weighted_land_price_3'] = weighted_all\n",
    "    out['log_weighted_land_price_3'] = np.log1p(out['weighted_land_price_3'])\n",
    "\n",
    "    out['nearest_land_price_prev'] = nearest_prev_all\n",
    "    out['weighted_land_price_3_prev'] = weighted_prev_all\n",
    "\n",
    "    # yoy/dlog（前年がNaNの行は自然にNaNになる）\n",
    "    eps = 1.0\n",
    "    denom_n = np.maximum(out['nearest_land_price_prev'].to_numpy(), eps)\n",
    "    denom_w = np.maximum(out['weighted_land_price_3_prev'].to_numpy(), eps)\n",
    "\n",
    "    out['land_price_yoy_nearest'] = (out['nearest_land_price'] - out['nearest_land_price_prev']) / denom_n\n",
    "    out['land_price_yoy_w3'] = (out['weighted_land_price_3'] - out['weighted_land_price_3_prev']) / denom_w\n",
    "    out['land_price_dlog_nearest'] = np.log1p(out['nearest_land_price']) - np.log1p(out['nearest_land_price_prev'])\n",
    "    out['land_price_dlog_w3'] = np.log1p(out['weighted_land_price_3']) - np.log1p(out['weighted_land_price_3_prev'])\n",
    "\n",
    "    # --- 6) train/test に戻す（元index保持） ---\n",
    "    train_out = out.xs('train', level='__set__')\n",
    "    test_out = out.xs('test', level='__set__')\n",
    "\n",
    "    assert train_out.index.equals(train_df.index)\n",
    "    assert test_out.index.equals(test_df.index)\n",
    "\n",
    "    return train_out, test_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "269f079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_geo, test_df_geo = add_land_price_features_haversine(\n",
    "    train_df=train_df_geo,\n",
    "    test_df=test_df_geo,\n",
    "    land_gdf=land_gdf,\n",
    "    lat_col='lat_jgd',\n",
    "    lon_col='lon_jgd',\n",
    "    year_col='target_year'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c1470bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del land_gdf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0037e6",
   "metadata": {},
   "source": [
    "## 1kmメッシュ将来人口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a16436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu_pop_path = gis_path + '人口メッシュ/1km_mesh_2024_GEOJSON/future_pop_1km.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fa9e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu_pop_df = gpd.read_parquet(fu_pop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68d4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NOTE: 歪みが気になるのであればclipを用いる\n",
    "# display(fu_pop_df[['pop_trend_slope', 'pop_trend_rate']].describe())\n",
    "# q_low, q_high = fu_pop_df['pop_trend_rate'].quantile([0.01, 0.99])\n",
    "\n",
    "# fu_pop_df['pop_trend_rate_clip'] = (\n",
    "#     fu_pop_df['pop_trend_rate']\n",
    "#     .clip(lower=q_low, upper=q_high)\n",
    "# )\n",
    "\n",
    "# display(fu_pop_df[['pop_trend_slope', 'pop_trend_rate_clip']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36a20b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_point_gdf_from_lonlat(df: pd.DataFrame,\n",
    "                                 lon_col: str,\n",
    "                                 lat_col: str,\n",
    "                                 crs: str) -> gpd.GeoDataFrame:\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df.copy(),\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs=crs\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "def add_pop_knn_features(train_df_geo: pd.DataFrame,\n",
    "                         test_df_geo: pd.DataFrame,\n",
    "                         fu_pop_df: gpd.GeoDataFrame,\n",
    "                         pop_cols: list[str],\n",
    "                         lon_col: str = 'lon_jgd',\n",
    "                         lat_col: str = 'lat_jgd',\n",
    "                         in_crs: str = 'EPSG:4612',\n",
    "                         work_crs: str = 'EPSG:6677',\n",
    "                         ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    train/test の各点に対して、人口メッシュ点の\n",
    "    - 最近傍（nn）\n",
    "    - k近傍の距離加重平均（idw）\n",
    "    を付与する。\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) train/test を Point GeoDataFrame 化（EPSG:4612）\n",
    "    tr_gdf = _ensure_point_gdf_from_lonlat(train_df_geo, lon_col, lat_col, in_crs)\n",
    "    te_gdf = _ensure_point_gdf_from_lonlat(test_df_geo, lon_col, lat_col, in_crs)\n",
    "\n",
    "    # --- 2) population を Point 化（Polygon の場合は centroid）\n",
    "    pop_gdf = fu_pop_df.copy()\n",
    "    if not all(pop_gdf.geometry.geom_type == 'Point'):\n",
    "        pop_gdf = pop_gdf.to_crs(work_crs)\n",
    "        pop_gdf['geometry'] = pop_gdf.geometry.centroid\n",
    "        pop_gdf = pop_gdf.to_crs(in_crs)\n",
    "\n",
    "    # --- 3) 作業用 CRS（メートル系）へ\n",
    "    tr_m = tr_gdf.to_crs(work_crs)\n",
    "    te_m = te_gdf.to_crs(work_crs)\n",
    "    pop_m = pop_gdf.to_crs(work_crs)\n",
    "\n",
    "    # --- 4) KNN 検索（メートル座標で）\n",
    "    pop_xy = np.column_stack([pop_m.geometry.x.values, pop_m.geometry.y.values])\n",
    "    nn = NearestNeighbors(n_neighbors=3, algorithm='auto', metric='euclidean')\n",
    "    nn.fit(pop_xy)\n",
    "\n",
    "    def _apply(gdf_m: gpd.GeoDataFrame, prefix: str) -> pd.DataFrame:\n",
    "        q_xy = np.column_stack([gdf_m.geometry.x.values, gdf_m.geometry.y.values])\n",
    "        dists, idxs = nn.kneighbors(q_xy, return_distance=True)  # shapes: (n, k)\n",
    "\n",
    "        out = pd.DataFrame(index=gdf_m.index)\n",
    "\n",
    "        # 最近傍距離\n",
    "        out[f'{prefix}_dist_nn_m'] = dists[:, 0].astype(np.float32)\n",
    "\n",
    "        # 各 pop_col について nn を作る\n",
    "        for col in pop_cols:\n",
    "            vals = pop_m[col].values[idxs]  # (n, k)\n",
    "            out[f'{col}_nn'] = vals[:, 0]\n",
    "\n",
    "        return out\n",
    "\n",
    "    tr_feat = _apply(tr_m, 'pop')\n",
    "    te_feat = _apply(te_m, 'pop')\n",
    "\n",
    "    train_out = train_df_geo.join(tr_feat)\n",
    "    test_out = test_df_geo.join(te_feat)\n",
    "\n",
    "    return train_out, test_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca49e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_cols = [\n",
    "    'PTN_2020', # 2020年の総数人口\n",
    "    'RTA_2025', # 2025年男女計0～14歳人口/比率\n",
    "    'RTB_2025', # 2025年男女計15～64歳人口/比率\n",
    "    'RTC_2025', # 2025年男女計65歳以上人口/比率\n",
    "    'RTD_2025', # 2025年男女計75歳以上人口/比率\n",
    "    'RTE_2025', # 2025年男女計80歳以上人口/比率\n",
    "    'pop_trend_rate'\n",
    "]\n",
    "\n",
    "train_df_geo, test_df_geo = add_pop_knn_features(\n",
    "    train_df_geo=train_df_geo,\n",
    "    test_df_geo=test_df_geo,\n",
    "    fu_pop_df=fu_pop_df,\n",
    "    pop_cols=pop_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2a1776d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del fu_pop_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f584d",
   "metadata": {},
   "source": [
    "## 道路密度・道路延長メッシュ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bb724d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "road_mesh_path = gis_path + '道路密度・道路延長メッシュ/road_mesh_fe.parquet'\n",
    "road_path = gis_path + '道路/N01-07L-48-01.0a_GML/N01-07L-2K_Road.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68f19649",
   "metadata": {},
   "outputs": [],
   "source": [
    "road_mesh_df = gpd.read_parquet(road_mesh_path)\n",
    "road_df = read_dataframe(road_path)\n",
    "\n",
    "road_df = road_df.set_crs('EPSG:4612')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77ef09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Sequence\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RoadLineFeatureConfig:\n",
    "    # 距離・バッファ計算用の投影座標系（メートル単位）\n",
    "    proj_epsg: int = 6677\n",
    "\n",
    "    # road_df の道路種別コード列\n",
    "    road_type_col: str = 'N01_001'\n",
    "\n",
    "    # 主要道路とみなすコード（要件次第で調整）\n",
    "    # 例: 1=高速, 2=一般国道, 3=主要地方道, 5=特例都道 などを含める\n",
    "    major_codes: tuple[int, ...] = (1, 2, 3, 5)\n",
    "\n",
    "    # 近傍カウント/延長を作る半径（m）\n",
    "    radii_m: tuple[int, ...] = (100, 300, 500)\n",
    "\n",
    "    # 半径内の延長（m）を計算するか（重い）\n",
    "    compute_length_in_buffer: bool = False\n",
    "\n",
    "    # sjoin の集計で「同一路線名をユニークカウント」したい場合に使う列（なければ None）\n",
    "    # 例: 'N01_002'（路線名）など\n",
    "    unique_key_col: str | None = None\n",
    "\n",
    "\n",
    "def _to_points_gdf(\n",
    "    df: pd.DataFrame,\n",
    "    lon_col: str,\n",
    "    lat_col: str,\n",
    "    crs: str = 'EPSG:4612',\n",
    ") -> gpd.GeoDataFrame:\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df.copy(),\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs=crs,\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def _safe_int_series(s: pd.Series) -> pd.Series:\n",
    "    # 'unknown' 等が混ざっても落ちないようにする\n",
    "    return (\n",
    "        s.replace('unknown', pd.NA)\n",
    "         .astype('Int64')\n",
    "    )\n",
    "\n",
    "\n",
    "def _nearest_distance(\n",
    "    pts_p: gpd.GeoDataFrame,\n",
    "    lines_p: gpd.GeoDataFrame,\n",
    "    out_col: str,\n",
    ") -> pd.Series:\n",
    "    if lines_p is None or len(lines_p) == 0:\n",
    "        return pd.Series(np.nan, index=pts_p.index, name=out_col)\n",
    "\n",
    "    joined = gpd.sjoin_nearest(\n",
    "        pts_p[['geometry']],\n",
    "        lines_p[['geometry']],\n",
    "        how='left',\n",
    "        distance_col=out_col,\n",
    "    )\n",
    "\n",
    "    # 同距離などで点indexが重複するケースがあるので、点ごとに最小距離に潰す\n",
    "    s = joined[out_col]\n",
    "    if not s.index.is_unique:\n",
    "        s = s.groupby(level=0).min()\n",
    "\n",
    "    # pts_p の index に整列して返す（欠けは NaN）\n",
    "    s = s.reindex(pts_p.index)\n",
    "\n",
    "    return s.astype('float32')\n",
    "\n",
    "def _count_lines_within_radius(\n",
    "    pts_p: gpd.GeoDataFrame,\n",
    "    lines_p: gpd.GeoDataFrame,\n",
    "    radius_m: int,\n",
    "    out_col: str,\n",
    "    unique_key_col: str | None = None,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    半径バッファ内に intersect する road セグメント数（または unique_key_col のユニーク数）\n",
    "    \"\"\"\n",
    "    if lines_p is None or len(lines_p) == 0:\n",
    "        return pd.Series(0, index=pts_p.index, name=out_col, dtype='int32')\n",
    "\n",
    "    buf = pts_p[['geometry']].copy()\n",
    "    buf['geometry'] = buf.geometry.buffer(radius_m)\n",
    "\n",
    "    joined = gpd.sjoin(\n",
    "        buf,\n",
    "        lines_p,\n",
    "        how='left',\n",
    "        predicate='intersects',\n",
    "    )\n",
    "\n",
    "    if unique_key_col is not None and unique_key_col in joined.columns:\n",
    "        cnt = joined.groupby(level=0)[unique_key_col].nunique(dropna=True)\n",
    "    else:\n",
    "        # セグメント（行）数。index_right のユニークで数えるのが安定\n",
    "        if 'index_right' in joined.columns:\n",
    "            cnt = joined.groupby(level=0)['index_right'].nunique(dropna=True)\n",
    "        else:\n",
    "            cnt = joined.groupby(level=0).size()\n",
    "\n",
    "    out = pd.Series(0, index=pts_p.index, name=out_col, dtype='int32')\n",
    "    out.loc[cnt.index] = cnt.astype('int32')\n",
    "    return out\n",
    "\n",
    "\n",
    "def _length_lines_within_radius(\n",
    "    pts_p: gpd.GeoDataFrame,\n",
    "    lines_p: gpd.GeoDataFrame,\n",
    "    radius_m: int,\n",
    "    out_col: str,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    半径バッファ内の道路延長（m）合計。\n",
    "    注意: 計算コストが高いので radii が大きい・件数が多い場合は重くなる。\n",
    "    \"\"\"\n",
    "    if lines_p is None or len(lines_p) == 0:\n",
    "        return pd.Series(0.0, index=pts_p.index, name=out_col, dtype='float32')\n",
    "\n",
    "    buf = pts_p[['geometry']].copy()\n",
    "    buf['geometry'] = buf.geometry.buffer(radius_m)\n",
    "\n",
    "    joined = gpd.sjoin(\n",
    "        buf,\n",
    "        lines_p[['geometry']],\n",
    "        how='left',\n",
    "        predicate='intersects',\n",
    "    )\n",
    "\n",
    "    # index の対応でバッファ形状を引けるようにする\n",
    "    buf_geom = buf.geometry\n",
    "\n",
    "    # intersect した道路をバッファでクリップして長さ\n",
    "    # （ベクトル化しにくいので apply。重い）\n",
    "    def _clip_len(row) -> float:\n",
    "        if pd.isna(row.get('index_right')):\n",
    "            return 0.0\n",
    "        g_line = row['geometry_right']\n",
    "        g_buf = buf_geom.loc[row.name]\n",
    "        return float(g_line.intersection(g_buf).length)\n",
    "\n",
    "    # sjoin の結果に right geometry が入らない場合があるので付与\n",
    "    joined = joined.rename(columns={'geometry': 'geometry_left'})\n",
    "    joined = joined.merge(\n",
    "        lines_p[['geometry']].rename(columns={'geometry': 'geometry_right'}),\n",
    "        left_on='index_right',\n",
    "        right_index=True,\n",
    "        how='left',\n",
    "    )\n",
    "\n",
    "    lens = joined.apply(_clip_len, axis=1)\n",
    "    agg = lens.groupby(level=0).sum()\n",
    "\n",
    "    out = pd.Series(0.0, index=pts_p.index, name=out_col, dtype='float32')\n",
    "    out.loc[agg.index] = agg.astype('float32')\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_road_features(\n",
    "    df: pd.DataFrame,\n",
    "    road_poly: gpd.GeoDataFrame,\n",
    "    road_line: gpd.GeoDataFrame,\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    poly_join_predicate: str = 'within',\n",
    "    poly_keep_cols: Sequence[str] | None = None,\n",
    "    line_cfg: RoadLineFeatureConfig = RoadLineFeatureConfig(),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    df（train_df_geo / test_df_geo想定）に対して、\n",
    "      A) road_mesh_df(POLYGON) 由来の列を sjoin で付与\n",
    "      B) road_df(LINESTRING) 由来の近傍本数・主要道路距離等を付与\n",
    "    をまとめて実行する。\n",
    "\n",
    "    返り値: 元の df と同じ index の DataFrame（geometry は持たない）\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # ----------------------------\n",
    "    # A) POLYGON（メッシュ特徴量）を sjoin\n",
    "    # ----------------------------\n",
    "    pts_4612 = _to_points_gdf(out, lon_col=lon_col, lat_col=lat_col, crs='EPSG:4612')\n",
    "    poly_4612 = road_poly.to_crs('EPSG:4612')\n",
    "\n",
    "    joined_poly = gpd.sjoin(\n",
    "        pts_4612[['geometry']],\n",
    "        poly_4612,\n",
    "        how='left',\n",
    "        predicate=poly_join_predicate,\n",
    "    )\n",
    "\n",
    "    # 必要な列だけ付与（index_right 等の管理列は落とす）\n",
    "    if poly_keep_cols is None:\n",
    "        # 典型的にはあなたの road_mesh_df が保持している特徴量列をすべて付与でOK\n",
    "        drop_cols = [c for c in ['index_right', 'geometry'] if c in joined_poly.columns]\n",
    "        poly_cols = [c for c in joined_poly.columns if c not in drop_cols]\n",
    "    else:\n",
    "        poly_cols = [c for c in poly_keep_cols if c in joined_poly.columns]\n",
    "\n",
    "    out = out.join(joined_poly[poly_cols])\n",
    "\n",
    "    # ----------------------------\n",
    "    # B) LINESTRING（近傍・距離）特徴量\n",
    "    # ----------------------------\n",
    "    # 投影CRSへ\n",
    "    pts_p = pts_4612.to_crs(epsg=line_cfg.proj_epsg)\n",
    "\n",
    "    road_line_4612 = road_line\n",
    "    if road_line_4612.crs is None:\n",
    "        # ここはあなたのデータ取得仕様に合わせて変更可\n",
    "        road_line_4612 = road_line_4612.set_crs('EPSG:4612')\n",
    "\n",
    "    lines_p = road_line_4612.to_crs(epsg=line_cfg.proj_epsg)\n",
    "\n",
    "    # 道路種別コードを整形\n",
    "    if line_cfg.road_type_col in lines_p.columns:\n",
    "        lines_p = lines_p.copy()\n",
    "        lines_p[line_cfg.road_type_col] = _safe_int_series(lines_p[line_cfg.road_type_col])\n",
    "\n",
    "    # サブセット\n",
    "    major_mask = pd.Series(False, index=lines_p.index)\n",
    "    if line_cfg.road_type_col in lines_p.columns:\n",
    "        major_mask = lines_p[line_cfg.road_type_col].isin(list(line_cfg.major_codes))\n",
    "\n",
    "    lines_major = lines_p.loc[major_mask].copy()\n",
    "    lines_highway = lines_p.loc[lines_p[line_cfg.road_type_col].isin([1])] if line_cfg.road_type_col in lines_p.columns else lines_p.iloc[0:0]\n",
    "\n",
    "    # 最近傍距離\n",
    "    out['dist_to_road_any_m'] = _nearest_distance(pts_p, lines_p, 'dist_to_road_any_m')\n",
    "    out['dist_to_road_major_m'] = _nearest_distance(pts_p, lines_major, 'dist_to_road_major_m')\n",
    "    out['dist_to_road_highway_m'] = _nearest_distance(pts_p, lines_highway, 'dist_to_road_highway_m')\n",
    "\n",
    "    # 半径内の道路本数（セグメント数 or 路線名ユニーク数）\n",
    "    for r in line_cfg.radii_m:\n",
    "        out[f'road_cnt_any_in_{r}m'] = _count_lines_within_radius(\n",
    "            pts_p,\n",
    "            lines_p if line_cfg.unique_key_col is None else lines_p[[line_cfg.unique_key_col, 'geometry']].copy(),\n",
    "            radius_m=r,\n",
    "            out_col=f'road_cnt_any_in_{r}m',\n",
    "            unique_key_col=line_cfg.unique_key_col,\n",
    "        )\n",
    "        out[f'road_cnt_major_in_{r}m'] = _count_lines_within_radius(\n",
    "            pts_p,\n",
    "            lines_major if line_cfg.unique_key_col is None else lines_major[[line_cfg.unique_key_col, 'geometry']].copy(),\n",
    "            radius_m=r,\n",
    "            out_col=f'road_cnt_major_in_{r}m',\n",
    "            unique_key_col=line_cfg.unique_key_col,\n",
    "        )\n",
    "\n",
    "        # （任意）半径内延長\n",
    "        if line_cfg.compute_length_in_buffer:\n",
    "            out[f'road_len_any_in_{r}m'] = _length_lines_within_radius(\n",
    "                pts_p,\n",
    "                lines_p,\n",
    "                radius_m=r,\n",
    "                out_col=f'road_len_any_in_{r}m',\n",
    "            )\n",
    "            out[f'road_len_major_in_{r}m'] = _length_lines_within_radius(\n",
    "                pts_p,\n",
    "                lines_major,\n",
    "                radius_m=r,\n",
    "                out_col=f'road_len_major_in_{r}m',\n",
    "            )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbc6fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = RoadLineFeatureConfig(\n",
    "    proj_epsg=6677,\n",
    "    major_codes=(1, 2, 3, 5),\n",
    "    radii_m=(100, 300, 500),\n",
    "    compute_length_in_buffer=False,     # 重いのでまずはOFF推奨\n",
    "    unique_key_col=None,               # 'N01_002' で路線名ユニーク数にしたいなら 'N01_002'\n",
    ")\n",
    "\n",
    "train_df_geo = add_road_features(\n",
    "    df=train_df_geo,\n",
    "    road_poly=road_mesh_df,\n",
    "    road_line=road_df,\n",
    "    lon_col='lon_jgd',\n",
    "    lat_col='lat_jgd',\n",
    "    poly_keep_cols=None,   # 付けたいメッシュ列だけに絞るなら ['road_len_total', ...] を指定\n",
    "    line_cfg=cfg,\n",
    ")\n",
    "\n",
    "test_df_geo = add_road_features(\n",
    "    df=test_df_geo,\n",
    "    road_poly=road_mesh_df,\n",
    "    road_line=road_df,\n",
    "    lon_col='lon_jgd',\n",
    "    lat_col='lat_jgd',\n",
    "    poly_keep_cols=None,\n",
    "    line_cfg=cfg,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56beda09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del cfg, road_mesh_df, road_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b36070",
   "metadata": {},
   "source": [
    "## 用途地域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e5f3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "youto_path = gis_path + '用途地域/youto.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e261f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "youto_df = gpd.read_parquet(youto_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "568e63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_youto_features_sjoin(\n",
    "    df: pd.DataFrame,\n",
    "    youto_poly: gpd.GeoDataFrame,\n",
    "    *,\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    df_crs: str = 'EPSG:4612',   # ★ 明示的に 4612\n",
    "    keep_cols: tuple[str, ...] = (\n",
    "        'A29_004', 'A29_005', 'A29_006', 'A29_007',\n",
    "        'zone_group', 'zone_residential_rank',\n",
    "        'is_lowrise_residential',\n",
    "        'kenpei', 'youseki', 'kenpei_missing', 'youseki_missing',\n",
    "        'kenpei_bin', 'youseki_bin', 'density_cat'\n",
    "    ),\n",
    "    predicate_primary: str = 'intersects',\n",
    "    predicate_fallback: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # --- point gdf: pt_id を作って index 依存を消す ---\n",
    "    gdf_pts = gpd.GeoDataFrame(\n",
    "    out[[lon_col, lat_col]].copy(),\n",
    "    geometry=gpd.points_from_xy(out[lon_col], out[lat_col]),\n",
    "    crs=df_crs,   # ★ 4612\n",
    "    )\n",
    "    gdf_pts['pt_id'] = np.arange(len(gdf_pts), dtype=np.int64)\n",
    "\n",
    "\n",
    "    # --- poly ---\n",
    "    youto = youto_poly.copy()\n",
    "    if youto.crs is None:\n",
    "        raise ValueError('youto_poly.crs が None です。CRS を設定してください。')\n",
    "\n",
    "    if youto.crs.to_string() != df_crs:\n",
    "        youto = youto.to_crs(df_crs)\n",
    "\n",
    "    cols_exist = [c for c in keep_cols if c in youto.columns]\n",
    "    youto_small = youto[cols_exist + ['geometry']].copy()\n",
    "\n",
    "    # 代表選択キー（厳しい規制を優先：小さいほど優先）\n",
    "    if 'kenpei' not in youto_small.columns and 'A29_006' in youto_small.columns:\n",
    "        youto_small['kenpei'] = pd.to_numeric(youto_small['A29_006'], errors='coerce').astype('float64')\n",
    "    if 'youseki' not in youto_small.columns and 'A29_007' in youto_small.columns:\n",
    "        youto_small['youseki'] = pd.to_numeric(youto_small['A29_007'], errors='coerce').astype('float64')\n",
    "\n",
    "    youto_small['kenpei_key'] = youto_small.get('kenpei', pd.Series(np.nan, index=youto_small.index)).fillna(1e9)\n",
    "    youto_small['youseki_key'] = youto_small.get('youseki', pd.Series(np.nan, index=youto_small.index)).fillna(1e9)\n",
    "\n",
    "    # --- primary join ---\n",
    "    j1 = gpd.sjoin(\n",
    "        gdf_pts[['pt_id', 'geometry']],\n",
    "        youto_small,\n",
    "        how='left',\n",
    "        predicate=predicate_primary,\n",
    "    )\n",
    "\n",
    "    # --- fallback join（必要なときだけ）---\n",
    "    if predicate_fallback is not None:\n",
    "        miss = j1['index_right'].isna() if 'index_right' in j1.columns else j1.isna().any(axis=1)\n",
    "        miss_pt_ids = j1.loc[miss, 'pt_id'].unique()\n",
    "\n",
    "        if len(miss_pt_ids) > 0:\n",
    "            pts_miss = gdf_pts.loc[gdf_pts['pt_id'].isin(miss_pt_ids), ['pt_id', 'geometry']]\n",
    "            j2 = gpd.sjoin(\n",
    "                pts_miss,\n",
    "                youto_small,\n",
    "                how='left',\n",
    "                predicate=predicate_fallback,\n",
    "            )\n",
    "            j_all = pd.concat([j1, j2], axis=0, ignore_index=True)\n",
    "        else:\n",
    "            j_all = j1\n",
    "    else:\n",
    "        j_all = j1\n",
    "\n",
    "    # --- 複数マッチを 1件に落とす（pt_id単位）---\n",
    "    j_all = j_all.sort_values(['pt_id', 'kenpei_key', 'youseki_key'])\n",
    "    j_best = j_all.groupby('pt_id', sort=False).head(1).copy()\n",
    "\n",
    "    # --- df の行順に合わせて join ---\n",
    "    j_best = j_best.set_index('pt_id')\n",
    "\n",
    "    drop_cols = [c for c in ['index_right', 'geometry', 'kenpei_key', 'youseki_key'] if c in j_best.columns]\n",
    "    j_best = j_best.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    out2 = out.reset_index(drop=True).join(j_best, how='left')\n",
    "    out2.index = out.index\n",
    "\n",
    "    return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bffd7e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1523642298941537\n",
      "0.14870549730071062\n",
      "0.1523642298941537\n",
      "0.14870549730071062\n"
     ]
    }
   ],
   "source": [
    "train_df_geo = add_youto_features_sjoin(train_df_geo, youto_df)\n",
    "test_df_geo  = add_youto_features_sjoin(test_df_geo,  youto_df)\n",
    "\n",
    "print(train_df_geo['A29_005'].isna().mean())\n",
    "print(test_df_geo['A29_005'].isna().mean())\n",
    "print(train_df_geo['density_cat'].isna().mean())\n",
    "print(test_df_geo['density_cat'].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0ca9137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del youto_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfefa0ca",
   "metadata": {},
   "source": [
    "## 都市計画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2cf3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "koudoti_path = gis_path + '都市計画/koudoti.parquet'\n",
    "chikukei_path = gis_path + '都市計画/chikukei.parquet'\n",
    "tochiku_path = gis_path + '都市計画/tochiku.parquet'\n",
    "koudori_path = gis_path + '都市計画/koudori.parquet'\n",
    "toshisaisei_path = gis_path + '都市計画/toshisaisei.parquet'\n",
    "tokureiyouseki_path = gis_path + '都市計画/tokureiyouseki.parquet'\n",
    "kousoujyukyo_path = gis_path + '都市計画/kousoujyukyo.parquet'\n",
    "tokuteibousai_path = gis_path + '都市計画/tokuteibousai.parquet'\n",
    "fukkousaiseikyoten_path = gis_path + '都市計画/fukkousaiseikyoten.parquet'\n",
    "senbiki_path = gis_path + '都市計画/senbiki_df.parquet'\n",
    "bouka_path = gis_path + '都市計画/bouka_df.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ab59cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "koudoti_df = gpd.read_parquet(koudoti_path)\n",
    "chikukei_df = gpd.read_parquet(chikukei_path)\n",
    "tochiku_df = gpd.read_parquet(tochiku_path)\n",
    "koudori_df = gpd.read_parquet(koudori_path)\n",
    "toshisaisei_df = gpd.read_parquet(toshisaisei_path)\n",
    "tokureiyouseki_df = gpd.read_parquet(tokureiyouseki_path)\n",
    "kousoujyukyo_df = gpd.read_parquet(kousoujyukyo_path)\n",
    "tokuteibousai_df = gpd.read_parquet(tokuteibousai_path)\n",
    "fukkousaiseikyoten_df = gpd.read_parquet(fukkousaiseikyoten_path)\n",
    "senbiki_df = gpd.read_parquet(senbiki_path)\n",
    "bouka_df = gpd.read_parquet(bouka_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51924062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_points(df: pd.DataFrame, lon_col: str, lat_col: str, crs: str) -> gpd.GeoDataFrame:\n",
    "    pts = gpd.GeoDataFrame(\n",
    "        df[[lon_col, lat_col]].copy(),\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs=crs,\n",
    "    )\n",
    "    pts['__pt_id__'] = np.arange(len(pts), dtype=np.int64)\n",
    "    return pts\n",
    "\n",
    "\n",
    "def _sjoin_flags_one_layer(\n",
    "    df: pd.DataFrame,\n",
    "    poly: gpd.GeoDataFrame,\n",
    "    flag_cols: list[str],\n",
    "    *,\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    df_crs: str = 'EPSG:4612',\n",
    "    predicate_main: str = 'within',\n",
    "    predicate_fallback: str = 'intersects',\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    poly(フラグ列+geometry) を df(point) に空間結合し、flag_cols を付与。\n",
    "    複数マッチは OR(max) で集約。\n",
    "    \"\"\"\n",
    "    if poly is None or len(poly) == 0:\n",
    "        out = df.copy()\n",
    "        for c in flag_cols:\n",
    "            if c not in out.columns:\n",
    "                out[c] = 0\n",
    "            out[c] = out[c].fillna(0).astype('int8')\n",
    "        return out\n",
    "\n",
    "    if poly.crs is None:\n",
    "        raise ValueError('poly.crs is None. set_crs / to_crs を確認してください。')\n",
    "\n",
    "    poly_ = poly.to_crs(df_crs).copy()\n",
    "    pts = _make_points(df, lon_col, lat_col, df_crs)\n",
    "\n",
    "    # main join\n",
    "    j1 = gpd.sjoin(\n",
    "        pts[['__pt_id__', 'geometry']],\n",
    "        poly_[flag_cols + ['geometry']],\n",
    "        how='left',\n",
    "        predicate=predicate_main,\n",
    "    )\n",
    "\n",
    "    # fallback join（mainで当たらなかったptだけ）\n",
    "    if predicate_fallback is not None:\n",
    "        miss_ids = j1.loc[j1['index_right'].isna(), '__pt_id__'].unique()\n",
    "        if len(miss_ids) > 0:\n",
    "            pts_miss = pts.loc[pts['__pt_id__'].isin(miss_ids), ['__pt_id__', 'geometry']]\n",
    "            j2 = gpd.sjoin(\n",
    "                pts_miss,\n",
    "                poly_[flag_cols + ['geometry']],\n",
    "                how='left',\n",
    "                predicate=predicate_fallback,\n",
    "            )\n",
    "            j = pd.concat([j1, j2], ignore_index=True)\n",
    "        else:\n",
    "            j = j1\n",
    "    else:\n",
    "        j = j1\n",
    "\n",
    "    # OR集約（フラグなのでmaxでよい）\n",
    "    for c in flag_cols:\n",
    "        j[c] = pd.to_numeric(j[c], errors='coerce')\n",
    "\n",
    "    agg = (\n",
    "        j.groupby('__pt_id__', sort=False)[flag_cols]\n",
    "        .max()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    out = df.reset_index(drop=True).copy()\n",
    "    out = out.join(agg.set_index('__pt_id__'), how='left')\n",
    "    out.index = df.index\n",
    "\n",
    "    for c in flag_cols:\n",
    "        out[c] = out[c].fillna(0).astype('int8')\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e21e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cityplan_layers(\n",
    "    df: pd.DataFrame,\n",
    "    layers: dict[str, tuple[gpd.GeoDataFrame, list[str]]],\n",
    "    *,\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    df_crs: str = 'EPSG:4612',\n",
    "    predicate_main: str = 'within',\n",
    "    predicate_fallback: str = 'intersects',\n",
    ") -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    for name, (poly, cols) in layers.items():\n",
    "        # 念のため cols が poly にあるかチェック\n",
    "        if poly is not None and len(poly) > 0:\n",
    "            missing = [c for c in cols if c not in poly.columns]\n",
    "            if missing:\n",
    "                raise KeyError(f'layer={name} missing cols={missing}')\n",
    "\n",
    "        out = _sjoin_flags_one_layer(\n",
    "            out,\n",
    "            poly,\n",
    "            cols,\n",
    "            lon_col=lon_col,\n",
    "            lat_col=lat_col,\n",
    "            df_crs=df_crs,\n",
    "            predicate_main=predicate_main,\n",
    "            predicate_fallback=predicate_fallback,\n",
    "        )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79122108",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = {\n",
    "    'senbiki': (senbiki_df, ['is_urbanized_area', 'is_urban_control_area']),\n",
    "    'bouka': (bouka_df, ['is_fireproof_area', 'is_quasi_fireproof_area']),\n",
    "    'koudoti': (koudoti_df, ['has_height_limit']),\n",
    "    'chikukei': (chikukei_df, ['has_district_plan']),\n",
    "    'tochiku': (tochiku_df, ['is_land_readjustment_area']),\n",
    "    'koudori': (koudori_df, ['is_high_utilization_area']),\n",
    "    'toshisaisei': (toshisaisei_df, ['is_urban_renaissance_area']),\n",
    "    'tokureiyouseki': (tokureiyouseki_df, ['is_special_far_area']),\n",
    "    'kousoujyukyo': (kousoujyukyo_df, ['is_highrise_residential_area']),\n",
    "    'tokuteibousai': (tokuteibousai_df, ['is_disaster_prevention_block']),\n",
    "    'fukkousaiseikyoten': (fukkousaiseikyoten_df, ['is_redevelopment_core_area']),\n",
    "}\n",
    "\n",
    "train_df_geo = add_cityplan_layers(train_df_geo, layers, lon_col='lon_jgd', lat_col='lat_jgd')\n",
    "test_df_geo  = add_cityplan_layers(test_df_geo,  layers, lon_col='lon_jgd', lat_col='lat_jgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd7de9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del layers\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c8118",
   "metadata": {},
   "source": [
    "## 標高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef5847b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_path = gis_path + '標高/height.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3176171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_df = gpd.read_parquet(height_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a66c1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_height_features(\n",
    "    df: pd.DataFrame,\n",
    "    height_poly: gpd.GeoDataFrame,\n",
    "    lon_col='lon_jgd',\n",
    "    lat_col='lat_jgd',\n",
    "):\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df.copy(),\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs='EPSG:4612',\n",
    "    )\n",
    "\n",
    "    height_poly = height_poly.set_crs('EPSG:4612')\n",
    "\n",
    "    joined = gpd.sjoin(\n",
    "        gdf,\n",
    "        height_poly,\n",
    "        how='left',\n",
    "        predicate='within',\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(joined.drop(columns='geometry'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b51563e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT_NUM_COLS = [\n",
    "    'G04c_002', 'G04c_003', 'G04c_004',\n",
    "    'G04c_006', 'G04c_008', 'G04c_010'\n",
    "]\n",
    "\n",
    "def coerce_height_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    for c in HEIGHT_NUM_COLS:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(\n",
    "                out[c].replace('unknown', np.nan),\n",
    "                errors='coerce'\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "805a8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 結合（sjoin）\n",
    "train_df_geo = add_height_features(train_df_geo, height_df)\n",
    "test_df_geo  = add_height_features(test_df_geo, height_df)\n",
    "\n",
    "# 2) 数値化（unknown -> NaN, object -> float）\n",
    "train_df_geo = coerce_height_numeric(train_df_geo)\n",
    "test_df_geo  = coerce_height_numeric(test_df_geo)\n",
    "\n",
    "# 3) 特徴量作成\n",
    "for df in [train_df_geo, test_df_geo]:\n",
    "    df['elev_mean']   = df['G04c_002']\n",
    "    df['elev_range']  = df['G04c_003'] - df['G04c_004']\n",
    "\n",
    "    df['slope_mean']  = df['G04c_010']\n",
    "    df['slope_max']   = df['G04c_006']\n",
    "    df['slope_range'] = df['G04c_006'] - df['G04c_008']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23f1239e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del height_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d630c",
   "metadata": {},
   "source": [
    "## 駅別乗降者数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1682149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:6668\n"
     ]
    }
   ],
   "source": [
    "file = Path(gis_path + '駅別乗降者数/S12-24_GML/S12-24_NumberOfPassengers.geojson')\n",
    "eki_gdf = read_dataframe(file) # 高速化しないとめちゃ時間かかる\n",
    "print(eki_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17befbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eki_gdf = eki_gdf.rename(columns={\n",
    "    'S12_001': '駅名',\n",
    "    'S12_003': '路線名',\n",
    "    'S12_037': '乗降客数_2018',\n",
    "    'S12_041': '乗降客数_2019',\n",
    "    'S12_045': '乗降客数_2020',\n",
    "    'S12_049': '乗降客数_2021',\n",
    "    'S12_053': '乗降客数_2022',\n",
    "    'S12_057': '乗降客数_2023',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f6267d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cols = [\n",
    "    '乗降客数_2018', '乗降客数_2019',\n",
    "    '乗降客数_2020', '乗降客数_2021',\n",
    "    '乗降客数_2022', '乗降客数_2023'\n",
    "]\n",
    "\n",
    "eki_gdf[year_cols] = eki_gdf[year_cols].replace(0, np.nan)\n",
    "\n",
    "normal_cols = [\n",
    "    '乗降客数_2018',\n",
    "    '乗降客数_2019',\n",
    "    '乗降客数_2022',\n",
    "    '乗降客数_2023',\n",
    "]\n",
    "\n",
    "eki_gdf['passenger_normal'] = (\n",
    "    eki_gdf[normal_cols]\n",
    "    .median(axis=1, skipna=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8ed1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_agg = (\n",
    "    eki_gdf\n",
    "    .dropna(subset=['passenger_normal'])\n",
    "    .groupby('駅名', as_index=False)\n",
    "    .agg(\n",
    "        passenger_normal=('passenger_normal', 'sum'),\n",
    "        geometry=('geometry', 'first')  # ほぼ同一なので代表\n",
    "    )\n",
    ")\n",
    "\n",
    "station_agg['passenger_normal_log'] = np.log1p(\n",
    "    station_agg['passenger_normal']\n",
    ")\n",
    "\n",
    "station_agg = gpd.GeoDataFrame(\n",
    "    station_agg,\n",
    "    geometry='geometry',\n",
    "    crs='EPSG:6668'   # 駅データのCRS\n",
    ")\n",
    "\n",
    "station_agg['geometry'] = station_agg.geometry.centroid\n",
    "\n",
    "station_agg['lon'] = station_agg.geometry.x\n",
    "station_agg['lat'] = station_agg.geometry.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbbb88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "def add_station_power_summax3(\n",
    "    df: pd.DataFrame,\n",
    "    station_agg: pd.DataFrame,\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    station_lon_col: str = 'lon',\n",
    "    station_lat_col: str = 'lat',\n",
    "    passenger_col: str = 'passenger_normal_log',\n",
    "    k: int = 3,\n",
    "    tau_km: float = 1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    物件ごとに近傍3駅の駅力スコアを作成（ソフト減衰）\n",
    "      - station_power_sum3\n",
    "      - station_power_max3\n",
    "\n",
    "    score_i = passenger_log_i * exp(-distance_km / tau_km)\n",
    "\n",
    "    前提:\n",
    "      - df: 物件DF（lon_jgd / lat_jgd）\n",
    "      - station_agg: 1駅=1行（lon / lat / passenger_normal_log）\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # --- 物件座標（ラジアン） ---\n",
    "    prop_xy = np.deg2rad(\n",
    "        out[[lat_col, lon_col]].to_numpy()\n",
    "    )\n",
    "\n",
    "    # --- 駅座標（ラジアン） ---\n",
    "    station_xy = np.deg2rad(\n",
    "        station_agg[[station_lat_col, station_lon_col]].to_numpy()\n",
    "    )\n",
    "\n",
    "    # --- BallTree (haversine) ---\n",
    "    tree = BallTree(station_xy, metric='haversine')\n",
    "\n",
    "    # k近傍探索（距離はラジアン）\n",
    "    dist_rad, idx = tree.query(prop_xy, k=k)\n",
    "\n",
    "    # km に変換\n",
    "    dist_km = dist_rad * 6371.0\n",
    "\n",
    "    # 駅の passenger_log\n",
    "    pax = station_agg[passenger_col].to_numpy()\n",
    "\n",
    "    # --- スコア計算 ---\n",
    "    scores = np.zeros_like(dist_km)\n",
    "\n",
    "    for i in range(k):\n",
    "        scores[:, i] = pax[idx[:, i]] * np.exp(-dist_km[:, i] / tau_km)\n",
    "\n",
    "    # --- 集約 ---\n",
    "    out['station_power_sum3'] = scores.sum(axis=1)\n",
    "    out['station_power_max3'] = scores.max(axis=1)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f5298d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_geo = add_station_power_summax3(\n",
    "    train_df_geo,\n",
    "    station_agg,\n",
    "    lon_col='lon_jgd',\n",
    "    lat_col='lat_jgd',\n",
    "    tau_km=1.0,   # まずは 1km\n",
    ")\n",
    "\n",
    "test_df_geo = add_station_power_summax3(\n",
    "    test_df_geo,\n",
    "    station_agg,\n",
    "    lon_col='lon_jgd',\n",
    "    lat_col='lat_jgd',\n",
    "    tau_km=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6910f636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del station_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9c68f",
   "metadata": {},
   "source": [
    "## 災害危険区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60d7fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_path = gis_path + '災害危険区域/disaster.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "433f713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_df = gpd.read_parquet(disaster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "beb5dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_disaster_features_sjoin(\n",
    "    df: pd.DataFrame,\n",
    "    disaster_poly: gpd.GeoDataFrame,\n",
    "    *,\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    df_crs: str = 'EPSG:4612',\n",
    "    reason_col: str = 'A48_007',\n",
    "    predicate_primary: str = 'within',\n",
    "    predicate_fallback: str | None = None,\n",
    "    alpha_multi: float = 0.3,\n",
    "    score_clip_max: float | None = 2.0,\n",
    "    weight_map: dict[int, float] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    災害危険区域（A48）ポリゴンと物件代表点を空間結合し、\n",
    "    指定理由（A48_007）を重み付けして災害スコアを付与する。\n",
    "\n",
    "    付与する列:\n",
    "      - disaster_hit (0/1)\n",
    "      - disaster_n_types\n",
    "      - disaster_score_sum\n",
    "      - disaster_score_max\n",
    "      - disaster_score (max + alpha*(n_types-1), optional clip)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # --- point gdf ---\n",
    "    gdf_pts = gpd.GeoDataFrame(\n",
    "        out[[lon_col, lat_col]].copy(),\n",
    "        geometry=gpd.points_from_xy(out[lon_col], out[lat_col]),\n",
    "        crs=df_crs,\n",
    "    )\n",
    "    gdf_pts['pt_id'] = np.arange(len(gdf_pts), dtype=np.int64)\n",
    "\n",
    "    # --- poly ---\n",
    "    dis = disaster_poly.copy()\n",
    "    if dis.crs is None:\n",
    "        raise ValueError('disaster_poly.crs が None です。CRS を設定してください。')\n",
    "\n",
    "    if dis.crs.to_string() != df_crs:\n",
    "        dis = dis.to_crs(df_crs)\n",
    "\n",
    "    # reason_col を確実に持つ\n",
    "    if reason_col not in dis.columns:\n",
    "        raise ValueError(f'reason_col={reason_col} が disaster_poly に存在しません。')\n",
    "\n",
    "    dis_small = dis[[reason_col, 'geometry']].copy()\n",
    "\n",
    "    # --- join primary ---\n",
    "    j1 = gpd.sjoin(\n",
    "        gdf_pts[['pt_id', 'geometry']],\n",
    "        dis_small,\n",
    "        how='left',\n",
    "        predicate=predicate_primary,\n",
    "    )\n",
    "\n",
    "    # --- fallback join（必要なら）---\n",
    "    if predicate_fallback is not None:\n",
    "        miss = j1['index_right'].isna()\n",
    "        miss_pt_ids = j1.loc[miss, 'pt_id'].unique()\n",
    "\n",
    "        if len(miss_pt_ids) > 0:\n",
    "            pts_miss = gdf_pts.loc[gdf_pts['pt_id'].isin(miss_pt_ids), ['pt_id', 'geometry']]\n",
    "            j2 = gpd.sjoin(\n",
    "                pts_miss,\n",
    "                dis_small,\n",
    "                how='left',\n",
    "                predicate=predicate_fallback,\n",
    "            )\n",
    "            j_all = pd.concat([j1, j2], axis=0, ignore_index=True)\n",
    "        else:\n",
    "            j_all = j1\n",
    "    else:\n",
    "        j_all = j1\n",
    "\n",
    "    # --- 重み付け ---\n",
    "    if weight_map is None:\n",
    "        weight_map = {\n",
    "            1: 0.8,  # 水害（河川）\n",
    "            2: 0.7,  # 水害（海）\n",
    "            3: 1.0,  # 水害（河川・海）\n",
    "            4: 0.9,  # 急傾斜地崩壊等\n",
    "            5: 0.9,  # 地すべり等\n",
    "            6: 0.6,  # 火山災害\n",
    "            7: 0.4,  # その他\n",
    "        }\n",
    "\n",
    "    j_all['reason_code'] = pd.to_numeric(j_all[reason_col], errors='coerce')\n",
    "    j_all['reason_w'] = j_all['reason_code'].map(weight_map).astype('float64')\n",
    "\n",
    "    # --- pt_id 単位に集計（複数マッチを“集計して残す”）---\n",
    "    agg = (\n",
    "        j_all.groupby('pt_id', as_index=False)\n",
    "             .agg(\n",
    "                 disaster_hit=('reason_code', lambda x: int(x.notna().any())),\n",
    "                 disaster_n_types=('reason_code', lambda x: x.dropna().nunique()),\n",
    "                 disaster_score_sum=('reason_w', 'sum'),\n",
    "                 disaster_score_max=('reason_w', 'max'),\n",
    "             )\n",
    "    )\n",
    "\n",
    "    # --- 元df順で戻す ---\n",
    "    feat = pd.DataFrame({'pt_id': np.arange(len(out), dtype=np.int64)})\n",
    "    feat = feat.merge(agg, on='pt_id', how='left')\n",
    "\n",
    "    for c in ['disaster_hit', 'disaster_n_types', 'disaster_score_sum', 'disaster_score_max']:\n",
    "        feat[c] = feat[c].fillna(0)\n",
    "\n",
    "    # 複合リスク強調（max + alpha*(n_types-1)）\n",
    "    feat['disaster_score'] = (\n",
    "        feat['disaster_score_max'] + alpha_multi * np.maximum(feat['disaster_n_types'] - 1, 0)\n",
    "    )\n",
    "\n",
    "    if score_clip_max is not None:\n",
    "        feat['disaster_score'] = feat['disaster_score'].clip(upper=score_clip_max)\n",
    "\n",
    "    feat = feat.drop(columns=['pt_id'])\n",
    "\n",
    "    out2 = out.reset_index(drop=True).join(feat, how='left')\n",
    "    out2.index = out.index\n",
    "\n",
    "    return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62769a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_map = {\n",
    "    1: 0.8,  # 水害（河川）\n",
    "    2: 0.7,  # 水害（海）\n",
    "    3: 1.0,  # 水害（河川・海）\n",
    "    4: 0.9,  # 急傾斜地崩壊等\n",
    "    5: 0.9,  # 地すべり等\n",
    "    6: 0.6,  # 火山災害\n",
    "    7: 0.4,  # その他\n",
    "}\n",
    "\n",
    "train_df_geo = add_disaster_features_sjoin(train_df_geo, disaster_df, predicate_primary='within', weight_map=weight_map)\n",
    "test_df_geo  = add_disaster_features_sjoin(test_df_geo,  disaster_df, predicate_primary='within', weight_map=weight_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "294b6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train全体 disaster_hit率: 0.5025%\n",
      "該当件数: 565 / 112,437\n"
     ]
    }
   ],
   "source": [
    "rate_all = test_df_geo['disaster_hit'].mean()\n",
    "\n",
    "print(f'train全体 disaster_hit率: {rate_all:.4%}')\n",
    "print(f'該当件数: {test_df_geo[\"disaster_hit\"].sum():,.0f} / {len(test_df_geo):,.0f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f70e7bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del disaster_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f491ef",
   "metadata": {},
   "source": [
    "## 学校"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64395c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_path = Path(gis_path + '学校/P29-23_GML/P29-23.geojson')\n",
    "school_df = gpd.read_file(school_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b942bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_use = school_df.loc[\n",
    "    school_df['P29_007'] != 2\n",
    "].copy()\n",
    "\n",
    "school_use['school_type_norm'] = school_use['P29_003']\n",
    "\n",
    "# 中等教育学校 → 中学校に寄せる\n",
    "school_use.loc[\n",
    "    school_use['school_type_norm'] == '16003',\n",
    "    'school_type_norm'\n",
    "] = '16002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db8622e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_elem = school_use.loc[\n",
    "    school_use['school_type_norm'] == '16001'\n",
    "].copy()\n",
    "\n",
    "school_junior = school_use.loc[\n",
    "    school_use['school_type_norm'] == '16002'\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43205344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_school_features_from_gdfs(\n",
    "    df: pd.DataFrame,\n",
    "    school_elem: gpd.GeoDataFrame,\n",
    "    school_junior: gpd.GeoDataFrame,\n",
    "    *,\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    df_crs: str = 'EPSG:4612',\n",
    "    # 距離の計算用投影（m単位）\n",
    "    proj_crs: str = 'EPSG:3857',\n",
    "    # bin しきい値\n",
    "    elem_threshold_m: int = 500,\n",
    "    junior_threshold_m: int = 1000,\n",
    "    # 欠損対策（通常は不要だが安全に）\n",
    "    fill_missing_distance_m: float = 10_000.0,\n",
    "    add_log_features: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    物件代表点（lon/lat）に対し、school_elem（小学校）・school_junior（中学校）\n",
    "    の最近傍距離を計算して特徴量追加する。\n",
    "\n",
    "    追加列:\n",
    "      - dist_elem_school_m\n",
    "      - dist_junior_school_m\n",
    "      - elem_school_500m\n",
    "      - junior_school_1km\n",
    "      - (optional) dist_elem_school_log, dist_junior_school_log\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # --- 入力チェック ---\n",
    "    if lon_col not in out.columns or lat_col not in out.columns:\n",
    "        raise ValueError(f'df に {lon_col}/{lat_col} がありません。')\n",
    "\n",
    "    if school_elem is None or len(school_elem) == 0:\n",
    "        raise ValueError('school_elem が空です。')\n",
    "    if school_junior is None or len(school_junior) == 0:\n",
    "        raise ValueError('school_junior が空です。')\n",
    "\n",
    "    if school_elem.crs is None or school_junior.crs is None:\n",
    "        raise ValueError('school_elem / school_junior の crs が None です。')\n",
    "\n",
    "    # --- 物件点GDF ---\n",
    "    gdf_pts = gpd.GeoDataFrame(\n",
    "        out[[lon_col, lat_col]].copy(),\n",
    "        geometry=gpd.points_from_xy(out[lon_col], out[lat_col]),\n",
    "        crs=df_crs,\n",
    "    )\n",
    "    gdf_pts['pt_id'] = np.arange(len(gdf_pts), dtype=np.int64)\n",
    "\n",
    "    # --- 距離計算は投影してm単位で ---\n",
    "    pts_proj = gdf_pts.to_crs(proj_crs)\n",
    "    elem_proj = school_elem.to_crs(proj_crs)\n",
    "    junior_proj = school_junior.to_crs(proj_crs)\n",
    "\n",
    "    # sjoin_nearest で最近傍距離（高速）\n",
    "    j_elem = gpd.sjoin_nearest(\n",
    "        pts_proj[['pt_id', 'geometry']],\n",
    "        elem_proj[['geometry']],\n",
    "        how='left',\n",
    "        distance_col='dist_elem_school_m',\n",
    "    )\n",
    "    j_jun = gpd.sjoin_nearest(\n",
    "        pts_proj[['pt_id', 'geometry']],\n",
    "        junior_proj[['geometry']],\n",
    "        how='left',\n",
    "        distance_col='dist_junior_school_m',\n",
    "    )\n",
    "\n",
    "    # pt_id に揃えて距離列を抽出\n",
    "    dist_elem = j_elem.set_index('pt_id')['dist_elem_school_m']\n",
    "    dist_jun = j_jun.set_index('pt_id')['dist_junior_school_m']\n",
    "\n",
    "    feat = pd.DataFrame({'pt_id': np.arange(len(out), dtype=np.int64)})\n",
    "    feat = feat.join(dist_elem, on='pt_id')\n",
    "    feat = feat.join(dist_jun, on='pt_id')\n",
    "\n",
    "    # 欠損（通常は起きないが）\n",
    "    feat['dist_elem_school_m'] = pd.to_numeric(feat['dist_elem_school_m'], errors='coerce').fillna(fill_missing_distance_m)\n",
    "    feat['dist_junior_school_m'] = pd.to_numeric(feat['dist_junior_school_m'], errors='coerce').fillna(fill_missing_distance_m)\n",
    "\n",
    "    # bin（閾値効果）\n",
    "    feat['elem_school_500m'] = (feat['dist_elem_school_m'] <= elem_threshold_m).astype('int8')\n",
    "    feat['junior_school_1km'] = (feat['dist_junior_school_m'] <= junior_threshold_m).astype('int8')\n",
    "\n",
    "    # log（Tableau/非線形吸収用：入れるかは後で判断可）\n",
    "    if add_log_features:\n",
    "        feat['dist_elem_school_log'] = np.log1p(feat['dist_elem_school_m'])\n",
    "        feat['dist_junior_school_log'] = np.log1p(feat['dist_junior_school_m'])\n",
    "\n",
    "    feat = feat.drop(columns=['pt_id'])\n",
    "\n",
    "    out2 = out.reset_index(drop=True).join(feat, how='left')\n",
    "    out2.index = out.index\n",
    "    return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22316f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_geo = add_school_features_from_gdfs(train_df_geo, school_elem, school_junior)\n",
    "test_df_geo  = add_school_features_from_gdfs(test_df_geo,  school_elem, school_junior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "341ca6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del school_elem, school_junior\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0bc5d7",
   "metadata": {},
   "source": [
    "## 病院"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2cee976",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_path = Path(gis_path + '病院/P04-20_GML/P04-20.geojson')\n",
    "hospital_df = gpd.read_file(hospital_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f1c7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_medical_features_from_gdf(\n",
    "    df: pd.DataFrame,\n",
    "    medical_gdf: gpd.GeoDataFrame,\n",
    "    *,\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    df_crs: str = 'EPSG:4612',\n",
    "    proj_crs: str = 'EPSG:3857',  # m距離用（全国一律の近似）\n",
    "    type_col: str = 'P04_001',     # 1=病院,2=診療所,3=歯科\n",
    "    beds_col: str = 'P04_008',     # 病床数（病院のみ想定）\n",
    "    emergency_col: str = 'P04_009',# 救急告示: 指定あり=1, 指定なし=9（想定）\n",
    "    disaster_col: str = 'P04_010', # 災害拠点: 基幹=1, 地域=2, 指定なし=9（想定）\n",
    "    use_types: tuple[int, ...] = (1, 2),  # まずは病院・診療所\n",
    "    # bin しきい値\n",
    "    hospital_threshold_m: int = 1000,\n",
    "    clinic_threshold_m: int = 500,\n",
    "    # 欠損距離の埋め（通常起きないはず）\n",
    "    fill_missing_distance_m: float = 10_000.0,\n",
    "    add_log_features: bool = True,\n",
    "    add_nearest_attributes: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    医療機関データ（点）から、物件点への最近傍距離を算出し特徴量追加。\n",
    "    - 病院(1)と診療所(2)をデフォルトで利用（歯科(3)は原則後回し）\n",
    "    - 距離は投影してm単位で計算\n",
    "    - 任意で「最寄り病院の病床数」「救急/災害拠点フラグ」も付与可能\n",
    "\n",
    "    追加される列（use_typesにより変動）:\n",
    "      病院:\n",
    "        dist_hospital_m, hospital_1km, dist_hospital_log\n",
    "        (optional) hospital_beds_nearest, hospital_is_emergency_nearest, hospital_is_disaster_nearest\n",
    "      診療所:\n",
    "        dist_clinic_m, clinic_500m, dist_clinic_log\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # --- 入力チェック ---\n",
    "    if lon_col not in out.columns or lat_col not in out.columns:\n",
    "        raise ValueError(f'df に {lon_col}/{lat_col} がありません。')\n",
    "    if medical_gdf is None or len(medical_gdf) == 0:\n",
    "        raise ValueError('medical_gdf が空です。')\n",
    "    if medical_gdf.crs is None:\n",
    "        raise ValueError('medical_gdf.crs が None です。')\n",
    "\n",
    "    # --- 物件点 ---\n",
    "    gdf_pts = gpd.GeoDataFrame(\n",
    "        out[[lon_col, lat_col]].copy(),\n",
    "        geometry=gpd.points_from_xy(out[lon_col], out[lat_col]),\n",
    "        crs=df_crs,\n",
    "    )\n",
    "    gdf_pts['pt_id'] = np.arange(len(gdf_pts), dtype=np.int64)\n",
    "\n",
    "    # --- 医療データ前処理（タイプ絞り） ---\n",
    "    med = medical_gdf.copy()\n",
    "    med[type_col] = pd.to_numeric(med[type_col], errors='coerce')\n",
    "\n",
    "    med = med.loc[med[type_col].isin(use_types)].copy()\n",
    "    if len(med) == 0:\n",
    "        raise ValueError(f'use_types={use_types} の条件で医療機関が0件になりました。')\n",
    "\n",
    "    # --- 投影して距離(m) ---\n",
    "    pts_proj = gdf_pts.to_crs(proj_crs)\n",
    "    med_proj = med.to_crs(proj_crs)\n",
    "\n",
    "    # --- タイプ別に最近傍 ---\n",
    "    feat = pd.DataFrame({'pt_id': np.arange(len(out), dtype=np.int64)})\n",
    "\n",
    "    def _nearest_for_type(type_code: int, dist_col_out: str, take_attrs: bool):\n",
    "        m_sub = med_proj.loc[med_proj[type_col] == type_code].copy()\n",
    "        if len(m_sub) == 0:\n",
    "            # 施設が存在しない場合は遠距離で埋める\n",
    "            feat[dist_col_out] = fill_missing_distance_m\n",
    "            return None\n",
    "\n",
    "        # 必要な属性列だけ保持（重い列は落とす）\n",
    "        keep = ['geometry']\n",
    "        if take_attrs:\n",
    "            for c in [beds_col, emergency_col, disaster_col]:\n",
    "                if c in m_sub.columns:\n",
    "                    keep.append(c)\n",
    "        m_sub = m_sub[keep].copy()\n",
    "\n",
    "        j = gpd.sjoin_nearest(\n",
    "            pts_proj[['pt_id', 'geometry']],\n",
    "            m_sub,\n",
    "            how='left',\n",
    "            distance_col=dist_col_out,\n",
    "        )\n",
    "        # 代表行（pt_idごとに1行）\n",
    "        j = j.drop_duplicates('pt_id').set_index('pt_id')\n",
    "\n",
    "        feat[dist_col_out] = pd.to_numeric(j[dist_col_out], errors='coerce')\n",
    "\n",
    "        return j\n",
    "\n",
    "    # 病院(1)\n",
    "    j_hosp = None\n",
    "    if 1 in use_types:\n",
    "        j_hosp = _nearest_for_type(1, 'dist_hospital_m', add_nearest_attributes)\n",
    "\n",
    "    # 診療所(2)\n",
    "    j_clinic = None\n",
    "    if 2 in use_types:\n",
    "        j_clinic = _nearest_for_type(2, 'dist_clinic_m', False)\n",
    "\n",
    "    # 欠損埋め\n",
    "    for c in ['dist_hospital_m', 'dist_clinic_m']:\n",
    "        if c in feat.columns:\n",
    "            feat[c] = feat[c].fillna(fill_missing_distance_m)\n",
    "\n",
    "    # --- bin ---\n",
    "    if 'dist_hospital_m' in feat.columns:\n",
    "        feat['hospital_1km'] = (feat['dist_hospital_m'] <= hospital_threshold_m).astype('int8')\n",
    "        if add_log_features:\n",
    "            feat['dist_hospital_log'] = np.log1p(feat['dist_hospital_m'])\n",
    "\n",
    "    if 'dist_clinic_m' in feat.columns:\n",
    "        feat['clinic_500m'] = (feat['dist_clinic_m'] <= clinic_threshold_m).astype('int8')\n",
    "        if add_log_features:\n",
    "            feat['dist_clinic_log'] = np.log1p(feat['dist_clinic_m'])\n",
    "\n",
    "    # --- 最寄り病院の属性（任意） ---\n",
    "    if add_nearest_attributes and (j_hosp is not None):\n",
    "        # 病床数\n",
    "        if beds_col in j_hosp.columns:\n",
    "            feat['hospital_beds_nearest'] = pd.to_numeric(j_hosp[beds_col], errors='coerce').fillna(0.0)\n",
    "\n",
    "        # 救急告示（指定あり=1、それ以外=0）\n",
    "        if emergency_col in j_hosp.columns:\n",
    "            e = pd.to_numeric(j_hosp[emergency_col], errors='coerce')\n",
    "            feat['hospital_is_emergency_nearest'] = (e == 1).astype('int8')\n",
    "\n",
    "        # 災害拠点（基幹=1,地域=2 を 1扱い、それ以外0）\n",
    "        if disaster_col in j_hosp.columns:\n",
    "            d = pd.to_numeric(j_hosp[disaster_col], errors='coerce')\n",
    "            feat['hospital_is_disaster_nearest'] = (d.isin([1, 2])).astype('int8')\n",
    "\n",
    "    feat = feat.drop(columns=['pt_id'])\n",
    "\n",
    "    out2 = out.reset_index(drop=True).join(feat, how='left')\n",
    "    out2.index = out.index\n",
    "    return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aac063dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_geo = add_medical_features_from_gdf(train_df_geo, hospital_df, use_types=(1, 2))\n",
    "test_df_geo  = add_medical_features_from_gdf(test_df_geo,  hospital_df, use_types=(1, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63980356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del hospital_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113be28",
   "metadata": {},
   "source": [
    "## 洪水浸水"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041d89f",
   "metadata": {},
   "source": [
    "#### 内水"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40217ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ame_path = gis_path + '洪水浸水/ame.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c8eeb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ame_df = gpd.read_parquet(ame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "953d7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_depth_upper_m(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    A51_005 のような文字列から「浸水深の上限(m)」を作る。\n",
    "    例:\n",
    "      '0.3m未満' -> 0.3\n",
    "      '0.5m以上1m未満' -> 1.0\n",
    "      '1m以上3m未満' -> 3.0\n",
    "      '3m以上5m未満' -> 5.0\n",
    "      '5m以上10m未満' -> 10.0\n",
    "      '10m以上20m未満' -> 20.0\n",
    "      '20m以上' -> 30.0 (上限が無いので便宜上 30m に丸め)\n",
    "    \"\"\"\n",
    "    ss = s.astype('string').fillna('')\n",
    "\n",
    "    # 'Xm未満' -> X\n",
    "    m1 = ss.str.extract(r'^\\s*([0-9.]+)\\s*m\\s*未満\\s*$')[0]\n",
    "\n",
    "    # 'A以上B未満' -> B\n",
    "    m2 = ss.str.extract(r'^\\s*[0-9.]+\\s*m\\s*以上\\s*([0-9.]+)\\s*m\\s*未満\\s*$')[0]\n",
    "\n",
    "    # 'A以上' -> 便宜上 30m\n",
    "    m3 = ss.str.contains(r'以上\\s*$', na=False)\n",
    "\n",
    "    out = pd.to_numeric(m1, errors='coerce')\n",
    "    out = out.fillna(pd.to_numeric(m2, errors='coerce'))\n",
    "    out = out.astype('float32')\n",
    "\n",
    "    out = out.where(~m3, 30.0).astype('float32')\n",
    "    out = out.where(out > 0, np.nan).astype('float32')\n",
    "    return out\n",
    "\n",
    "def _depth_rank_from_upper(upper: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    上限(m)をランク整数に（単調増加なら何でもよい）\n",
    "    \"\"\"\n",
    "    # 0.3, 1, 3, 5, 10, 20, 30 を想定\n",
    "    bins = [0, 0.3, 1, 3, 5, 10, 20, 30, np.inf]\n",
    "    # 0.3->1, 1->2, ... のような順位にする\n",
    "    r = pd.cut(upper, bins=bins, labels=False, include_lowest=True)\n",
    "    return r.astype('float32')\n",
    "\n",
    "def add_ame_features(prop_df: pd.DataFrame, ame_poly: gpd.GeoDataFrame,\n",
    "                     lon_col: str = 'lon_jgd', lat_col: str = 'lat_jgd',\n",
    "                     prefix: str = 'ame', work_crs: str = 'EPSG:6677',\n",
    "                     max_dist_m: float = 10000.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    雨水出水(内水)ポリゴンを物件(point)に突合して特徴量を返す。\n",
    "    \"\"\"\n",
    "    # 物件点\n",
    "    prop_g = gpd.GeoDataFrame(\n",
    "        prop_df[[lon_col, lat_col]].copy(),\n",
    "        geometry=gpd.points_from_xy(prop_df[lon_col], prop_df[lat_col]),\n",
    "        crs='EPSG:4326'\n",
    "    ).to_crs(work_crs)\n",
    "\n",
    "    a = ame_poly[['A51_005', 'geometry']].copy()\n",
    "    a = a.to_crs(work_crs)\n",
    "    a['depth_upper_m'] = _parse_depth_upper_m(a['A51_005'])\n",
    "    a['depth_rank'] = _depth_rank_from_upper(a['depth_upper_m'])\n",
    "\n",
    "    # intersects（複数ヒット想定）\n",
    "    j = prop_g[['geometry']].sjoin(a[['depth_upper_m', 'depth_rank', 'geometry']],\n",
    "                                  how='left', predicate='intersects')\n",
    "\n",
    "    out = pd.DataFrame(index=prop_df.index)\n",
    "\n",
    "    out[f'{prefix}_in'] = (\n",
    "        j['index_right'].notna()\n",
    "        .groupby(level=0).max()\n",
    "        .reindex(prop_df.index, fill_value=False)\n",
    "        .astype('int8')\n",
    "    )\n",
    "\n",
    "    out[f'{prefix}_depth_max_m'] = (\n",
    "        pd.to_numeric(j['depth_upper_m'], errors='coerce')\n",
    "        .groupby(level=0).max()\n",
    "        .reindex(prop_df.index)\n",
    "        .astype('float32')\n",
    "    )\n",
    "\n",
    "    out[f'{prefix}_depth_rank_max'] = (\n",
    "        pd.to_numeric(j['depth_rank'], errors='coerce')\n",
    "        .groupby(level=0).max()\n",
    "        .reindex(prop_df.index)\n",
    "        .astype('float32')\n",
    "    )\n",
    "\n",
    "    # 最近傍距離（複数行→minで1行化）\n",
    "    near = prop_g[['geometry']].sjoin_nearest(\n",
    "        a[['geometry']],\n",
    "        how='left',\n",
    "        max_distance=max_dist_m,\n",
    "        distance_col=f'{prefix}_dist_m'\n",
    "    )\n",
    "    out[f'{prefix}_dist_m'] = (\n",
    "        near[f'{prefix}_dist_m']\n",
    "        .groupby(level=0).min()\n",
    "        .reindex(prop_df.index)\n",
    "        .astype('float32')\n",
    "    )\n",
    "\n",
    "    # 追加の安定特徴\n",
    "    out[f'{prefix}_depth_max_log1p'] = np.log1p(out[f'{prefix}_depth_max_m']).astype('float32')\n",
    "\n",
    "    out[f'{prefix}_depth_ge1m'] = (out[f'{prefix}_depth_max_m'] >= 1.0).astype('int8')\n",
    "    out[f'{prefix}_depth_ge3m'] = (out[f'{prefix}_depth_max_m'] >= 3.0).astype('int8')\n",
    "\n",
    "    # 10km以上は同一視してログ変換\n",
    "    out['ame_dist_m_clip'] = out['ame_dist_m'].clip(upper=10000)\n",
    "    out['ame_dist_log'] = np.log1p(out['ame_dist_m_clip'])\n",
    "\n",
    "    out['ame_depth_effect'] = (\n",
    "        out['ame_depth_max_log1p']\n",
    "        .where(out['ame_dist_m'] <= 2000, 0.0)\n",
    "    )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65bfc96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ame = add_ame_features(train_df_geo, ame_df, prefix='ame', max_dist_m=10000.0)\n",
    "test_ame  = add_ame_features(test_df_geo,  ame_df, prefix='ame', max_dist_m=10000.0)\n",
    "\n",
    "train_df_geo = train_df_geo.join(train_ame)\n",
    "test_df_geo  = test_df_geo.join(test_ame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61eac218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ame_in hit rate train: 0.0007501566261087479\n",
      "ame_in hit rate test : 0.0007204034259185143\n"
     ]
    }
   ],
   "source": [
    "print('ame_in hit rate train:', train_df_geo['ame_in'].mean())\n",
    "print('ame_in hit rate test :', test_df_geo['ame_in'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47460e71",
   "metadata": {},
   "source": [
    "#### 外水"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8b24aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = Path(gis_path + '洪水浸水/1次メッシュ')\n",
    "keikaku_files = sorted(in_dir.glob('*/10_計画規模/*.geojson'))\n",
    "saidai_files = sorted(in_dir.glob('*/20_想定最大規模/*.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "60e77676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "def _extract_region_id(fp: str | Path) -> str:\n",
    "    name = Path(fp).name\n",
    "    m = re.search(r'_(\\d{4})\\.geojson$', name)\n",
    "    if not m:\n",
    "        raise ValueError(f'Unexpected filename format: {name}')\n",
    "    return m.group(1)\n",
    "\n",
    "\n",
    "def _pair_files_by_region(files: list[str | Path]) -> dict[str, Path]:\n",
    "    out: dict[str, Path] = {}\n",
    "    for fp in files:\n",
    "        rid = _extract_region_id(fp)\n",
    "        out[rid] = Path(fp)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _read_geojson_mincols(fp: Path, cols: list[str]) -> gpd.GeoDataFrame:\n",
    "    gdf = gpd.read_file(fp, engine='pyogrio')\n",
    "    keep = [c for c in cols if c in gdf.columns] + ['geometry']\n",
    "    return gdf[keep]\n",
    "\n",
    "\n",
    "def _agg_rank_and_in(joined: gpd.GeoDataFrame, prop_index: pd.Index, rank_col: str) -> tuple[pd.Series, pd.Series]:\n",
    "    in_flag = (\n",
    "        joined['index_right'].notna()\n",
    "        .groupby(level=0).max()\n",
    "        .reindex(prop_index, fill_value=False)\n",
    "        .astype('int8')\n",
    "    )\n",
    "    rank_max = (\n",
    "        pd.to_numeric(joined[rank_col], errors='coerce')\n",
    "        .groupby(level=0).max()\n",
    "        .reindex(prop_index)\n",
    "        .astype('float32')\n",
    "    )\n",
    "    return in_flag, rank_max\n",
    "\n",
    "\n",
    "def _nearest_dist_m(\n",
    "    prop_m: gpd.GeoDataFrame,\n",
    "    poly_m: gpd.GeoDataFrame,\n",
    "    prop_index: pd.Index,\n",
    "    max_dist_m: float,\n",
    "    dist_name: str\n",
    ") -> pd.Series:\n",
    "    near = prop_m[['geometry']].sjoin_nearest(\n",
    "        poly_m[['geometry']],\n",
    "        how='left',\n",
    "        max_distance=max_dist_m,\n",
    "        distance_col=dist_name,\n",
    "    )\n",
    "    dist = (\n",
    "        near[dist_name]\n",
    "        .groupby(level=0).min()\n",
    "        .reindex(prop_index)\n",
    "        .astype('float32')\n",
    "    )\n",
    "    return dist\n",
    "\n",
    "\n",
    "def add_kouzui_keikaku_saidai_features_stream(\n",
    "    prop_df: pd.DataFrame,\n",
    "    keikaku_files: list[str | Path],\n",
    "    saidai_files: list[str | Path],\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    src_crs: str = 'EPSG:6668',\n",
    "    work_crs_m: str = 'EPSG:6677',\n",
    "    max_dist_m: float = 10000.0,\n",
    "    prefix: str = 'kouzui',\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    prop_g = gpd.GeoDataFrame(\n",
    "        prop_df[[lon_col, lat_col]].copy(),\n",
    "        geometry=gpd.points_from_xy(prop_df[lon_col], prop_df[lat_col]),\n",
    "        crs='EPSG:4326',\n",
    "        index=prop_df.index,\n",
    "    ).to_crs(src_crs)\n",
    "\n",
    "    out = pd.DataFrame(index=prop_df.index)\n",
    "    out[f'{prefix}_keikaku_in'] = np.int8(0)\n",
    "    out[f'{prefix}_keikaku_rank_max'] = np.float32(np.nan)\n",
    "    out[f'{prefix}_keikaku_dist_m'] = np.float32(np.nan)\n",
    "\n",
    "    out[f'{prefix}_saidai_in'] = np.int8(0)\n",
    "    out[f'{prefix}_saidai_rank_max'] = np.float32(np.nan)\n",
    "    out[f'{prefix}_saidai_dist_m'] = np.float32(np.nan)\n",
    "\n",
    "    keikaku_map = _pair_files_by_region(keikaku_files)\n",
    "    saidai_map = _pair_files_by_region(saidai_files)\n",
    "    region_ids = sorted(set(keikaku_map.keys()) | set(saidai_map.keys()))\n",
    "\n",
    "    if verbose:\n",
    "        print(f'[{prefix}] regions={len(region_ids)} (keikaku={len(keikaku_map)}, saidai={len(saidai_map)})')\n",
    "\n",
    "    for i, rid in enumerate(region_ids, 1):\n",
    "        fp_k = keikaku_map.get(rid)\n",
    "        fp_s = saidai_map.get(rid)\n",
    "        if fp_k is None and fp_s is None:\n",
    "            continue\n",
    "\n",
    "        k_gdf = None\n",
    "        s_gdf = None\n",
    "\n",
    "        try:\n",
    "            if fp_k is not None:\n",
    "                k_gdf = _read_geojson_mincols(fp_k, cols=['A31b_101'])\n",
    "                if k_gdf.crs is None:\n",
    "                    k_gdf = k_gdf.set_crs(src_crs)\n",
    "                else:\n",
    "                    k_gdf = k_gdf.to_crs(src_crs)\n",
    "\n",
    "            if fp_s is not None:\n",
    "                s_gdf = _read_geojson_mincols(fp_s, cols=['A31b_201'])\n",
    "                if s_gdf.crs is None:\n",
    "                    s_gdf = s_gdf.set_crs(src_crs)\n",
    "                else:\n",
    "                    s_gdf = s_gdf.to_crs(src_crs)\n",
    "\n",
    "            bounds_list = []\n",
    "            if k_gdf is not None and len(k_gdf) > 0:\n",
    "                bounds_list.append(k_gdf.total_bounds)\n",
    "            if s_gdf is not None and len(s_gdf) > 0:\n",
    "                bounds_list.append(s_gdf.total_bounds)\n",
    "            if not bounds_list:\n",
    "                continue\n",
    "\n",
    "            minx = min(b[0] for b in bounds_list)\n",
    "            miny = min(b[1] for b in bounds_list)\n",
    "            maxx = max(b[2] for b in bounds_list)\n",
    "            maxy = max(b[3] for b in bounds_list)\n",
    "\n",
    "            cand_mask = (\n",
    "                (prop_g.geometry.x >= minx) & (prop_g.geometry.x <= maxx) &\n",
    "                (prop_g.geometry.y >= miny) & (prop_g.geometry.y <= maxy)\n",
    "            )\n",
    "            cand_idx = prop_g.index[cand_mask]\n",
    "            if len(cand_idx) == 0:\n",
    "                continue\n",
    "\n",
    "            prop_sub = prop_g.loc[cand_idx, ['geometry']]\n",
    "            prop_sub_m = prop_sub.to_crs(work_crs_m)\n",
    "\n",
    "            # 計画規模\n",
    "            if k_gdf is not None and len(k_gdf) > 0 and 'A31b_101' in k_gdf.columns:\n",
    "                k_m = k_gdf[['A31b_101', 'geometry']].copy().to_crs(work_crs_m)\n",
    "\n",
    "                j_k = prop_sub_m.sjoin(k_m, how='left', predicate='intersects')\n",
    "                in_k, rank_k = _agg_rank_and_in(j_k, prop_sub_m.index, rank_col='A31b_101')\n",
    "                dist_k = _nearest_dist_m(\n",
    "                    prop_sub_m, k_m, prop_sub_m.index,\n",
    "                    max_dist_m, dist_name=f'{prefix}_keikaku_dist_m'\n",
    "                )\n",
    "\n",
    "                out.loc[cand_idx, f'{prefix}_keikaku_in'] = in_k.values\n",
    "                out.loc[cand_idx, f'{prefix}_keikaku_rank_max'] = rank_k.values\n",
    "                out.loc[cand_idx, f'{prefix}_keikaku_dist_m'] = dist_k.values\n",
    "\n",
    "            # 最大規模\n",
    "            if s_gdf is not None and len(s_gdf) > 0 and 'A31b_201' in s_gdf.columns:\n",
    "                s_m = s_gdf[['A31b_201', 'geometry']].copy().to_crs(work_crs_m)\n",
    "\n",
    "                j_s = prop_sub_m.sjoin(s_m, how='left', predicate='intersects')\n",
    "                in_s, rank_s = _agg_rank_and_in(j_s, prop_sub_m.index, rank_col='A31b_201')\n",
    "                dist_s = _nearest_dist_m(\n",
    "                    prop_sub_m, s_m, prop_sub_m.index,\n",
    "                    max_dist_m, dist_name=f'{prefix}_saidai_dist_m'\n",
    "                )\n",
    "\n",
    "                out.loc[cand_idx, f'{prefix}_saidai_in'] = in_s.values\n",
    "                out.loc[cand_idx, f'{prefix}_saidai_rank_max'] = rank_s.values\n",
    "                out.loc[cand_idx, f'{prefix}_saidai_dist_m'] = dist_s.values\n",
    "\n",
    "        finally:\n",
    "            del k_gdf, s_gdf\n",
    "            gc.collect()\n",
    "\n",
    "        if verbose and (i % 50 == 0 or i == len(region_ids)):\n",
    "            print(f'[{prefix}] processed {i}/{len(region_ids)} regions')\n",
    "\n",
    "    # 型整形\n",
    "    out[f'{prefix}_keikaku_in'] = out[f'{prefix}_keikaku_in'].astype('int8')\n",
    "    out[f'{prefix}_saidai_in'] = out[f'{prefix}_saidai_in'].astype('int8')\n",
    "\n",
    "    out[f'{prefix}_keikaku_rank_max'] = out[f'{prefix}_keikaku_rank_max'].astype('float32')\n",
    "    out[f'{prefix}_saidai_rank_max'] = out[f'{prefix}_saidai_rank_max'].astype('float32')\n",
    "\n",
    "    # --- dist: clip + log1p を標準装備にする（ここが追加） ---\n",
    "    for col in [f'{prefix}_keikaku_dist_m', f'{prefix}_saidai_dist_m']:\n",
    "        s = pd.to_numeric(out[col], errors='coerce').astype('float32')\n",
    "        s = s.fillna(np.float32(max_dist_m))\n",
    "        s = s.clip(lower=np.float32(0.0), upper=np.float32(max_dist_m)).astype('float32')\n",
    "        out[col] = s\n",
    "        out[f'{col}_log1p'] = np.log1p(s).astype('float32')\n",
    "\n",
    "    # rank>=3 フラグ\n",
    "    out[f'{prefix}_keikaku_rank_ge3'] = (out[f'{prefix}_keikaku_rank_max'] >= 3).astype('int8')\n",
    "    out[f'{prefix}_saidai_rank_ge3'] = (out[f'{prefix}_saidai_rank_max'] >= 3).astype('int8')\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d399e473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kouzui] regions=105 (keikaku=101, saidai=105)\n",
      "[kouzui] processed 50/105 regions\n",
      "[kouzui] regions=105 (keikaku=101, saidai=105)\n",
      "[kouzui] processed 50/105 regions\n"
     ]
    }
   ],
   "source": [
    "train_kouzui = add_kouzui_keikaku_saidai_features_stream(\n",
    "    prop_df=train_df_geo,                  # lon_jgd/lat_jgd を持つDF\n",
    "    keikaku_files=keikaku_files,        # Pathのlist\n",
    "    saidai_files=saidai_files,          # Pathのlist\n",
    "    lon_col='lon_jgd',\n",
    "    lat_col='lat_jgd',\n",
    "    max_dist_m=10000.0,\n",
    "    prefix='kouzui',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "test_kouzui = add_kouzui_keikaku_saidai_features_stream(\n",
    "    prop_df=test_df_geo,\n",
    "    keikaku_files=keikaku_files,\n",
    "    saidai_files=saidai_files,\n",
    "    lon_col='lon_jgd',\n",
    "    lat_col='lat_jgd',\n",
    "    max_dist_m=10000.0,\n",
    "    prefix='kouzui',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_df_geo = pd.concat([train_df_geo, train_kouzui], axis=1)\n",
    "test_df_geo = pd.concat([test_df_geo, test_kouzui], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f17ea371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kouzui_keikaku_in hit rate train: 0.042077466723821455\n",
      "kouzui_keikaku_in hit rate test : 0.0449407223600772\n",
      "kouzui_saidai_in hit rate train: 0.08733691649904925\n",
      "kouzui_saidai_in hit rate test : 0.0898814447201544\n"
     ]
    }
   ],
   "source": [
    "print('kouzui_keikaku_in hit rate train:', train_df_geo['kouzui_keikaku_in'].mean())\n",
    "print('kouzui_keikaku_in hit rate test :', test_df_geo['kouzui_keikaku_in'].mean())\n",
    "\n",
    "print('kouzui_saidai_in hit rate train:', train_df_geo['kouzui_saidai_in'].mean())\n",
    "print('kouzui_saidai_in hit rate test :', test_df_geo['kouzui_saidai_in'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db1f61",
   "metadata": {},
   "source": [
    "## 土砂災害"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c8a9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "kyuusha_path = gis_path + '土砂災害/kyuusha.parquet'\n",
    "doshamesh_path = gis_path + '土砂災害/doshamesh.parquet'\n",
    "doshakeikai_path = gis_path + '土砂災害/doshakeikai.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18413389",
   "metadata": {},
   "outputs": [],
   "source": [
    "kyuusha_df = gpd.read_parquet(kyuusha_path)\n",
    "doshamesh_df = gpd.read_parquet(doshamesh_path)\n",
    "doshakeikai_df = gpd.read_parquet(doshakeikai_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7b6ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# GeoJSONが巨大なときの制限解除（必要なら）\n",
    "os.environ.setdefault('OGR_GEOJSON_MAX_OBJ_SIZE', '0')\n",
    "\n",
    "def _ensure_cols(df, cols, name='df'):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f'{name}: missing columns: {missing}')\n",
    "\n",
    "def _to_points_gdf(df: pd.DataFrame, lon_col: str, lat_col: str, crs='EPSG:4326') -> gpd.GeoDataFrame:\n",
    "    _ensure_cols(df, [lon_col, lat_col], name='properties')\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df.copy(),\n",
    "        geometry=gpd.points_from_xy(df[lon_col].astype(float), df[lat_col].astype(float)),\n",
    "        crs=crs,\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "def _to_crs_safe(gdf: gpd.GeoDataFrame, crs: str) -> gpd.GeoDataFrame:\n",
    "    if gdf.crs is None:\n",
    "        return gdf.set_crs(crs)\n",
    "    else:\n",
    "        return gdf.to_crs(crs)\n",
    "\n",
    "def _parse_mm_per(s: pd.Series) -> pd.Series:\n",
    "    # '74mm/24hr' '38mm/hr' などを float に\n",
    "    s = s.astype('string')\n",
    "    s = s.str.replace('mm/24hr', '', regex=False).str.replace('mm/hr', '', regex=False)\n",
    "    s = s.str.replace('mm', '', regex=False).str.strip()\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def _parse_m3(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype('string').str.replace('m3', '', regex=False).str.strip()\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def _parse_m(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype('string').str.replace('m', '', regex=False).str.strip()\n",
    "    return pd.to_numeric(s, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ee3a0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 物件\n",
    "train_prop = _to_points_gdf(train_df_geo, 'lon_jgd', 'lat_jgd', crs='EPSG:4326')\n",
    "test_prop  = _to_points_gdf(test_df_geo,  'lon_jgd', 'lat_jgd', crs='EPSG:4326')\n",
    "\n",
    "# まず4326へ統一\n",
    "kyuusha_4326      = _to_crs_safe(kyuusha_df, 'EPSG:4326')\n",
    "dosham_mesh_4326  = _to_crs_safe(doshamesh_df, 'EPSG:4326')\n",
    "doshakeikai_4326  = _to_crs_safe(doshakeikai_df, 'EPSG:4326')\n",
    "\n",
    "# 距離用（メートル系）\n",
    "train_m = train_prop.to_crs('EPSG:3857')\n",
    "test_m  = test_prop.to_crs('EPSG:3857')\n",
    "\n",
    "kyuusha_m     = kyuusha_4326.to_crs('EPSG:3857')\n",
    "mesh_m        = dosham_mesh_4326.to_crs('EPSG:3857')\n",
    "keikai_m      = doshakeikai_4326.to_crs('EPSG:3857')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7843a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polygon_hit_and_dist(\n",
    "    prop_m: gpd.GeoDataFrame,\n",
    "    poly_m: gpd.GeoDataFrame,\n",
    "    prefix: str,\n",
    "    max_dist_m: float = 5000.0\n",
    ") -> pd.DataFrame:\n",
    "    # hit（intersects）: 1物件が複数ヒットするので groupby(max) で集約\n",
    "    hit = prop_m[['geometry']].sjoin(poly_m[['geometry']], how='left', predicate='intersects')\n",
    "    in_flag = (\n",
    "        hit['index_right']\n",
    "        .notna()\n",
    "        .groupby(level=0)\n",
    "        .max()\n",
    "        .reindex(prop_m.index, fill_value=False)\n",
    "        .astype('int8')\n",
    "        .rename(f'{prefix}_in')\n",
    "    )\n",
    "\n",
    "    # nearest（距離）: タイ等で複数行になることがあるので groupby(min) で集約\n",
    "    near = prop_m[['geometry']].sjoin_nearest(\n",
    "        poly_m[['geometry']],\n",
    "        how='left',\n",
    "        max_distance=max_dist_m,\n",
    "        distance_col=f'{prefix}_dist_m'\n",
    "    )\n",
    "    dist = (\n",
    "        near[f'{prefix}_dist_m']\n",
    "        .groupby(level=0)\n",
    "        .min()\n",
    "        .reindex(prop_m.index)\n",
    "        .astype('float32')\n",
    "        .rename(f'{prefix}_dist_m')\n",
    "    )\n",
    "\n",
    "    out = pd.concat([in_flag, dist], axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "train_kyuusha = add_polygon_hit_and_dist(train_m, kyuusha_m, prefix='kyuusha', max_dist_m=10000.0)\n",
    "test_kyuusha  = add_polygon_hit_and_dist(test_m,  kyuusha_m, prefix='kyuusha', max_dist_m=10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "790eb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genshou_code: 1=急傾斜地の崩壊, 2=土石流, 3=地滑り\n",
    "# kuiki_code: 1=警戒(指定済), 2=特別警戒(指定済), 3=警戒(指定前), 4=特別警戒(指定前)\n",
    "\n",
    "def add_keikai_features(\n",
    "    prop_m: gpd.GeoDataFrame,\n",
    "    keikai_m: gpd.GeoDataFrame,\n",
    "    target_ym_series: pd.Series,          # 物件側のYYYYMM（indexはprop_m.indexと同じ）\n",
    "    max_dist_m: float = 5000.0\n",
    ") -> pd.DataFrame:\n",
    "    # fail-fast\n",
    "    if target_ym_series is None:\n",
    "        raise ValueError('target_ym_series is required for kouji_date leak control')\n",
    "    if not target_ym_series.index.equals(prop_m.index):\n",
    "        # 同じ並びでなくてもreindexで揃える\n",
    "        target_ym_series = target_ym_series.reindex(prop_m.index)\n",
    "\n",
    "    k = keikai_m[['genshou_code', 'kuiki_code', 'kouji_date', 'geometry']].copy()\n",
    "\n",
    "    # intersects（複数ヒット想定）\n",
    "    j = prop_m[['geometry']].sjoin(k, how='left', predicate='intersects')\n",
    "\n",
    "    def _flag(col, val, name):\n",
    "        return (\n",
    "            (j[col] == val)\n",
    "            .groupby(level=0)\n",
    "            .max()\n",
    "            .reindex(prop_m.index, fill_value=False)\n",
    "            .astype('int8')\n",
    "            .rename(name)\n",
    "        )\n",
    "\n",
    "    # 現象別\n",
    "    f_g1 = _flag('genshou_code', 1, 'keikai_genshou1_in')\n",
    "    f_g2 = _flag('genshou_code', 2, 'keikai_genshou2_in')\n",
    "    f_g3 = _flag('genshou_code', 3, 'keikai_genshou3_in')\n",
    "\n",
    "    # 区域別\n",
    "    f_k1 = _flag('kuiki_code', 1, 'keikai_kuiki1_in')\n",
    "    f_k2 = _flag('kuiki_code', 2, 'keikai_kuiki2_in')\n",
    "    f_k3 = _flag('kuiki_code', 3, 'keikai_kuiki3_in')\n",
    "    f_k4 = _flag('kuiki_code', 4, 'keikai_kuiki4_in')\n",
    "\n",
    "    # =========================\n",
    "    # 公示日（最小）with leak control\n",
    "    # =========================\n",
    "    kd = pd.to_datetime(j['kouji_date'], errors='coerce')\n",
    "\n",
    "    # target_ym（YYYYMM）→ datetime（当月1日）に変換（prop indexへ付与）\n",
    "    ym = target_ym_series.astype('float64')\n",
    "    y = np.floor(ym / 100.0)\n",
    "    m = ym - y * 100.0\n",
    "    base = pd.to_datetime(\n",
    "        dict(year=y, month=m, day=np.ones_like(y)),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # j（多重ヒット）の index（=prop index）に合わせて base を展開\n",
    "    base_for_j = base.reindex(j.index)\n",
    "\n",
    "    # 未来の公示日を無効化（NaT）\n",
    "    kd = kd.where((kd.isna()) | (base_for_j.isna()) | (kd <= base_for_j), pd.NaT)\n",
    "\n",
    "    # 未来無効化後の最小公示日\n",
    "    kouji_min = kd.groupby(level=0).min().reindex(prop_m.index).rename('keikai_kouji_date_min')\n",
    "\n",
    "    # 派生：経過年数（取引日 - 公示日）\n",
    "    kouji_age_year = ((base - kouji_min).dt.days / 365.25).astype('float32').rename('keikai_kouji_age_year')\n",
    "\n",
    "    # nearest距離（タイ等で複数行→groupby(min)で1行化）\n",
    "    near = prop_m[['geometry']].sjoin_nearest(\n",
    "        k[['geometry']],\n",
    "        how='left',\n",
    "        max_distance=max_dist_m,\n",
    "        distance_col='keikai_dist_m'\n",
    "    )\n",
    "    dist = (\n",
    "        near['keikai_dist_m']\n",
    "        .groupby(level=0)\n",
    "        .min()\n",
    "        .reindex(prop_m.index)\n",
    "        .astype('float32')\n",
    "        .rename('keikai_dist_m')\n",
    "    )\n",
    "\n",
    "    out = pd.concat(\n",
    "        [f_g1, f_g2, f_g3, f_k1, f_k2, f_k3, f_k4, kouji_min, kouji_age_year, dist],\n",
    "        axis=1\n",
    "    )\n",
    "    return out\n",
    "\n",
    "train_keikai = add_keikai_features(\n",
    "    train_m, keikai_m,\n",
    "    target_ym_series=train_df_geo.loc[train_m.index, 'target_ym'],\n",
    "    max_dist_m=10000.0\n",
    ")\n",
    "\n",
    "test_keikai = add_keikai_features(\n",
    "    test_m, keikai_m,\n",
    "    target_ym_series=test_df_geo.loc[test_m.index, 'target_ym'],\n",
    "    max_dist_m=10000.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "84abf88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = mesh_m.copy()\n",
    "\n",
    "# 災害種別（A30a5_004）を保持\n",
    "# 数値項目をfloat化（存在する列だけ）\n",
    "for c in ['A30a5_005', 'A30a5_006']:\n",
    "    if c in mesh.columns:\n",
    "        mesh[c] = _parse_mm_per(mesh[c])\n",
    "\n",
    "if 'A30a5_008' in mesh.columns:\n",
    "    mesh['A30a5_008'] = _parse_m3(mesh['A30a5_008'])\n",
    "if 'A30a5_009' in mesh.columns:\n",
    "    mesh['A30a5_009'] = _parse_m(mesh['A30a5_009'])\n",
    "\n",
    "def add_mesh_features(prop_m: gpd.GeoDataFrame, mesh_m: gpd.GeoDataFrame, prefix='dosham') -> pd.DataFrame:\n",
    "    cols = [c for c in ['A30a5_004','A30a5_005','A30a5_006','A30a5_007','A30a5_008','A30a5_009','A30a5_010'] if c in mesh_m.columns]\n",
    "    j = prop_m[['geometry']].sjoin(mesh_m[cols + ['geometry']], how='left', predicate='intersects')\n",
    "\n",
    "    # 物件ごとに集約\n",
    "    out = pd.DataFrame(index=prop_m.index)\n",
    "\n",
    "    # inフラグ（メッシュが存在）\n",
    "    out[f'{prefix}_in'] = j['index_right'].notna().groupby(level=0).max().fillna(False).astype('int8')\n",
    "\n",
    "    # 災害種別（A30a5_004）が列挙型なら one-hot 的に持つ（代表的な値を見て調整）\n",
    "    if 'A30a5_004' in j.columns:\n",
    "        # 例：'がけ崩れ','土石流','地すべり','雪崩(表層)' などが入り得る\n",
    "        phen = j['A30a5_004'].astype('string')\n",
    "        # よく出るカテゴリは実データで確認して追加してください\n",
    "        for key, name in [\n",
    "            ('がけ崩れ', f'{prefix}_phen_gake'),\n",
    "            ('土石流', f'{prefix}_phen_doseki'),\n",
    "            ('地すべり', f'{prefix}_phen_jisuberi'),\n",
    "            ('雪崩', f'{prefix}_phen_nadare'),\n",
    "        ]:\n",
    "            out[name] = phen.str.contains(key, na=False).groupby(level=0).max().astype('int8')\n",
    "\n",
    "    # 数値系：max（保守的に最大リスク寄り）\n",
    "    for c in ['A30a5_005','A30a5_006','A30a5_007','A30a5_008','A30a5_009','A30a5_010']:\n",
    "        if c in j.columns:\n",
    "            out[f'{prefix}_{c}_max'] = pd.to_numeric(j[c], errors='coerce').groupby(level=0).max()\n",
    "\n",
    "    return out.reindex(prop_m.index)\n",
    "\n",
    "train_mesh = add_mesh_features(train_m, mesh, prefix='dosham')\n",
    "test_mesh  = add_mesh_features(test_m,  mesh, prefix='dosham')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b39b8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_add = pd.concat([train_kyuusha, train_keikai, train_mesh], axis=1)\n",
    "test_add  = pd.concat([test_kyuusha,  test_keikai,  test_mesh], axis=1)\n",
    "\n",
    "train_df_geo = train_df_geo.join(train_add)\n",
    "test_df_geo  = test_df_geo.join(test_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a48ee38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kyuusha_in hit rate train: 0.003514470054187138\n",
      "kyuusha_in hit rate test : 0.0026681608367352385\n",
      "dosham_in hit rate train: 0.197513766610611\n",
      "dosham_in hit rate test : 0.19574517285235288\n",
      "dosham_in hit rate train: 0.197513766610611\n",
      "dosham_in hit rate test : 0.19574517285235288\n"
     ]
    }
   ],
   "source": [
    "print('kyuusha_in hit rate train:', train_df_geo['kyuusha_in'].mean())\n",
    "print('kyuusha_in hit rate test :', test_df_geo['kyuusha_in'].mean())\n",
    "\n",
    "print('dosham_in hit rate train:', train_df_geo['dosham_in'].mean())\n",
    "print('dosham_in hit rate test :', test_df_geo['dosham_in'].mean())\n",
    "\n",
    "print('dosham_in hit rate train:', train_df_geo['dosham_in'].mean())\n",
    "print('dosham_in hit rate test :', test_df_geo['dosham_in'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b5f3a",
   "metadata": {},
   "source": [
    "## 地盤沈下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bdaf8103",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(gis_path + '地盤沈下地域/C35-60L-48-01.0a_GML/C35-60L-2K_SubsidenceDistrict.shp')\n",
    "jiban_gdf = read_dataframe(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "36b5aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_jiban_lines(\n",
    "    jiban_gdf: gpd.GeoDataFrame,\n",
    "    src_crs: str = 'EPSG:4612',\n",
    "    work_crs_m: str = 'EPSG:6677',\n",
    ") -> gpd.GeoDataFrame:\n",
    "    g = jiban_gdf\n",
    "    if g.crs is None:\n",
    "        g = g.set_crs(src_crs)\n",
    "    else:\n",
    "        g = g.to_crs(src_crs)\n",
    "\n",
    "    # 必要列だけに絞る（メモリ・速度に効く）\n",
    "    cols = [c for c in ['C35_002', 'C35_003'] if c in g.columns]\n",
    "    g = g[cols + ['geometry']].copy()\n",
    "\n",
    "    # 距離計算用CRSへ（ここを1回で済ませるのが重要）\n",
    "    g = g.to_crs(work_crs_m)\n",
    "\n",
    "    # sindex を事前に作っておく（環境によっては効く）\n",
    "    _ = g.sindex\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1b69dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_target_year(\n",
    "    df: pd.DataFrame,\n",
    "    target_ym_col: str = 'target_ym',\n",
    "    target_year_col: str = 'target_year',\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    target_year があればそれを使用。\n",
    "    なければ target_ym (YYYYMM想定) から年を抽出。\n",
    "    どちらも無ければ全NaNを返す（リーク対策ができないため）。\n",
    "    \"\"\"\n",
    "    if target_year_col in df.columns:\n",
    "        y = pd.to_numeric(df[target_year_col], errors='coerce')\n",
    "        return y.astype('float32')\n",
    "\n",
    "    if target_ym_col in df.columns:\n",
    "        v = df[target_ym_col]\n",
    "        if np.issubdtype(v.dtype, np.number):\n",
    "            y = (pd.to_numeric(v, errors='coerce') // 100).astype('float32')\n",
    "            return y\n",
    "\n",
    "        s = v.astype('string')\n",
    "        y = pd.to_numeric(s.str.slice(0, 4), errors='coerce').astype('float32')\n",
    "        return y\n",
    "\n",
    "    return pd.Series(np.nan, index=df.index, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_jiban_chinka_features_fast(\n",
    "#     prop_df: pd.DataFrame,\n",
    "#     jiban_m: gpd.GeoDataFrame,              # prepare済み（EPSG:6677）\n",
    "#     lon_col: str = 'lon_jgd',\n",
    "#     lat_col: str = 'lat_jgd',\n",
    "#     target_ym_col: str = 'target_ym',\n",
    "#     target_year_col: str = 'target_year',\n",
    "#     prop_src_crs: str = 'EPSG:4326',\n",
    "#     work_crs_m: str = 'EPSG:6677',\n",
    "#     max_dist_m: float = 10000.0,\n",
    "#     prefix: str = 'jiban',\n",
    "# ) -> pd.DataFrame:\n",
    "#     # 物件点 → m系\n",
    "#     prop_g = gpd.GeoDataFrame(\n",
    "#         prop_df[[lon_col, lat_col]].copy(),\n",
    "#         geometry=gpd.points_from_xy(prop_df[lon_col], prop_df[lat_col]),\n",
    "#         crs=prop_src_crs,\n",
    "#         index=prop_df.index,\n",
    "#     ).to_crs(work_crs_m)\n",
    "\n",
    "#     # nearest（属性も取る）\n",
    "#     need_cols = [c for c in ['C35_002', 'C35_003'] if c in jiban_m.columns]\n",
    "#     near = prop_g[['geometry']].sjoin_nearest(\n",
    "#         jiban_m[need_cols + ['geometry']],\n",
    "#         how='left',\n",
    "#         max_distance=max_dist_m,\n",
    "#         distance_col=f'{prefix}_dist_m',\n",
    "#     )\n",
    "\n",
    "#     out = pd.DataFrame(index=prop_df.index)\n",
    "\n",
    "#     # dist：minで一意化\n",
    "#     dist = (\n",
    "#         pd.to_numeric(near[f'{prefix}_dist_m'], errors='coerce')\n",
    "#         .groupby(level=0).min()\n",
    "#         .reindex(out.index)\n",
    "#         .astype('float32')\n",
    "#     )\n",
    "#     out[f'{prefix}_dist_m'] = dist\n",
    "\n",
    "#     # clip + log1p（標準装備）\n",
    "#     out[f'{prefix}_dist_m_clip'] = out[f'{prefix}_dist_m'].clip(0.0, float(max_dist_m)).astype('float32')\n",
    "#     out[f'{prefix}_dist_log1p'] = np.log1p(out[f'{prefix}_dist_m_clip']).astype('float32')\n",
    "\n",
    "#     # 閾値フラグ（NaNはFalseにする）\n",
    "#     out[f'{prefix}_near_100m'] = (out[f'{prefix}_dist_m'] <= 100.0).fillna(False).astype('int8')\n",
    "#     out[f'{prefix}_near_300m'] = (out[f'{prefix}_dist_m'] <= 300.0).fillna(False).astype('int8')\n",
    "#     out[f'{prefix}_near_1000m'] = (out[f'{prefix}_dist_m'] <= 1000.0).fillna(False).astype('int8')\n",
    "\n",
    "#     # ========= nearest属性（NaN idx を除外して安全に拾う） =========\n",
    "#     d = pd.to_numeric(near[f'{prefix}_dist_m'], errors='coerce')\n",
    "#     idx_min = d.groupby(level=0).idxmin()          # 見つからない物件は NaN\n",
    "#     idx_min_valid = idx_min.dropna().astype(int)   # ここがポイント（nan排除）\n",
    "\n",
    "#     # nearest を “全indexぶん欠損で初期化”\n",
    "#     year_near = pd.Series(np.nan, index=out.index, dtype='float32')\n",
    "#     type_near = pd.Series(pd.NA, index=out.index, dtype='string')\n",
    "\n",
    "#     if len(idx_min_valid) > 0:\n",
    "#         nearest = near.loc[idx_min_valid.values, :].copy()\n",
    "#         nearest.index = idx_min_valid.index  # index=prop_index\n",
    "\n",
    "#         if 'C35_002' in nearest.columns:\n",
    "#             year_near.loc[nearest.index] = pd.to_numeric(nearest['C35_002'], errors='coerce').astype('float32')\n",
    "\n",
    "#         if 'C35_003' in nearest.columns:\n",
    "#             type_near.loc[nearest.index] = nearest['C35_003'].astype('string')\n",
    "\n",
    "#     # リーク対策\n",
    "#     target_year = _ensure_target_year(prop_df, target_ym_col=target_ym_col, target_year_col=target_year_col)\n",
    "#     is_future = year_near.notna() & target_year.notna() & (year_near > target_year)\n",
    "\n",
    "#     year_near = year_near.mask(is_future, np.nan).astype('float32')\n",
    "#     out[f'{prefix}_year_nearest'] = year_near\n",
    "#     out[f'{prefix}_year_age'] = (target_year - out[f'{prefix}_year_nearest']).astype('float32')\n",
    "\n",
    "#     # タイプone-hot（nearestのみ・未来は無効）\n",
    "#     for code in ['02', '04', '70', '80', '90', '99']:\n",
    "#         col = f'{prefix}_type_{code}_nearest'\n",
    "#         flag = (type_near == code)\n",
    "#         flag = flag.where(~is_future, False)  # 未来は無効\n",
    "#         out[col] = flag.fillna(False).astype('int8')\n",
    "\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d3085",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 41968885 elements, new values have 233031 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m jiban_m = prepare_jiban_lines(jiban_gdf, src_crs=\u001b[33m'\u001b[39m\u001b[33mEPSG:4612\u001b[39m\u001b[33m'\u001b[39m, work_crs_m=\u001b[33m'\u001b[39m\u001b[33mEPSG:6677\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_jiban = \u001b[43madd_jiban_chinka_features_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df_geo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjiban_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop_src_crs\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEPSG:4326\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjiban\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m test_jiban  = add_jiban_chinka_features_fast(test_df_geo,  jiban_m, prop_src_crs=\u001b[33m'\u001b[39m\u001b[33mEPSG:4326\u001b[39m\u001b[33m'\u001b[39m, prefix=\u001b[33m'\u001b[39m\u001b[33mjiban\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m train_df_geo = pd.concat([train_df_geo, train_jiban], axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36madd_jiban_chinka_features_fast\u001b[39m\u001b[34m(prop_df, jiban_m, lon_col, lat_col, target_ym_col, target_year_col, prop_src_crs, work_crs_m, max_dist_m, prefix)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(idx_min_valid) > \u001b[32m0\u001b[39m:\n\u001b[32m     60\u001b[39m     nearest = near.loc[idx_min_valid.values, :].copy()\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[43mnearest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m = idx_min_valid.index  \u001b[38;5;66;03m# index=prop_index\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mC35_002\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m nearest.columns:\n\u001b[32m     64\u001b[39m         year_near.loc[nearest.index] = pd.to_numeric(nearest[\u001b[33m'\u001b[39m\u001b[33mC35_002\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/geopandas/geodataframe.py:206\u001b[39m, in \u001b[36mGeoDataFrame.__setattr__\u001b[39m\u001b[34m(self, attr, val)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, val)\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/generic.py:6313\u001b[39m, in \u001b[36mNDFrame.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6311\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   6312\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[32m-> \u001b[39m\u001b[32m6313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6314\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m   6315\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mproperties.pyx:69\u001b[39m, in \u001b[36mpandas._libs.properties.AxisProperty.__set__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/generic.py:814\u001b[39m, in \u001b[36mNDFrame._set_axis\u001b[39m\u001b[34m(self, axis, labels)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    810\u001b[39m \u001b[33;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[33;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    813\u001b[39m labels = ensure_index(labels)\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[38;5;28mself\u001b[39m._clear_item_cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/managers.py:238\u001b[39m, in \u001b[36mBaseBlockManager.set_axis\u001b[39m\u001b[34m(self, axis, new_labels)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28mself\u001b[39m.axes[axis] = new_labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/base.py:98\u001b[39m, in \u001b[36mDataManager._validate_set_axis\u001b[39m\u001b[34m(self, axis, new_labels)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m new_len != old_len:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     99\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elements, new \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elements\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Length mismatch: Expected axis has 41968885 elements, new values have 233031 elements"
     ]
    }
   ],
   "source": [
    "# jiban_m = prepare_jiban_lines(jiban_gdf, src_crs='EPSG:4612', work_crs_m='EPSG:6677')\n",
    "\n",
    "# train_jiban = add_jiban_chinka_features_fast(train_df_geo, jiban_m, prop_src_crs='EPSG:4326', prefix='jiban')\n",
    "# test_jiban  = add_jiban_chinka_features_fast(test_df_geo,  jiban_m, prop_src_crs='EPSG:4326', prefix='jiban')\n",
    "\n",
    "# train_df_geo = pd.concat([train_df_geo, train_jiban], axis=1)\n",
    "# test_df_geo  = pd.concat([test_df_geo,  test_jiban], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bf334",
   "metadata": {},
   "source": [
    "## 沿岸浸水"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8930272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsunami_path = gis_path + '津波浸水/tsunami.parquet'\n",
    "takashio_path = gis_path + '高潮浸水/takashio.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9104958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsunami_df = gpd.read_parquet(tsunami_path)\n",
    "takashio_df = gpd.read_parquet(takashio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49aaeec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_depth_upper_m_generic(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    浸水深の区分文字列から「上限(m)」を抽出する（A40_003 / A49_003想定）。\n",
    "    例:\n",
    "      '0.3m未満' -> 0.3\n",
    "      '0.01m以上〜0.3m未満' -> 0.3\n",
    "      '10.0m以上〜20.0m未満' -> 20.0\n",
    "      '20m以上' -> 30.0（上限なしは便宜上30m）\n",
    "    \"\"\"\n",
    "    ss = s.astype('string').fillna('')\n",
    "\n",
    "    # 波ダッシュ等を正規化\n",
    "    ss = ss.str.replace('～', '〜', regex=False)\n",
    "\n",
    "    # 'Xm未満' -> X\n",
    "    m1 = ss.str.extract(r'^\\s*([0-9.]+)\\s*m\\s*未満\\s*$', expand=False)\n",
    "\n",
    "    # 'A以上〜B未満' / 'A以上～B未満' -> B\n",
    "    m2 = ss.str.extract(\n",
    "        r'^\\s*[0-9.]+\\s*m\\s*以上\\s*[〜\\-]\\s*([0-9.]+)\\s*m\\s*未満\\s*$',\n",
    "        expand=False\n",
    "    )\n",
    "\n",
    "    # 'A以上B未満'（区切り記号なしも一応） -> B\n",
    "    m2b = ss.str.extract(\n",
    "        r'^\\s*[0-9.]+\\s*m\\s*以上\\s*([0-9.]+)\\s*m\\s*未満\\s*$',\n",
    "        expand=False\n",
    "    )\n",
    "\n",
    "    # 'A以上' -> 便宜上 30m\n",
    "    m3 = ss.str.contains(r'以上\\s*$', na=False)\n",
    "\n",
    "    out = pd.to_numeric(m1, errors='coerce')\n",
    "    out = out.fillna(pd.to_numeric(m2, errors='coerce'))\n",
    "    out = out.fillna(pd.to_numeric(m2b, errors='coerce'))\n",
    "    out = out.astype('float32')\n",
    "\n",
    "    out = out.where(~m3, 30.0).astype('float32')\n",
    "    out = out.where(out > 0, np.nan).astype('float32')\n",
    "    return out\n",
    "\n",
    "def _depth_rank_from_upper(upper: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    上限(m)を単調なランクへ（津波/高潮とも同じ思想でOK）\n",
    "    \"\"\"\n",
    "    bins = [0, 0.01, 0.3, 0.5, 1, 3, 5, 10, 20, 30, np.inf]\n",
    "    r = pd.cut(upper, bins=bins, labels=False, include_lowest=True)\n",
    "    return r.astype('float32')\n",
    "\n",
    "def add_inundation_polygon_features(\n",
    "    prop_df: pd.DataFrame,\n",
    "    poly_gdf: gpd.GeoDataFrame,\n",
    "    depth_col: str,\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    prefix: str = 'inund',\n",
    "    prop_input_crs: str = 'EPSG:4326',\n",
    "    poly_input_crs: str | None = None,\n",
    "    work_crs: str = 'EPSG:6677',   # 距離用（メートル）\n",
    "    max_dist_m: float = 10000.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    浸水ポリゴン（津波/高潮）を物件Pointに突合し、深さカテゴリ由来の特徴量 + 最近傍距離を返す。\n",
    "    \"\"\"\n",
    "\n",
    "    # 物件点\n",
    "    prop_g = gpd.GeoDataFrame(\n",
    "        prop_df[[lon_col, lat_col]].copy(),\n",
    "        geometry=gpd.points_from_xy(prop_df[lon_col], prop_df[lat_col]),\n",
    "        crs=prop_input_crs\n",
    "    )\n",
    "\n",
    "    # ポリゴン側（CRSが付いていない/怪しい場合の保険）\n",
    "    a = poly_gdf[[depth_col, 'geometry']].copy()\n",
    "    if poly_input_crs is not None:\n",
    "        a = a.set_crs(poly_input_crs, allow_override=True)\n",
    "\n",
    "    # 距離計算・sjoin安定化のためメートル系へ\n",
    "    prop_m = prop_g.to_crs(work_crs)\n",
    "    a_m = a.to_crs(work_crs)\n",
    "\n",
    "    a_m['depth_upper_m'] = _parse_depth_upper_m_generic(a_m[depth_col])\n",
    "    a_m['depth_rank'] = _depth_rank_from_upper(a_m['depth_upper_m'])\n",
    "\n",
    "    # intersects（複数ヒット前提 -> groupby(level=0) で 1物件1行へ）\n",
    "    j = prop_m[['geometry']].sjoin(\n",
    "        a_m[['depth_upper_m', 'depth_rank', 'geometry']],\n",
    "        how='left',\n",
    "        predicate='intersects'\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame(index=prop_df.index)\n",
    "\n",
    "    out[f'{prefix}_in'] = (\n",
    "        j['index_right'].notna()\n",
    "        .groupby(level=0).max()\n",
    "        .reindex(prop_df.index, fill_value=False)\n",
    "        .astype('int8')\n",
    "    )\n",
    "\n",
    "    out[f'{prefix}_depth_max_m'] = (\n",
    "        pd.to_numeric(j['depth_upper_m'], errors='coerce')\n",
    "        .groupby(level=0).max()\n",
    "        .reindex(prop_df.index)\n",
    "        .astype('float32')\n",
    "    )\n",
    "\n",
    "    out[f'{prefix}_depth_rank_max'] = (\n",
    "        pd.to_numeric(j['depth_rank'], errors='coerce')\n",
    "        .groupby(level=0).max()\n",
    "        .reindex(prop_df.index)\n",
    "        .astype('float32')\n",
    "    )\n",
    "\n",
    "    # 最近傍距離（複数行->minで1行化）\n",
    "    near = prop_m[['geometry']].sjoin_nearest(\n",
    "        a_m[['geometry']],\n",
    "        how='left',\n",
    "        max_distance=max_dist_m,\n",
    "        distance_col=f'{prefix}_dist_m'\n",
    "    )\n",
    "\n",
    "    out[f'{prefix}_dist_m'] = (\n",
    "        near[f'{prefix}_dist_m']\n",
    "        .groupby(level=0).min()\n",
    "        .reindex(prop_df.index)\n",
    "        .astype('float32')\n",
    "    )\n",
    "\n",
    "    # 追加の安定特徴\n",
    "    out[f'{prefix}_depth_max_log1p'] = np.log1p(out[f'{prefix}_depth_max_m']).astype('float32')\n",
    "    out[f'{prefix}_depth_ge1m'] = (out[f'{prefix}_depth_max_m'] >= 1.0).astype('int8')\n",
    "    out[f'{prefix}_depth_ge3m'] = (out[f'{prefix}_depth_max_m'] >= 3.0).astype('int8')\n",
    "    out[f'{prefix}_depth_ge5m'] = (out[f'{prefix}_depth_max_m'] >= 5.0).astype('int8')\n",
    "    out[f'{prefix}_depth_ge10m'] = (out[f'{prefix}_depth_max_m'] >= 10.0).astype('int8')\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "769bc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 津波（A40_003）\n",
    "train_tsunami = add_inundation_polygon_features(\n",
    "    prop_df=train_df_geo,\n",
    "    poly_gdf=tsunami_df,\n",
    "    depth_col='A40_003',\n",
    "    prefix='tsunami',\n",
    "    prop_input_crs='EPSG:4326',   # lon/lat の実態に合わせて調整（後述）\n",
    "    poly_input_crs='EPSG:6668',   # tsunami_df が EPSG:6668\n",
    "    work_crs='EPSG:6677',\n",
    "    max_dist_m=10000.0\n",
    ")\n",
    "\n",
    "test_tsunami = add_inundation_polygon_features(\n",
    "    prop_df=test_df_geo,\n",
    "    poly_gdf=tsunami_df,\n",
    "    depth_col='A40_003',\n",
    "    prefix='tsunami',\n",
    "    prop_input_crs='EPSG:4326',\n",
    "    poly_input_crs='EPSG:6668',\n",
    "    work_crs='EPSG:6677',\n",
    "    max_dist_m=10000.0\n",
    ")\n",
    "\n",
    "# 高潮（A49_003）\n",
    "train_takashio = add_inundation_polygon_features(\n",
    "    prop_df=train_df_geo,\n",
    "    poly_gdf=takashio_df,\n",
    "    depth_col='A49_003',\n",
    "    prefix='takashio',\n",
    "    prop_input_crs='EPSG:4326',\n",
    "    poly_input_crs='EPSG:6668',\n",
    "    work_crs='EPSG:6677',\n",
    "    max_dist_m=10000.0\n",
    ")\n",
    "\n",
    "test_takashio = add_inundation_polygon_features(\n",
    "    prop_df=test_df_geo,\n",
    "    poly_gdf=takashio_df,\n",
    "    depth_col='A49_003',\n",
    "    prefix='takashio',\n",
    "    prop_input_crs='EPSG:4326',\n",
    "    poly_input_crs='EPSG:6668',\n",
    "    work_crs='EPSG:6677',\n",
    "    max_dist_m=10000.0\n",
    ")\n",
    "\n",
    "# 追加\n",
    "train_df_geo = pd.concat([train_df_geo, train_tsunami, train_takashio], axis=1)\n",
    "test_df_geo  = pd.concat([test_df_geo,  test_tsunami,  test_takashio], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1f1cca81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsunami_in hit rate train: 0.055448390323254305\n",
      "tsunami_in hit rate test : 0.05870843227763103\n",
      "takashio_in hit rate train: 0.15667007397148855\n",
      "takashio_in hit rate test : 0.1619395750509174\n"
     ]
    }
   ],
   "source": [
    "print('tsunami_in hit rate train:', train_df_geo['tsunami_in'].mean())\n",
    "print('tsunami_in hit rate test :', test_df_geo['tsunami_in'].mean())\n",
    "\n",
    "print('takashio_in hit rate train:', train_df_geo['takashio_in'].mean())\n",
    "print('takashio_in hit rate test :', test_df_geo['takashio_in'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8abc0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_inund_features(df: pd.DataFrame, prefix: str, max_dist_m: float) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    dist = f'{prefix}_dist_m'\n",
    "    if dist in out.columns:\n",
    "        out[dist] = out[dist].astype('float32').fillna(np.float32(max_dist_m + 1.0))\n",
    "        out[f'{prefix}_dist_log1p'] = np.log1p(out[dist].clip(upper=10000)).astype('float32')\n",
    "        out[f'{prefix}_near_500m'] = (out[dist] <= 500.0).astype('int8')\n",
    "        out[f'{prefix}_near_1km'] = (out[dist] <= 1000.0).astype('int8')\n",
    "        out[f'{prefix}_near_3km'] = (out[dist] <= 3000.0).astype('int8')\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a6855a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_geo = postprocess_inund_features(train_df_geo, 'tsunami', max_dist_m=10000.0)\n",
    "train_df_geo = postprocess_inund_features(train_df_geo, 'takashio', max_dist_m=10000.0)\n",
    "test_df_geo = postprocess_inund_features(test_df_geo, 'tsunami', max_dist_m=10000.0)\n",
    "test_df_geo = postprocess_inund_features(test_df_geo, 'takashio', max_dist_m=10000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9f843",
   "metadata": {},
   "source": [
    "## 避難施設"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3cd9ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hinanjo_path = gis_path + '避難施設/hinanjo.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "da62d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "hinanjo_df = gpd.read_parquet(hinanjo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "40aa730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "def add_hinanjo_features(\n",
    "    prop_df: pd.DataFrame,\n",
    "    hinanjo_gdf: gpd.GeoDataFrame,\n",
    "    lon_col: str = 'lon_jgd',\n",
    "    lat_col: str = 'lat_jgd',\n",
    "    prefix: str = 'hinanjo',\n",
    "    prop_src_crs: str = 'EPSG:4326',\n",
    "    hinanjo_src_crs: str = 'EPSG:4612',\n",
    "    work_crs_m: str = 'EPSG:6677',\n",
    "    max_dist_m: float = 10000.0,\n",
    "    radii_m: tuple[int, ...] = (500, 1000, 2000),\n",
    "    hazard_flags: tuple[str, ...] = ('for_jishin_flg', 'for_tsunami_flg', 'for_suigai_flg', 'for_kazan_flg', 'for_all_flg'),\n",
    "    use_top_shubetsu_k: int = 2,\n",
    "    chunk_size: int = 50000,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    避難所(ポイント)から特徴量を作る（GeoPandas sjoin(distance=) 不要）。\n",
    "    - 最近傍距離 + clip + log1p\n",
    "    - 半径内: count, capacity sum/max, area sum/max (+log1p)\n",
    "    - 災害フラグ別: 最近傍距離 + (r=1000,2000 の count/cap sum)\n",
    "    - shubetsu上位: 最近傍距離(+log1p) + 1000m以内count/cap sum\n",
    "    \"\"\"\n",
    "\n",
    "    # scipy が無い環境もあるので、import を関数内にしてエラーを明確化\n",
    "    try:\n",
    "        from scipy.spatial import cKDTree\n",
    "    except Exception as e:\n",
    "        raise ImportError('scipy が必要です（cKDTree を使います）。pip install scipy をお願いします。') from e\n",
    "\n",
    "    out = pd.DataFrame(index=prop_df.index)\n",
    "\n",
    "    # --- 物件点 -> メートルCRS ---\n",
    "    prop_g = gpd.GeoDataFrame(\n",
    "        prop_df[[lon_col, lat_col]].copy(),\n",
    "        geometry=gpd.points_from_xy(prop_df[lon_col], prop_df[lat_col]),\n",
    "        crs=prop_src_crs,\n",
    "        index=prop_df.index,\n",
    "    ).to_crs(work_crs_m)\n",
    "\n",
    "    # --- 避難所 -> メートルCRS ---\n",
    "    h = hinanjo_gdf.copy()\n",
    "    if h.crs is None:\n",
    "        h = h.set_crs(hinanjo_src_crs)\n",
    "    h = h.to_crs(work_crs_m)\n",
    "\n",
    "    # 数値整形（-1 等は欠損扱い）\n",
    "    if 'capacity' in h.columns:\n",
    "        h['capacity'] = pd.to_numeric(h['capacity'], errors='coerce')\n",
    "        h.loc[h['capacity'] < 0, 'capacity'] = np.nan\n",
    "    else:\n",
    "        h['capacity'] = np.nan\n",
    "\n",
    "    if 'area' in h.columns:\n",
    "        h['area'] = pd.to_numeric(h['area'], errors='coerce')\n",
    "        h.loc[h['area'] < 0, 'area'] = np.nan\n",
    "    else:\n",
    "        h['area'] = np.nan\n",
    "\n",
    "    if 'shubetsu' not in h.columns:\n",
    "        h['shubetsu'] = pd.Series(index=h.index, dtype='string')\n",
    "\n",
    "    # 座標配列\n",
    "    prop_xy = np.column_stack([prop_g.geometry.x.values, prop_g.geometry.y.values]).astype('float64')\n",
    "    h_xy = np.column_stack([h.geometry.x.values, h.geometry.y.values]).astype('float64')\n",
    "\n",
    "    # KDTree 構築（全避難所）\n",
    "    tree_all = cKDTree(h_xy)\n",
    "\n",
    "    # ========== 1) 最近傍距離（全避難所） ==========\n",
    "    d1, _ = tree_all.query(prop_xy, k=1, distance_upper_bound=max_dist_m)\n",
    "    # cKDTree は範囲外を inf にする\n",
    "    d1 = np.where(np.isfinite(d1), d1, np.nan).astype('float32')\n",
    "\n",
    "    out[f'{prefix}_dist_m'] = d1\n",
    "    out[f'{prefix}_dist_m_clip'] = np.clip(d1, 0.0, float(max_dist_m)).astype('float32')\n",
    "    out[f'{prefix}_dist_log1p'] = np.log1p(np.nan_to_num(out[f'{prefix}_dist_m_clip'].values, nan=float(max_dist_m))).astype('float32')\n",
    "\n",
    "    # ========== 2) 半径内集計（全避難所） ==========\n",
    "    cap = h['capacity'].values.astype('float32')\n",
    "    area = h['area'].values.astype('float32')\n",
    "\n",
    "    for r in radii_m:\n",
    "        cnt = np.zeros(len(prop_xy), dtype='int32')\n",
    "        cap_sum = np.full(len(prop_xy), np.nan, dtype='float32')\n",
    "        cap_max = np.full(len(prop_xy), np.nan, dtype='float32')\n",
    "        area_sum = np.full(len(prop_xy), np.nan, dtype='float32')\n",
    "        area_max = np.full(len(prop_xy), np.nan, dtype='float32')\n",
    "\n",
    "        for st in range(0, len(prop_xy), chunk_size):\n",
    "            ed = min(st + chunk_size, len(prop_xy))\n",
    "            neigh = tree_all.query_ball_point(prop_xy[st:ed], r=r)\n",
    "\n",
    "            for i, idxs in enumerate(neigh):\n",
    "                if not idxs:\n",
    "                    continue\n",
    "                cnt[st + i] = len(idxs)\n",
    "\n",
    "                c = cap[idxs]\n",
    "                a = area[idxs]\n",
    "\n",
    "                # sum は min_count=1 相当（全欠損なら NaN）\n",
    "                if np.isfinite(c).any():\n",
    "                    cap_sum[st + i] = np.nansum(c)\n",
    "                    cap_max[st + i] = np.nanmax(c)\n",
    "                if np.isfinite(a).any():\n",
    "                    area_sum[st + i] = np.nansum(a)\n",
    "                    area_max[st + i] = np.nanmax(a)\n",
    "\n",
    "        out[f'{prefix}_cnt_r{r}'] = cnt.astype('int16')\n",
    "        out[f'{prefix}_cap_sum_r{r}'] = cap_sum\n",
    "        out[f'{prefix}_cap_max_r{r}'] = cap_max\n",
    "        out[f'{prefix}_area_sum_r{r}'] = area_sum\n",
    "        out[f'{prefix}_area_max_r{r}'] = area_max\n",
    "\n",
    "        out[f'{prefix}_cap_sum_log1p_r{r}'] = np.log1p(np.nan_to_num(cap_sum, nan=0.0)).astype('float32')\n",
    "        out[f'{prefix}_area_sum_log1p_r{r}'] = np.log1p(np.nan_to_num(area_sum, nan=0.0)).astype('float32')\n",
    "\n",
    "    # ========== 3) 災害フラグ別 ==========\n",
    "    for hf in hazard_flags:\n",
    "        if hf not in h.columns:\n",
    "            continue\n",
    "\n",
    "        mask = h[hf].fillna(0).astype(int).values == 1\n",
    "        if mask.sum() == 0:\n",
    "            out[f'{prefix}_dist_{hf}_m'] = np.float32(np.nan)\n",
    "            out[f'{prefix}_dist_{hf}_clip'] = np.float32(np.nan)\n",
    "            out[f'{prefix}_dist_{hf}_log1p'] = np.float32(np.nan)\n",
    "            for r in (1000, 2000):\n",
    "                if r in radii_m:\n",
    "                    out[f'{prefix}_cnt_{hf}_r{r}'] = np.int16(0)\n",
    "                    out[f'{prefix}_cap_sum_{hf}_r{r}'] = np.float32(np.nan)\n",
    "            continue\n",
    "\n",
    "        h_xy_h = h_xy[mask]\n",
    "        tree_h = cKDTree(h_xy_h)\n",
    "        cap_h = cap[mask]\n",
    "\n",
    "        d_h, _ = tree_h.query(prop_xy, k=1, distance_upper_bound=max_dist_m)\n",
    "        d_h = np.where(np.isfinite(d_h), d_h, np.nan).astype('float32')\n",
    "\n",
    "        out[f'{prefix}_dist_{hf}_m'] = d_h\n",
    "        out[f'{prefix}_dist_{hf}_clip'] = np.clip(d_h, 0.0, float(max_dist_m)).astype('float32')\n",
    "        out[f'{prefix}_dist_{hf}_log1p'] = np.log1p(np.nan_to_num(out[f'{prefix}_dist_{hf}_clip'].values, nan=float(max_dist_m))).astype('float32')\n",
    "\n",
    "        for r in (1000, 2000):\n",
    "            if r not in radii_m:\n",
    "                continue\n",
    "\n",
    "            cnt = np.zeros(len(prop_xy), dtype='int32')\n",
    "            cap_sum = np.full(len(prop_xy), np.nan, dtype='float32')\n",
    "\n",
    "            for st in range(0, len(prop_xy), chunk_size):\n",
    "                ed = min(st + chunk_size, len(prop_xy))\n",
    "                neigh = tree_h.query_ball_point(prop_xy[st:ed], r=r)\n",
    "                for i, idxs in enumerate(neigh):\n",
    "                    if not idxs:\n",
    "                        continue\n",
    "                    cnt[st + i] = len(idxs)\n",
    "                    c = cap_h[idxs]\n",
    "                    if np.isfinite(c).any():\n",
    "                        cap_sum[st + i] = np.nansum(c)\n",
    "\n",
    "            out[f'{prefix}_cnt_{hf}_r{r}'] = cnt.astype('int16')\n",
    "            out[f'{prefix}_cap_sum_{hf}_r{r}'] = cap_sum\n",
    "\n",
    "    # ========== 4) 種別(shubetsu)上位 ==========\n",
    "    if use_top_shubetsu_k and use_top_shubetsu_k > 0:\n",
    "        vc = h['shubetsu'].astype('string').value_counts(dropna=True)\n",
    "        top_vals = vc.head(use_top_shubetsu_k).index.tolist()\n",
    "\n",
    "        for v in top_vals:\n",
    "            m = h['shubetsu'].astype('string') == v\n",
    "            if m.sum() == 0:\n",
    "                continue\n",
    "            hv_xy = h_xy[m.values]\n",
    "            tree_v = cKDTree(hv_xy)\n",
    "            cap_v = cap[m.values]\n",
    "\n",
    "            name = str(v).replace(' ', '_')\n",
    "            d_v, _ = tree_v.query(prop_xy, k=1, distance_upper_bound=max_dist_m)\n",
    "            d_v = np.where(np.isfinite(d_v), d_v, np.nan).astype('float32')\n",
    "\n",
    "            out[f'{prefix}_dist_shubetsu_{name}_m'] = d_v\n",
    "            out[f'{prefix}_dist_shubetsu_{name}_log1p'] = np.log1p(np.clip(np.nan_to_num(d_v, nan=float(max_dist_m)), 0.0, float(max_dist_m))).astype('float32')\n",
    "\n",
    "            # 軽量に 1000m だけ\n",
    "            r = 1000\n",
    "            if r in radii_m:\n",
    "                cnt = np.zeros(len(prop_xy), dtype='int32')\n",
    "                cap_sum = np.full(len(prop_xy), np.nan, dtype='float32')\n",
    "                for st in range(0, len(prop_xy), chunk_size):\n",
    "                    ed = min(st + chunk_size, len(prop_xy))\n",
    "                    neigh = tree_v.query_ball_point(prop_xy[st:ed], r=r)\n",
    "                    for i, idxs in enumerate(neigh):\n",
    "                        if not idxs:\n",
    "                            continue\n",
    "                        cnt[st + i] = len(idxs)\n",
    "                        c = cap_v[idxs]\n",
    "                        if np.isfinite(c).any():\n",
    "                            cap_sum[st + i] = np.nansum(c)\n",
    "\n",
    "                out[f'{prefix}_cnt_shubetsu_{name}_r{r}'] = cnt.astype('int16')\n",
    "                out[f'{prefix}_cap_sum_shubetsu_{name}_r{r}'] = cap_sum\n",
    "\n",
    "    # 型揃え\n",
    "    for c in out.columns:\n",
    "        if c.endswith('_cnt_r500') or c.endswith('_cnt_r1000') or c.endswith('_cnt_r2000') or '_cnt_' in c:\n",
    "            out[c] = out[c].astype('int16', errors='ignore')\n",
    "    for c in out.columns:\n",
    "        if out[c].dtype == 'float64':\n",
    "            out[c] = out[c].astype('float32')\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "097f3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hinanjo = add_hinanjo_features(train_df_geo, hinanjo_df, prop_src_crs='EPSG:4326', prefix='hinanjo')\n",
    "test_hinanjo  = add_hinanjo_features(test_df_geo,  hinanjo_df, prop_src_crs='EPSG:4326', prefix='hinanjo')\n",
    "\n",
    "train_df_geo = pd.concat([train_df_geo, train_hinanjo], axis=1)\n",
    "test_df_geo  = pd.concat([test_df_geo,  test_hinanjo], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88ee11",
   "metadata": {},
   "source": [
    "## 特徴量の追加・削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c5acac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target_ym',\n",
       " 'building_id',\n",
       " 'unit_id',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'target_year',\n",
       " 'lon_jgd',\n",
       " 'lat_jgd',\n",
       " 'nearest_land_price',\n",
       " 'distance_to_landpoint_m',\n",
       " 'log_land_price',\n",
       " 'weighted_land_price_3',\n",
       " 'log_weighted_land_price_3',\n",
       " 'nearest_land_price_prev',\n",
       " 'weighted_land_price_3_prev',\n",
       " 'land_price_yoy_nearest',\n",
       " 'land_price_yoy_w3',\n",
       " 'land_price_dlog_nearest',\n",
       " 'land_price_dlog_w3',\n",
       " 'pop_dist_nn_m',\n",
       " 'PTN_2020_nn',\n",
       " 'RTA_2025_nn',\n",
       " 'RTB_2025_nn',\n",
       " 'RTC_2025_nn',\n",
       " 'RTD_2025_nn',\n",
       " 'RTE_2025_nn',\n",
       " 'pop_trend_rate_nn',\n",
       " 'road_len_total',\n",
       " 'road_len_wide',\n",
       " 'road_len_narrow',\n",
       " 'road_wide_ratio',\n",
       " 'road_narrow_ratio',\n",
       " 'road_len_total_gap',\n",
       " 'road_narrow_ratio_gap',\n",
       " 'road_len_density',\n",
       " 'road_len_density_gap',\n",
       " 'dist_to_road_any_m',\n",
       " 'dist_to_road_major_m',\n",
       " 'dist_to_road_highway_m',\n",
       " 'road_cnt_any_in_100m',\n",
       " 'road_cnt_major_in_100m',\n",
       " 'road_cnt_any_in_300m',\n",
       " 'road_cnt_major_in_300m',\n",
       " 'road_cnt_any_in_500m',\n",
       " 'road_cnt_major_in_500m',\n",
       " 'A29_005',\n",
       " 'zone_group',\n",
       " 'zone_residential_rank',\n",
       " 'is_lowrise_residential',\n",
       " 'kenpei',\n",
       " 'youseki',\n",
       " 'kenpei_missing',\n",
       " 'youseki_missing',\n",
       " 'kenpei_bin',\n",
       " 'youseki_bin',\n",
       " 'density_cat',\n",
       " 'is_urbanized_area',\n",
       " 'is_urban_control_area',\n",
       " 'is_fireproof_area',\n",
       " 'is_quasi_fireproof_area',\n",
       " 'has_height_limit',\n",
       " 'has_district_plan',\n",
       " 'is_land_readjustment_area',\n",
       " 'is_high_utilization_area',\n",
       " 'is_urban_renaissance_area',\n",
       " 'is_special_far_area',\n",
       " 'is_highrise_residential_area',\n",
       " 'is_disaster_prevention_block',\n",
       " 'is_redevelopment_core_area',\n",
       " 'index_right',\n",
       " 'G04c_001',\n",
       " 'G04c_002',\n",
       " 'G04c_003',\n",
       " 'G04c_004',\n",
       " 'G04c_005',\n",
       " 'G04c_006',\n",
       " 'G04c_007',\n",
       " 'G04c_008',\n",
       " 'G04c_009',\n",
       " 'G04c_010',\n",
       " 'elev_mean',\n",
       " 'elev_range',\n",
       " 'slope_mean',\n",
       " 'slope_max',\n",
       " 'slope_range',\n",
       " 'station_power_sum3',\n",
       " 'station_power_max3',\n",
       " 'disaster_hit',\n",
       " 'disaster_n_types',\n",
       " 'disaster_score_sum',\n",
       " 'disaster_score_max',\n",
       " 'disaster_score',\n",
       " 'dist_elem_school_m',\n",
       " 'dist_junior_school_m',\n",
       " 'elem_school_500m',\n",
       " 'junior_school_1km',\n",
       " 'dist_elem_school_log',\n",
       " 'dist_junior_school_log',\n",
       " 'dist_hospital_m',\n",
       " 'dist_clinic_m',\n",
       " 'hospital_1km',\n",
       " 'dist_hospital_log',\n",
       " 'clinic_500m',\n",
       " 'dist_clinic_log',\n",
       " 'hospital_beds_nearest',\n",
       " 'hospital_is_emergency_nearest',\n",
       " 'hospital_is_disaster_nearest',\n",
       " 'ame_in',\n",
       " 'ame_depth_max_m',\n",
       " 'ame_depth_rank_max',\n",
       " 'ame_dist_m',\n",
       " 'ame_depth_max_log1p',\n",
       " 'ame_depth_ge1m',\n",
       " 'ame_depth_ge3m',\n",
       " 'ame_dist_m_clip',\n",
       " 'ame_dist_log',\n",
       " 'ame_depth_effect',\n",
       " 'kouzui_keikaku_in',\n",
       " 'kouzui_keikaku_rank_max',\n",
       " 'kouzui_keikaku_dist_m',\n",
       " 'kouzui_saidai_in',\n",
       " 'kouzui_saidai_rank_max',\n",
       " 'kouzui_saidai_dist_m',\n",
       " 'kouzui_keikaku_dist_m_log1p',\n",
       " 'kouzui_saidai_dist_m_log1p',\n",
       " 'kouzui_keikaku_rank_ge3',\n",
       " 'kouzui_saidai_rank_ge3',\n",
       " 'kyuusha_in',\n",
       " 'kyuusha_dist_m',\n",
       " 'keikai_genshou1_in',\n",
       " 'keikai_genshou2_in',\n",
       " 'keikai_genshou3_in',\n",
       " 'keikai_kuiki1_in',\n",
       " 'keikai_kuiki2_in',\n",
       " 'keikai_kuiki3_in',\n",
       " 'keikai_kuiki4_in',\n",
       " 'keikai_kouji_date_min',\n",
       " 'keikai_kouji_age_year',\n",
       " 'keikai_dist_m',\n",
       " 'dosham_in',\n",
       " 'dosham_phen_gake',\n",
       " 'dosham_phen_doseki',\n",
       " 'dosham_phen_jisuberi',\n",
       " 'dosham_phen_nadare',\n",
       " 'dosham_A30a5_005_max',\n",
       " 'dosham_A30a5_006_max',\n",
       " 'dosham_A30a5_007_max',\n",
       " 'dosham_A30a5_008_max',\n",
       " 'dosham_A30a5_009_max',\n",
       " 'dosham_A30a5_010_max',\n",
       " 'tsunami_in',\n",
       " 'tsunami_depth_max_m',\n",
       " 'tsunami_depth_rank_max',\n",
       " 'tsunami_dist_m',\n",
       " 'tsunami_depth_max_log1p',\n",
       " 'tsunami_depth_ge1m',\n",
       " 'tsunami_depth_ge3m',\n",
       " 'tsunami_depth_ge5m',\n",
       " 'tsunami_depth_ge10m',\n",
       " 'takashio_in',\n",
       " 'takashio_depth_max_m',\n",
       " 'takashio_depth_rank_max',\n",
       " 'takashio_dist_m',\n",
       " 'takashio_depth_max_log1p',\n",
       " 'takashio_depth_ge1m',\n",
       " 'takashio_depth_ge3m',\n",
       " 'takashio_depth_ge5m',\n",
       " 'takashio_depth_ge10m',\n",
       " 'tsunami_dist_log1p',\n",
       " 'tsunami_near_500m',\n",
       " 'tsunami_near_1km',\n",
       " 'tsunami_near_3km',\n",
       " 'takashio_dist_log1p',\n",
       " 'takashio_near_500m',\n",
       " 'takashio_near_1km',\n",
       " 'takashio_near_3km',\n",
       " 'hinanjo_dist_m',\n",
       " 'hinanjo_dist_m_clip',\n",
       " 'hinanjo_dist_log1p',\n",
       " 'hinanjo_cnt_r500',\n",
       " 'hinanjo_cap_sum_r500',\n",
       " 'hinanjo_cap_max_r500',\n",
       " 'hinanjo_area_sum_r500',\n",
       " 'hinanjo_area_max_r500',\n",
       " 'hinanjo_cap_sum_log1p_r500',\n",
       " 'hinanjo_area_sum_log1p_r500',\n",
       " 'hinanjo_cnt_r1000',\n",
       " 'hinanjo_cap_sum_r1000',\n",
       " 'hinanjo_cap_max_r1000',\n",
       " 'hinanjo_area_sum_r1000',\n",
       " 'hinanjo_area_max_r1000',\n",
       " 'hinanjo_cap_sum_log1p_r1000',\n",
       " 'hinanjo_area_sum_log1p_r1000',\n",
       " 'hinanjo_cnt_r2000',\n",
       " 'hinanjo_cap_sum_r2000',\n",
       " 'hinanjo_cap_max_r2000',\n",
       " 'hinanjo_area_sum_r2000',\n",
       " 'hinanjo_area_max_r2000',\n",
       " 'hinanjo_cap_sum_log1p_r2000',\n",
       " 'hinanjo_area_sum_log1p_r2000',\n",
       " 'hinanjo_dist_for_jishin_flg_m',\n",
       " 'hinanjo_dist_for_jishin_flg_clip',\n",
       " 'hinanjo_dist_for_jishin_flg_log1p',\n",
       " 'hinanjo_cnt_for_jishin_flg_r1000',\n",
       " 'hinanjo_cap_sum_for_jishin_flg_r1000',\n",
       " 'hinanjo_cnt_for_jishin_flg_r2000',\n",
       " 'hinanjo_cap_sum_for_jishin_flg_r2000',\n",
       " 'hinanjo_dist_for_tsunami_flg_m',\n",
       " 'hinanjo_dist_for_tsunami_flg_clip',\n",
       " 'hinanjo_dist_for_tsunami_flg_log1p',\n",
       " 'hinanjo_cnt_for_tsunami_flg_r1000',\n",
       " 'hinanjo_cap_sum_for_tsunami_flg_r1000',\n",
       " 'hinanjo_cnt_for_tsunami_flg_r2000',\n",
       " 'hinanjo_cap_sum_for_tsunami_flg_r2000',\n",
       " 'hinanjo_dist_for_suigai_flg_m',\n",
       " 'hinanjo_dist_for_suigai_flg_clip',\n",
       " 'hinanjo_dist_for_suigai_flg_log1p',\n",
       " 'hinanjo_cnt_for_suigai_flg_r1000',\n",
       " 'hinanjo_cap_sum_for_suigai_flg_r1000',\n",
       " 'hinanjo_cnt_for_suigai_flg_r2000',\n",
       " 'hinanjo_cap_sum_for_suigai_flg_r2000',\n",
       " 'hinanjo_dist_for_kazan_flg_m',\n",
       " 'hinanjo_dist_for_kazan_flg_clip',\n",
       " 'hinanjo_dist_for_kazan_flg_log1p',\n",
       " 'hinanjo_cnt_for_kazan_flg_r1000',\n",
       " 'hinanjo_cap_sum_for_kazan_flg_r1000',\n",
       " 'hinanjo_cnt_for_kazan_flg_r2000',\n",
       " 'hinanjo_cap_sum_for_kazan_flg_r2000',\n",
       " 'hinanjo_dist_for_all_flg_m',\n",
       " 'hinanjo_dist_for_all_flg_clip',\n",
       " 'hinanjo_dist_for_all_flg_log1p',\n",
       " 'hinanjo_cnt_for_all_flg_r1000',\n",
       " 'hinanjo_cap_sum_for_all_flg_r1000',\n",
       " 'hinanjo_cnt_for_all_flg_r2000',\n",
       " 'hinanjo_cap_sum_for_all_flg_r2000',\n",
       " 'hinanjo_dist_shubetsu_避難所_m',\n",
       " 'hinanjo_dist_shubetsu_避難所_log1p',\n",
       " 'hinanjo_cnt_shubetsu_避難所_r1000',\n",
       " 'hinanjo_cap_sum_shubetsu_避難所_r1000',\n",
       " 'hinanjo_dist_shubetsu_避難場所_m',\n",
       " 'hinanjo_dist_shubetsu_避難場所_log1p',\n",
       " 'hinanjo_cnt_shubetsu_避難場所_r1000',\n",
       " 'hinanjo_cap_sum_shubetsu_避難場所_r1000']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_geo.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8fa9c2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "for_all_flg  for_jishin_flg  for_tsunami_flg  for_suigai_flg  for_kazan_flg\n",
       "1            0               0                0               0                101967\n",
       "0            1               0                1               0                  6856\n",
       "             0               0                1               0                  5481\n",
       "                             1                0               0                  4041\n",
       "             1               0                0               0                  4036\n",
       "                             1                1               0                  1569\n",
       "             0               1                1               0                   504\n",
       "             1               1                0               0                   441\n",
       "             0               0                0               0                   256\n",
       "             1               1                1               1                   250\n",
       "             0               1                1               1                   249\n",
       "                             0                0               1                   125\n",
       "             1               1                0               1                    67\n",
       "             0               1                0               1                    30\n",
       "             1               0                0               1                    22\n",
       "                                              1               1                    14\n",
       "             0               0                1               1                    11\n",
       "1            1               0                0               0                     6\n",
       "             0               0                1               0                     1\n",
       "             1               0                1               0                     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hinanjo_df[['for_all_flg', 'for_jishin_flg', 'for_tsunami_flg', 'for_suigai_flg', 'for_kazan_flg']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "329e9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cols = [\n",
    "    # 地価\n",
    "    'nearest_land_price', 'weighted_land_price_3', 'distance_to_landpoint_m', 'log_land_price', 'log_weighted_land_price_3',\n",
    "    'land_price_yoy_nearest', 'land_price_yoy_w3', 'land_price_dlog_nearest', 'land_price_dlog_w3',\n",
    "\n",
    "    # 将来人口\n",
    "    'PTN_2020_nn', 'RTA_2025_nn', 'RTB_2025_nn', 'RTC_2025_nn', 'RTD_2025_nn', 'RTE_2025_nn',\n",
    "    'pop_trend_rate_nn',\n",
    "\n",
    "    # 道路\n",
    "    'road_len_total', 'road_len_wide', 'road_len_narrow',\n",
    "    'road_wide_ratio', 'road_narrow_ratio',\n",
    "    'road_len_total_gap', 'road_narrow_ratio_gap',\n",
    "    'road_len_density', 'road_len_density_gap',\n",
    "    'dist_to_road_any_m', 'dist_to_road_major_m', 'dist_to_road_highway_m',\n",
    "    'road_cnt_any_in_100m', 'road_cnt_major_in_100m',\n",
    "    'road_cnt_any_in_300m', 'road_cnt_major_in_300m',\n",
    "    'road_cnt_any_in_500m', 'road_cnt_major_in_500m',\n",
    "\n",
    "    # 用途地域\n",
    "    'zone_group', 'zone_residential_rank',\n",
    "    'is_lowrise_residential', 'kenpei', 'youseki',\n",
    "\n",
    "    # 都市計画\n",
    "    'is_urbanized_area', 'is_urban_control_area', 'is_fireproof_area',\n",
    "    'is_quasi_fireproof_area', 'has_height_limit', 'has_district_plan',\n",
    "    'is_land_readjustment_area', 'is_high_utilization_area',\n",
    "    'is_urban_renaissance_area', 'is_special_far_area',\n",
    "    'is_highrise_residential_area', 'is_disaster_prevention_block',\n",
    "    'is_redevelopment_core_area',\n",
    "\n",
    "    # 標高\n",
    "    'elev_mean', 'elev_range', 'slope_mean', 'slope_max', 'slope_range',\n",
    "    'station_power_sum3', 'station_power_max3',\n",
    "\n",
    "    # 災害危険区域\n",
    "    'disaster_hit', 'disaster_n_types', 'disaster_score_sum', 'disaster_score_max', 'disaster_score',\n",
    "\n",
    "    # 学校\n",
    "    'elem_school_500m', 'junior_school_1km', 'dist_elem_school_m', 'dist_junior_school_m',\n",
    "\n",
    "    # 病院\n",
    "    'dist_hospital_m', 'dist_clinic_m',\n",
    "    'hospital_1km', 'dist_hospital_log', 'clinic_500m', 'dist_clinic_log',\n",
    "    'hospital_beds_nearest', 'hospital_is_emergency_nearest', 'hospital_is_disaster_nearest',\n",
    "    \n",
    "    # 洪水浸水（内水）\n",
    "    'ame_dist_log', 'ame_depth_max_log1p', 'ame_depth_effect',\n",
    "    \n",
    "    # 洪水浸水（外水）\n",
    "    'kouzui_keikaku_in', 'kouzui_keikaku_rank_max', 'kouzui_keikaku_dist_m_log1p',\n",
    "    'kouzui_saidai_in', 'kouzui_saidai_rank_max', 'kouzui_saidai_dist_m_log1p',\n",
    "    'kouzui_keikaku_rank_ge3', 'kouzui_saidai_rank_ge3',\n",
    "    \n",
    "    # 土砂災害\n",
    "    'kyuusha_in', 'kyuusha_dist_m', \n",
    "    'keikai_genshou1_in', 'keikai_genshou2_in', 'keikai_genshou3_in', 'keikai_kuiki1_in', 'keikai_kuiki2_in', 'keikai_kuiki3_in', 'keikai_kuiki4_in',\n",
    "    'keikai_kouji_date_min', 'keikai_dist_m',\n",
    "    'dosham_in', 'dosham_phen_gake', 'dosham_phen_doseki', 'dosham_phen_jisuberi', 'dosham_phen_nadare',\n",
    "    'dosham_A30a5_005_max', 'dosham_A30a5_006_max', 'dosham_A30a5_007_max', 'dosham_A30a5_008_max', 'dosham_A30a5_009_max', 'dosham_A30a5_010_max',\n",
    "    \n",
    "    # 地盤沈下\n",
    "    # 'jiban_dist_m_log1p', 'jiban_near_100m', 'jiban_near_300m' 'jiban_near_1000m',\n",
    "    # 'jiban_year_nearest', 'jiban_year_age',\n",
    "    # 'jiban_type_02_nearest', 'jiban_type_04_nearest' ,'jiban_type_70_nearest' ,'jiban_type_80_nearest' ,'jiban_type_90_nearest', 'jiban_type_99_nearest',\n",
    "\n",
    "    # 沿岸浸水\n",
    "    'tsunami_in', 'tsunami_depth_max_m', 'tsunami_depth_rank_max', 'tsunami_dist_log1p',\n",
    "    'tsunami_depth_max_log1p', 'tsunami_depth_ge1m', 'tsunami_depth_ge3m', 'tsunami_depth_ge5m', 'tsunami_depth_ge10m',\n",
    "    'takashio_in', 'takashio_depth_max_m', 'takashio_depth_rank_max', 'takashio_dist_log1p', \n",
    "    'tsunami_near_500m', 'tsunami_near_1km', 'tsunami_near_3km',\n",
    "    'takashio_depth_max_log1p', 'takashio_depth_ge1m', 'takashio_depth_ge3m', 'takashio_depth_ge5m', 'takashio_depth_ge10m',\n",
    "    'takashio_near_500m', 'takashio_near_1km', 'takashio_near_3km',\n",
    "\n",
    "    # 避難所\n",
    "    'hinanjo_dist_m', 'hinanjo_dist_m_clip', 'hinanjo_dist_log1p', 'hinanjo_cnt_r500', 'hinanjo_cap_sum_r500',\n",
    "    'hinanjo_cap_max_r500', 'hinanjo_area_sum_r500', 'hinanjo_area_max_r500', 'hinanjo_cap_sum_log1p_r500',\n",
    "    'hinanjo_area_sum_log1p_r500', 'hinanjo_cnt_r1000', 'hinanjo_cap_sum_r1000', 'hinanjo_cap_max_r1000',\n",
    "    'hinanjo_area_sum_r1000', 'hinanjo_area_max_r1000', 'hinanjo_cap_sum_log1p_r1000', 'hinanjo_area_sum_log1p_r1000',\n",
    "    'hinanjo_cnt_r2000', 'hinanjo_cap_sum_r2000', 'hinanjo_cap_max_r2000', 'hinanjo_area_sum_r2000',\n",
    "    'hinanjo_area_max_r2000', 'hinanjo_cap_sum_log1p_r2000', 'hinanjo_area_sum_log1p_r2000',\n",
    "    'hinanjo_dist_for_jishin_flg_m', 'hinanjo_dist_for_jishin_flg_clip', 'hinanjo_dist_for_jishin_flg_log1p',\n",
    "    'hinanjo_cnt_for_jishin_flg_r1000', 'hinanjo_cap_sum_for_jishin_flg_r1000', \n",
    "    'hinanjo_cnt_for_jishin_flg_r2000', 'hinanjo_cap_sum_for_jishin_flg_r2000',\n",
    "    'hinanjo_dist_for_tsunami_flg_m', 'hinanjo_dist_for_tsunami_flg_clip',\n",
    "    'hinanjo_dist_for_tsunami_flg_log1p', 'hinanjo_cnt_for_tsunami_flg_r1000',\n",
    "    'hinanjo_cap_sum_for_tsunami_flg_r1000', 'hinanjo_cnt_for_tsunami_flg_r2000',\n",
    "    'hinanjo_cap_sum_for_tsunami_flg_r2000', 'hinanjo_dist_for_suigai_flg_m',\n",
    "    'hinanjo_dist_for_suigai_flg_clip', 'hinanjo_dist_for_suigai_flg_log1p',\n",
    "    'hinanjo_cnt_for_suigai_flg_r1000', 'hinanjo_cap_sum_for_suigai_flg_r1000',\n",
    "    'hinanjo_cnt_for_suigai_flg_r2000', 'hinanjo_cap_sum_for_suigai_flg_r2000',\n",
    "    'hinanjo_dist_for_kazan_flg_m', 'hinanjo_dist_for_kazan_flg_clip',\n",
    "    'hinanjo_dist_for_kazan_flg_log1p', 'hinanjo_cnt_for_kazan_flg_r1000',\n",
    "    'hinanjo_cap_sum_for_kazan_flg_r1000', 'hinanjo_cnt_for_kazan_flg_r2000',\n",
    "    'hinanjo_cap_sum_for_kazan_flg_r2000', 'hinanjo_dist_for_all_flg_m',\n",
    "    'hinanjo_dist_for_all_flg_clip', 'hinanjo_dist_for_all_flg_log1p',\n",
    "    'hinanjo_cnt_for_all_flg_r1000', 'hinanjo_cap_sum_for_all_flg_r1000',\n",
    "    'hinanjo_cnt_for_all_flg_r2000', 'hinanjo_cap_sum_for_all_flg_r2000',\n",
    "    'hinanjo_dist_shubetsu_避難所_m', 'hinanjo_dist_shubetsu_避難所_log1p',\n",
    "    'hinanjo_cnt_shubetsu_避難所_r1000', 'hinanjo_cap_sum_shubetsu_避難所_r1000',\n",
    "    'hinanjo_dist_shubetsu_避難場所_m', 'hinanjo_dist_shubetsu_避難場所_log1p',\n",
    "    'hinanjo_cnt_shubetsu_避難場所_r1000', 'hinanjo_cap_sum_shubetsu_避難場所_r1000'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7a29b",
   "metadata": {},
   "source": [
    "## 出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c83b8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_geo[pkey_cols + add_cols].to_parquet(f'{intermediate_path}train_df_geo_v{geo_ver}.parquet')\n",
    "test_df_geo[pkey_cols + add_cols].to_parquet(f'{intermediate_path}test_df_geo_v{geo_ver}.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
